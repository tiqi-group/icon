{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#ion-control-software-icon","title":"Ion CONtrol Software (ICON)","text":"<p>The Ion Control Software (ICON) is a control and data-acquisition framework developed in the Trapped Ion Quantum Computing research group at ETH Zurich. It is designed for laboratories that run experiments written in Python (with the <code>pycrystal</code> framework) on M-Action/Quench hardware.</p> <p>ICON acts as the interface between user-defined Python experiments and the laboratory control hardware. It provides:</p> <ul> <li>An overview of available experiments by parsing the experiment library (a repository containing hardware and experiment descriptions).</li> <li>Access to all experiment parameters, including those from external devices integrated via <code>pydase</code> services.</li> <li>A job history and live data visualisation for running and past experiments.</li> <li>Support for parameter scans, including device parameters from connected services.</li> </ul> <p>The system is built with:</p> <ul> <li>Backend: Python (API server, scheduler, pre-/post-processing, hardware orchestration).</li> <li>Frontend: React/TypeScript (configuration, monitoring, visualisation).</li> <li>Databases:<ul> <li>SQLite - job and device history</li> <li>InfluxDB - parameter time series</li> <li>HDF5 - experiment results</li> </ul> </li> </ul>"},{"location":"about/license/","title":"License","text":"<p><code>icon</code> is released under the MIT license:</p> <pre><code>MIT License\n\nCopyright (c) 2023-2025 ETH Zurich, Mose M\u00fcller, Gerhard Br\u00e4unlich\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"getting-started/architecture/","title":"Architecture","text":"<p>ICON is structured as a modular backend-frontend system.</p> <p></p> <p>The backend server runs as multiple processes, each with a clear responsibility:</p> <ul> <li> <p>API Server</p> <p>The main process. It exposes API endpoints (\u201ccontrollers\u201d) via <code>pydase</code>. All communication between clients and the backend goes through this server.</p> <ul> <li>Provides experiment and parameter metadata from the experiment library.</li> <li>Allows parameter values in InfluxDB to be read and updated.</li> <li>Provides access to data from running and past jobs.</li> <li>Accepts new jobs, storing them in the SQLite job table.</li> </ul> </li> <li> <p>Scheduler</p> <p>Retrieves jobs from the SQLite database and dispatches them to workers. Jobs are assigned a priority, then pushed into a priority queue consumed by the pre-processing workers.</p> </li> <li> <p>Pre-Processing Workers</p> <p>One or more workers that prepare jobs before execution. For each job, they:</p> <ul> <li>Fetch the relevant parameters from InfluxDB.</li> <li>Generate JSON sequences for each data point using the experiment library.</li> <li>Place the prepared data points into the queue consumed by the hardware worker.</li> </ul> </li> <li> <p>Hardware Worker</p> <p>A single worker that interfaces with the control hardware and external devices. For each data point:</p> <ul> <li>Ensures device parameters are set if they are part of the scan.</li> <li>Executes the JSON sequence on the control hardware.</li> <li>Places the raw results into the queue for post-processing.</li> </ul> </li> <li> <p>Post-Processing Worker</p> <p>Currently a single worker (but can be scaled out). It:</p> <ul> <li>Processes raw results.</li> <li>Stores experiment data into HDF5 files.</li> <li>Updates parameter values in InfluxDB when appropriate (e.g. calibrations - not yet implemented).</li> </ul> </li> </ul> <p>Clients connect to the API server via Socket.IO and provide user interfaces:</p> <ul> <li>Web browser client (React/TypeScript) - the main interface for scheduling experiments, configuring parameters, visualising live and past data, and controlling devices.</li> <li>Python client - intended for scripting and automation (work in progress).</li> </ul>"},{"location":"getting-started/configuration-file/","title":"Configuration File","text":"<p>ICON uses a YAML configuration file, located at <code>~/.config/icon/config.yaml</code> by default. You can override this path with the <code>-c</code> flag.</p> <ul> <li>If the file does not exist, ICON will create it with default values.</li> <li>You can adjust settings either in the file or through the frontend settings page.</li> </ul>"},{"location":"getting-started/configuration-file/#databases","title":"Databases","text":"<ul> <li> <p>HDF - file format of the experiment results. Each experiment job gets its own HDF file. The result directory defaults to <code>\"$(pwd)/output\"</code> and can be configured in the configuration file:   <pre><code>data:\n  results_dir: /my/results/output/dir/\n</code></pre></p> </li> <li> <p>SQLite - stores metadata about jobs and devices. By default, ICON will create <code>icon.db</code> in the current working directory. You can override this path in the config file:</p> <pre><code>databases:\n  sqlite:\n    file: /my/custom/sqlite/path/icon.db\n</code></pre> </li> <li> <p>InfluxDB - stores parameter time series. Both InfluxDB v1 and v2 are supported (v3 may work but is untested).</p> InfluxDB v1InfluxDB v2 <pre><code>databases:\n  influxdbv1:\n    database: testing\n    host: localhost\n    measurement: Experiment Parameters\n    password: passw0rd\n    port: 8086\n    username: tester\n    ...\n</code></pre> <pre><code>databases:\n  influxdbv1:\n    database: testing\n    host: localhost\n    measurement: Experiment Parameters\n    password: &lt;influxdb v2 token&gt;\n    port: 8086\n    ...\n</code></pre> </li> </ul>"},{"location":"getting-started/experiment-library/","title":"Experiment Library","text":"<p>ICON interfaces with the experiment library to retrieve metadata about experiments and their parameters.</p>"},{"location":"getting-started/experiment-library/#experiment-library-structure","title":"Experiment library structure","text":"<p>The experiment library is a standalone Python project. A typical layout looks like this:</p> <pre><code>.\n\u251c\u2500\u2500 experiment_library\n\u2502   \u251c\u2500\u2500 experiments\n\u2502   \u2502   \u251c\u2500\u2500 &lt;your_experiment_1&gt;.py\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 frames                  # Optional: reusable experiment logic\n\u2502   \u2502   \u251c\u2500\u2500 frame_cool_det.py\n\u2502   \u2502   \u2514\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 globals\n\u2502   \u2502   \u251c\u2500\u2500 global_functions.py\n\u2502   \u2502   \u251c\u2500\u2500 global_parameters.py\n\u2502   \u2502   \u2514\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 hardware_description\n\u2502   \u2502   \u251c\u2500\u2500 hardware.py\n\u2502   \u2502   \u2514\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 __init__.py\n\u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>The library uses pycrystal to define:</p> <ul> <li>the control hardware,</li> <li>global parameters,</li> <li>and experiment classes.</li> </ul> <p>The most relevant modules for ICON are:</p> <ul> <li><code>experiment_library.experiments</code> - experiment definitions.</li> <li><code>experiment_library.globals</code> - global parameters and functions.</li> </ul>"},{"location":"getting-started/experiment-library/#interaction-with-icon","title":"Interaction with ICON","text":"<p>ICON uses templates that are executed in the context of the experiment library. To do this, ICON:</p> <ol> <li>Spawns a subprocess using the Python environment of the experiment library.</li> <li>Fills the template values (parameter dictionary, and for experiment instances also the module name and instance name).</li> <li>Executes the template to retrieve metadata about experiments and parameters.</li> </ol> <p>This mechanism ensures ICON always uses the definitions and environment of the experiment library itself.</p>"},{"location":"getting-started/experiment-library/#experiment-class-structure","title":"Experiment class structure","text":"<p>Experiments are defined by subclassing <code>pycrystal.Experiment</code>. Local parameters (parameters scoped to an experiment instance) are created in <code>define_parameters</code>:</p> <pre><code># experiment_library/experiments/test_exp.py\n\nfrom pycrystal import Experiment\nfrom pycrystal.parameters import BooleanParameter\n\n\nclass TestExperiment(Experiment):\n    def define_parameters(self) -&gt; None:\n        self.threshold = BooleanParameter(\n            parameter_group=\"Local detection settings\",\n            description=\"Threshold\",\n            display_name_template=\"{description}\",\n        )\n        self.detect_background = BooleanParameter(\n            parameter_group=\"Local detection settings\",\n            description=\"Detect Background\",\n            display_name_template=\"{description}\",\n        )\n</code></pre> <p>This defines an experiment with two boolean parameters. Each parameter specifies:</p> <ul> <li>a parameter group (used for display grouping),</li> <li>a description,</li> <li>and a display name template (to control how the parameter is shown in the frontend).</li> </ul>"},{"location":"getting-started/experiment-library/#experiment-instances","title":"Experiment instances","text":"<p>To expose experiments to ICON, you define experiment instances in a <code>CONFIGURATION</code> dictionary inside the experiment module. This dictionary specifies which experiments should be available, how they are grouped, and which display groups are shown in the frontend:</p> <pre><code># experiment_library/experiments/test_exp.py\n\nCONFIGURATION = {\n    \"display_groups\": {\n        Repumping,\n        Detection,\n    },\n    \"experiment_instances\": [\n        (TestExperiment, {\"name\": \"Cool Det\"}),\n    ],\n}\n</code></pre> <ul> <li><code>display_groups</code> - defines parameter groups to be shown in the experiment overview. These can include both experiment parameters and device parameters.</li> <li><code>experiment_instances</code> - a list of tuples, each consisting of:<ul> <li>the experiment class,</li> <li>and a dictionary of constructor arguments (e.g. <code>{\"name\": \"Cool Det\"}</code>).</li> </ul> </li> </ul> <p>This mechanism allows you to register multiple instances of the same experiment class (e.g. with different configurations) and control what parameter groups are visible in ICON.</p>"},{"location":"getting-started/running-icon/","title":"Running ICON","text":"<p>ICON runs on Linux and requires Python 3 to start. The easiest way to start is by downloading the binary from the releases page. Make it executable and run it:</p> <pre><code>$ chmod +x icon-linux-amd64\n$ ./icon-linux-amd64 --help\nUsage: icon-linux-amd64 [OPTIONS]\n\n  Start the ICON server\n\nOptions:\n  -V, --version      Print version.\n  -v, --verbose      Increase verbosity (-v, -vv)\n  -q, --quiet        Decrease verbosity (-q)\n  -c, --config FILE  Path to the configuration file [default: ~/.config/icon/config.yaml]\n  -h, --help         Show this message and exit\n</code></pre> <p>If you prefer to run ICON from source, clone the repository and use <code>uv</code> as the dependency manager:</p> <pre><code>git clone https://github.com/tiqi-group/icon.git\ncd icon\nuv sync --extra server --extra zedboard\nuv run python -m icon.server\n</code></pre>"},{"location":"getting-started/running-icon/#ionpulse","title":"Ionpulse","text":"<p>To be able to run experiments, ICON must connect to Ionpulse, the server application running on the control system such QuENCH RFSoC, QuENCH MicroTCA or M-ACTION. Configure the URL or IP and port under Settings -&gt; Hardware.</p>"},{"location":"getting-started/running-icon/#development","title":"Development","text":"<p>If you are testing/developing ICON without the control system, you can use the ionpulse emulator. See the Ionpulse sw test README for more info on how to set up the emulator.</p>"},{"location":"getting-started/frontend/devices/","title":"Devices page","text":"<p>The Devices page gives you an overview of all devices connected to ICON. Devices are <code>pydase</code> services that ICON communicates with. They can be controlled through the ICON interface, and their parameters can be included in experiment scans.</p> <p></p> <p>You can add new devices by clicking the + Add Device button on the left.</p> <p></p>"},{"location":"getting-started/frontend/devices/#device-states","title":"Device states","text":"<p>Devices can be enabled or disabled:</p> <ul> <li> <p>Enabled devices</p> <ul> <li>ICON automatically attempts to connect to them.</li> <li>A reachability indicator shows whether the device is online.</li> <li>For reachable devices, you see:<ul> <li>A list of scannable parameters (<code>int</code> or <code>float</code>).</li> <li>A Disable button.</li> <li> <p>Two configuration fields:</p> <ul> <li>Retry Attempts \u2013 how many times the hardware worker should check if the desired parameter value was set.</li> <li>Retry Delay (s) \u2013 how long to wait between checks.</li> </ul> <p>These settings are used by the hardware worker to verify that parameter changes succeeded. If the checks fail, the job is marked as FAILED.</p> </li> </ul> </li> </ul> </li> <li> <p>Disabled devices</p> <ul> <li>ICON does not attempt to connect to them.</li> <li>You can still edit the device\u2019s URL that was defined when adding it.</li> </ul> </li> </ul> <p></p>"},{"location":"getting-started/frontend/devices/#display-modes","title":"Display modes","text":"<p>You can toggle between two views:</p> <ul> <li>Scannable parameters \u2013 shows parameters of type <code>int</code> or <code>float</code> that can be used in experiment scans.</li> <li>pydase interface \u2013 embeds the device\u2019s native <code>pydase</code> frontend via an <code>iframe</code>.</li> </ul> <p></p>"},{"location":"getting-started/frontend/experiments/","title":"Experiments page","text":"<p>The experiments page shows a list of experiments on the left. This list is parsed from the experiment library ICON connects to. When you click on an experiment, you will be greeted with the scan interface on the top. This allows you to define parameters you want to scan, the priority of the scan (number between 1 and 20), the number of shots for each data point, and the number of scan repetitions.</p> <p>On the bottom, you will get the display groups defined for the experiment (instance). These are either local parameters (i.e. parameters scoped to the experiment instance), global parmaeters (i.e. parameters that) or device parameters</p>"},{"location":"getting-started/frontend/parameters/","title":"Parameters page","text":"<p>The parameters page in ICON displays the <code>pycrystal</code> parameters defined in the experiment library. In <code>pycrystal</code>, parameters represent values used when generating control sequences (e.g., frequencies, durations, amplitudes). They act as interfaces to the InfluxDB database.</p>"},{"location":"getting-started/frontend/parameters/#how-parameters-are-displayed","title":"How Parameters are Displayed","text":"<p>How a parameter is displayed is defined through its namespace, display group and display name, i.e. each parameter will be displayed with its display name within its namespace, grouped under the display group.</p> <p>The namespace is defined by the python module where it is defined (e.g. <code>experiment_library.globals.global_parameters</code>) and optionally the experiment class and instance name when the parameter is defined within the <code>define_parameters</code> method of an <code>Experiment</code> class (e.g. <code>experiment_library.experiments.exp_cool_det.ExperimentCoolDet (Cool Det)</code>, where the class name is appended to the module name, and the instance name is within parentheses).</p> <p>The display group is defined by the <code>display_group</code> passed to the parameter. The parameters will be organized under this display group in the frontend. If it is not set manually, the display group is calculated as follows:</p> <ul> <li> <p>when the parameter is defined within the <code>define_parameters</code> method of an experiment class, the display group defaults to <code>\"Local Parameters\"</code></p> <p>Example</p> <p>The display group of the following parameter is <code>\"Local Parameters\"</code>:</p> <pre><code>from pycrystal.experiment import Experiment\n\nclass TestExperiment(Experiment):\n    # ...\n    def define_parameters(self) -&gt; None\n        self.my_frequency = FrequencyParameter()\n</code></pre> </li> <li> <p>when the parameter is defined as an attribute of a wrapper class that has defined a static-/class-method called <code>display_name</code>, the string returned from that method will be used. Otherwise, the name of the containing class is taken as the display group.</p> <p>Example</p> <p>The display group of the following parameter is <code>\"My display group\"</code>:</p> <pre><code>class MyDisplayGroup:\n    my_frequency = FrequencyParameter()\n\n    @staticmethod\n    def display_group() -&gt; str:\n        return \"My display group\"\n</code></pre> </li> </ul> <p>The display name is defined by the <code>display_name_template</code> that is passed to the parameter constructor. It is a template string used to define a custom display name format for each parameter combination. The template allows placeholders that will be dynamically filled based on the provided parameters and other class attributes. It defaults to <code>\"{specifiers}\"</code>.</p> <p>The following placeholders can be used:</p> <ul> <li><code>{namespace}</code>: The namespace associated with the parameter which is generated   from the module name and the experiment instance name of parameters defined   within an experiment\u2019s <code>define_parameters</code> method.</li> <li><code>{parameter_group}</code>: The name of the parameter group, which corresponds to a   field in the InfluxDB parameter backend.</li> <li><code>{specifiers}</code>: A summary of all parameter specifiers (tags in InfluxDB)   generated from <code>kwargs</code>, formatted as <code>EnumClass.EnumMember.value</code>. This   placeholder provides a full list of specifiers for a comprehensive display   name.</li> <li><code>{specifiers_full}</code>: A summary of all parameter specifiers (tags in InfluxDB)   generated from <code>kwargs</code>, formatted as <code>key=EnumClass.EnumMember.value</code>. This   placeholder provides a full list of specifiers for a comprehensive display   name.</li> <li>Additional <code>{key}</code> placeholders: Each <code>kwargs</code> key can also be used as a    placeholder to display specific parameter values. For example, <code>{mode}</code> would   refer to the value of <code>mode</code> if it is a key in <code>kwargs</code>.</li> </ul> <p>Example</p> <p>To generate display names with the format <code>\"experiment_library.globals.global_parameters: default - mode=MotionalModes.AXIAL\"</code>, use:</p> <pre><code>display_name_template = \"{namespace}: {parameter_group} - mode={mode}\"\n</code></pre>"},{"location":"getting-started/frontend/start/","title":"Start page","text":"<p>The web frontend is served automatically by the ICON backend. By default it is available at http://localhost:8004. The port and address can be changed in the config file.</p> <p></p> <p>On the start page, you are greeted with a status overview of three core components:</p> <ul> <li>InfluxDB database</li> <li>Hardware</li> <li>Devices</li> </ul> <p>Each entry includes a shortcut for configuration or navigation:</p> <ul> <li>For InfluxDB and hardware, an edit icon is provided. Clicking it takes you to the settings page, where you can update their configuration.</li> <li>For the devices table, a link is provided that takes you directly to the devices page.</li> </ul>"},{"location":"plantuml_diagrams/Readme/","title":"PlantUML Diagrams","text":"<p>This directory contains the PlantUML source code and vector graphics of important parts of ICON. The diagrams were generated using the  PlantUML software.</p> <p>If you are using VSCode, you can use this plugin to add support for PlantUML.</p> <p>With neovim, I am using nvim-soil and plantuml-syntax.</p> <p>You can also try it out on either of the following websites:</p> <ul> <li>PlantUML</li> <li>PlantText</li> </ul>"},{"location":"reference/server/","title":"Server","text":"<ul> <li><code>icon.server.api</code></li> <li><code>icon.server.data_access.models.enums</code></li> <li><code>icon.server.data_access.models.sqlite</code></li> <li><code>icon.server.data_access.repositories</code></li> <li><code>icon.server.hardware_processing</code></li> <li><code>icon.server.post_processing</code></li> <li><code>icon.server.pre_processing</code></li> <li><code>icon.server.scheduler</code></li> <li><code>icon.server.utils.types</code></li> <li><code>icon.server.web_server</code></li> </ul>"},{"location":"reference/server/#icon.server.api","title":"icon.server.api","text":"<p>This module defines the API layer of ICON, implemented as a <code>pydase.DataService</code>.</p> <p>The main entry point is the <code>APIService</code>, which is exposed by the <code>IconServer</code>. The <code>IconServer</code> itself is a <code>pydase.Server</code> hosting the API.</p>"},{"location":"reference/server/#icon.server.api--structure","title":"Structure","text":"<p>The <code>APIService</code> aggregates multiple \u201ccontroller\u201d services as attributes. Each controller is itself a <code>pydase.DataService</code> exposing related API methods.</p>"},{"location":"reference/server/#icon.server.api--background-tasks","title":"Background tasks","text":"<p>Controllers can define periodic <code>pydase</code> tasks, which are asyncio tasks automatically started with the service.</p>"},{"location":"reference/server/#icon.server.api.api_service","title":"api_service","text":""},{"location":"reference/server/#icon.server.api.api_service.APIService","title":"APIService","text":"<pre><code>APIService(\n    pre_processing_event_queues: list[Queue[UpdateQueue]],\n)\n</code></pre> <p>               Bases: <code>DataService</code></p> <p>Aggregates ICON\u2019s API controllers and manages background tasks.</p> <p>The <code>APIService</code> groups multiple controllers, each of which is a <code>pydase.DataService</code> exposing related API methods. It also defines background tasks for keeping experiment and parameter metadata in sync with the experiment library and InfluxDB.</p> Note <p>Controllers are <code>pydase.DataService</code> instances exposed as attributes to group related API methods. Background tasks are implemented with <code>pydase</code> tasks.</p> <p>Parameters:</p> Name Type Description Default <code>pre_processing_event_queues</code> <code>list[Queue[UpdateQueue]]</code> <p>Queues used by <code>ScansController</code> to notify pre-processing workers.</p> required Source code in <code>src/icon/server/api/api_service.py</code> <pre><code>def __init__(\n    self, pre_processing_event_queues: list[multiprocessing.Queue[UpdateQueue]]\n) -&gt; None:\n    \"\"\"\n    Args:\n        pre_processing_event_queues: Queues used by `ScansController` to notify\n            pre-processing workers.\n    \"\"\"\n\n    super().__init__()\n\n    self.devices = DevicesController()\n    \"\"\"Controller for managing external pydase-based devices.\"\"\"\n    self.scheduler = SchedulerController(devices_controller=self.devices)\n    \"\"\"Controller to submit, inspect, and cancel scheduled jobs.\"\"\"\n    self.experiments = ExperimentsController()\n    \"\"\"Controller for experiment metadata.\"\"\"\n    self.parameters = ParametersController()\n    \"\"\"Controller for parameter metadata and shared parameter values.\"\"\"\n    self.config = ConfigurationController()\n    \"\"\"Controller for managing and updating the application's configuration.\"\"\"\n    self.data = ExperimentDataController()\n    \"\"\"Controller for accessing stored experiment data.\"\"\"\n    self.scans = ScansController(\n        pre_processing_update_queues=pre_processing_event_queues\n    )\n    \"\"\"Controller for triggering update events for jobs across multiple worker\n    processes.\"\"\"\n    self.status = StatusController()\n    \"\"\"Controller for system status monitoring.\"\"\"\n</code></pre>"},{"location":"reference/server/#icon.server.api.api_service.APIService.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = ConfigurationController()\n</code></pre> <p>Controller for managing and updating the application\u2019s configuration.</p>"},{"location":"reference/server/#icon.server.api.api_service.APIService.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data = ExperimentDataController()\n</code></pre> <p>Controller for accessing stored experiment data.</p>"},{"location":"reference/server/#icon.server.api.api_service.APIService.devices","title":"devices  <code>instance-attribute</code>","text":"<pre><code>devices = DevicesController()\n</code></pre> <p>Controller for managing external pydase-based devices.</p>"},{"location":"reference/server/#icon.server.api.api_service.APIService.experiments","title":"experiments  <code>instance-attribute</code>","text":"<pre><code>experiments = ExperimentsController()\n</code></pre> <p>Controller for experiment metadata.</p>"},{"location":"reference/server/#icon.server.api.api_service.APIService.parameters","title":"parameters  <code>instance-attribute</code>","text":"<pre><code>parameters = ParametersController()\n</code></pre> <p>Controller for parameter metadata and shared parameter values.</p>"},{"location":"reference/server/#icon.server.api.api_service.APIService.scans","title":"scans  <code>instance-attribute</code>","text":"<pre><code>scans = ScansController(\n    pre_processing_update_queues=pre_processing_event_queues\n)\n</code></pre> <p>Controller for triggering update events for jobs across multiple worker processes.</p>"},{"location":"reference/server/#icon.server.api.api_service.APIService.scheduler","title":"scheduler  <code>instance-attribute</code>","text":"<pre><code>scheduler = SchedulerController(devices_controller=devices)\n</code></pre> <p>Controller to submit, inspect, and cancel scheduled jobs.</p>"},{"location":"reference/server/#icon.server.api.api_service.APIService.status","title":"status  <code>instance-attribute</code>","text":"<pre><code>status = StatusController()\n</code></pre> <p>Controller for system status monitoring.</p>"},{"location":"reference/server/#icon.server.api.configuration_controller","title":"configuration_controller","text":""},{"location":"reference/server/#icon.server.api.configuration_controller.ConfigurationController","title":"ConfigurationController","text":"<p>               Bases: <code>DataService</code></p> <p>Controller for managing and updating the application\u2019s configuration.</p> <p>This class provides an API to get and update the configuration, validate it, and save the updated configuration back to the source file.</p>"},{"location":"reference/server/#icon.server.api.configuration_controller.ConfigurationController.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; dict[str, Any]\n</code></pre> <p>Get current configuration dictionary.</p> Source code in <code>src/icon/server/api/configuration_controller.py</code> <pre><code>def get_config(self) -&gt; dict[str, Any]:\n    \"\"\"Get current configuration dictionary.\"\"\"\n\n    return get_config().model_dump()\n</code></pre>"},{"location":"reference/server/#icon.server.api.configuration_controller.ConfigurationController.update_config_option","title":"update_config_option","text":"<pre><code>update_config_option(key: str, value: Any) -&gt; bool\n</code></pre> <p>Update a specific configuration option.</p> <p>Traverses the configuration using the dot-separated key, updates the specified value, validates the entire configuration, and saves the changes.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The dot-separated key of the configuration option (e.g., \u201cexperiment_library.git_repository\u201d).</p> required <code>value</code> <code>Any</code> <p>The new value for the configuration option.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the update is successful, False otherwise.</p> Source code in <code>src/icon/server/api/configuration_controller.py</code> <pre><code>def update_config_option(self, key: str, value: Any) -&gt; bool:\n    \"\"\"Update a specific configuration option.\n\n    Traverses the configuration using the dot-separated key, updates the specified\n    value, validates the entire configuration, and saves the changes.\n\n    Args:\n        key:\n            The dot-separated key of the configuration option (e.g.,\n            \"experiment_library.git_repository\").\n        value:\n            The new value for the configuration option.\n\n    Returns:\n        True if the update is successful, False otherwise.\n    \"\"\"\n\n    try:\n        # Traverse to the nested key\n        fields = key.split(\".\")\n        current_config = get_config().model_dump()\n        current = current_config\n        for field in fields[:-1]:\n            if field not in current:\n                raise KeyError(f\"Key {key!r} not found in configuration.\")\n            current = current[field]\n\n        # Update the value\n        current[fields[-1]] = value\n\n        # Validate the updated configuration\n        updated_config = ServiceConfigV1(config_sources=DataSource(current_config))\n\n        # Save the updated configuration back to the file\n        self._save_configuration(updated_config)\n        emit_queue.put(\n            {\"event\": \"config.update\", \"data\": updated_config.model_dump()}\n        )\n        return True\n    except KeyError as e:\n        logger.exception(\"Failed to update configuration: %s\", e)\n        return False\n</code></pre>"},{"location":"reference/server/#icon.server.api.devices_controller","title":"devices_controller","text":""},{"location":"reference/server/#icon.server.api.devices_controller.DeviceParameterValueyType","title":"DeviceParameterValueyType  <code>module-attribute</code>","text":"<pre><code>DeviceParameterValueyType = int | bool | float\n</code></pre> <p>Allowed primitive types for device parameter values.</p> <p>A parameter value sent to or retrieved from a device may be one of these basic types. Quantities with units are handled separately via <code>pydase.units.Quantity</code>.</p>"},{"location":"reference/server/#icon.server.api.devices_controller.DevicesController","title":"DevicesController","text":"<pre><code>DevicesController()\n</code></pre> <p>               Bases: <code>DataService</code></p> <p>Controller for managing external pydase-based devices.</p> <p>Maintains client connections to configured devices, exposes helpers to add/update device entries in SQLite, and provides async accessors for device parameter values through pydase proxies. Also discovers scannable device parameters for integration with ICON scans.</p> Source code in <code>src/icon/server/api/devices_controller.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n    self._devices: dict[str, pydase.Client] = {}\n    self.device_proxies: dict[str, ProxyClass] = {}\n    \"\"\"Live pydase proxies keyed by device name.\"\"\"\n</code></pre>"},{"location":"reference/server/#icon.server.api.devices_controller.DevicesController.device_proxies","title":"device_proxies  <code>instance-attribute</code>","text":"<pre><code>device_proxies: dict[str, ProxyClass] = {}\n</code></pre> <p>Live pydase proxies keyed by device name.</p>"},{"location":"reference/server/#icon.server.api.devices_controller.DevicesController.add_device","title":"add_device","text":"<pre><code>add_device(\n    *,\n    name: str,\n    url: str,\n    status: Literal[\"disabled\", \"enabled\"] = \"enabled\",\n    description: str | None = None,\n    retry_delay_seconds: float = 0.0,\n    retry_attempts: int = 3,\n) -&gt; Device\n</code></pre> <p>Create a device record in SQLite and (optionally) connect to it.</p> <p>If <code>status==\"enabled\"</code>, a non-blocking pydase client is created and its proxy is registered.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique device name.</p> required <code>url</code> <code>str</code> <p>pydase server URL of the device.</p> required <code>status</code> <code>Literal['disabled', 'enabled']</code> <p>Whether the device should be connected immediately.</p> <code>'enabled'</code> <code>description</code> <code>str | None</code> <p>Optional human-readable description.</p> <code>None</code> <code>retry_delay_seconds</code> <code>float</code> <p>Backoff delay used by device-side logic.</p> <code>0.0</code> <code>retry_attempts</code> <code>int</code> <p>Number of retries used by device-side logic.</p> <code>3</code> <p>Returns:</p> Type Description <code>Device</code> <p>The <code>Device</code> SQLAlchemy model.</p> Source code in <code>src/icon/server/api/devices_controller.py</code> <pre><code>def add_device(  # noqa: PLR0913\n    self,\n    *,\n    name: str,\n    url: str,\n    status: Literal[\"disabled\", \"enabled\"] = \"enabled\",\n    description: str | None = None,\n    retry_delay_seconds: float = 0.0,\n    retry_attempts: int = 3,\n) -&gt; Device:\n    \"\"\"Create a device record in SQLite and (optionally) connect to it.\n\n    If `status==\"enabled\"`, a non-blocking pydase client is created and its\n    proxy is registered.\n\n    Args:\n        name: Unique device name.\n        url: pydase server URL of the device.\n        status: Whether the device should be connected immediately.\n        description: Optional human-readable description.\n        retry_delay_seconds: Backoff delay used by device-side logic.\n        retry_attempts: Number of retries used by device-side logic.\n\n    Returns:\n        The `Device` SQLAlchemy model.\n    \"\"\"\n\n    device = DeviceRepository.add_device(\n        device=Device(\n            name=name,\n            url=url,\n            status=DeviceStatus(status),\n            description=description,\n            retry_delay_seconds=retry_delay_seconds,\n            retry_attempts=retry_attempts,\n        )\n    )\n\n    if status == \"enabled\":\n        client = pydase.Client(\n            url=device.url,\n            client_id=\"icon-devices-controller\",\n            block_until_connected=False,\n        )\n        self._devices[name] = client\n        self.device_proxies[name] = client.proxy\n\n    return device\n</code></pre>"},{"location":"reference/server/#icon.server.api.devices_controller.DevicesController.get_devices_by_status","title":"get_devices_by_status","text":"<pre><code>get_devices_by_status(\n    *, status: DeviceStatus | None = None\n) -&gt; dict[str, DeviceDict]\n</code></pre> <p>List devices (optionally filtered by status) with reachability &amp; scan info.</p> <p>Augments each device entry with</p> <ul> <li><code>reachable</code>: Whether a live proxy is connected.</li> <li><code>scannable_params</code>: Flat list of scannable parameter access paths.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>status</code> <code>DeviceStatus | None</code> <p>Optional filter (<code>ENABLED</code>, <code>DISABLED</code>, or <code>None</code> for all).</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, DeviceDict]</code> <p>Mapping from device name to a <code>DeviceDict</code> payload suitable for the API.</p> Source code in <code>src/icon/server/api/devices_controller.py</code> <pre><code>def get_devices_by_status(\n    self, *, status: DeviceStatus | None = None\n) -&gt; dict[str, DeviceDict]:\n    \"\"\"List devices (optionally filtered by status) with reachability &amp; scan info.\n\n    Augments each device entry with\n\n    - `reachable`: Whether a live proxy is connected.\n    - `scannable_params`: Flat list of scannable parameter access paths.\n\n    Args:\n        status: Optional filter (`ENABLED`, `DISABLED`, or `None` for all).\n\n    Returns:\n        Mapping from device name to a `DeviceDict` payload suitable for the API.\n    \"\"\"\n\n    device_dict: dict[str, DeviceDict] = {\n        device.name: SQLAlchemyDictEncoder.encode(device)\n        for device in DeviceRepository.get_devices_by_status(status=status)\n    }\n\n    for name, value in device_dict.items():\n        client = self._devices.get(name, None)\n        value[\"reachable\"] = False\n        value[\"scannable_params\"] = []\n\n        if client is not None:\n            value[\"reachable\"] = client.proxy.connected\n            value[\"scannable_params\"] = get_scannable_params_list(\n                client.proxy.serialize(),\n                prefix=f'devices.device_proxies[\"{name}\"].',\n            )\n\n    return device_dict\n</code></pre>"},{"location":"reference/server/#icon.server.api.devices_controller.DevicesController.get_parameter_value","title":"get_parameter_value  <code>async</code>","text":"<pre><code>get_parameter_value(*, name: str, parameter_id: str) -&gt; Any\n</code></pre> <p>Get a parameter value from a connected device.</p> <p>Logs a warning if the device is not connected or not found.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Device name.</p> required <code>parameter_id</code> <code>str</code> <p>Access path on the device service.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The parameter value as returned by the device, or <code>None</code> if the device is unreachable or unknown.</p> Source code in <code>src/icon/server/api/devices_controller.py</code> <pre><code>async def get_parameter_value(self, *, name: str, parameter_id: str) -&gt; Any:\n    \"\"\"Get a parameter value from a connected device.\n\n    Logs a warning if the device is not connected or not found.\n\n    Args:\n        name: Device name.\n        parameter_id: Access path on the device service.\n\n    Returns:\n        The parameter value as returned by the device, or `None` if the device is\n            unreachable or unknown.\n    \"\"\"\n\n    try:\n        return await asyncio.to_thread(\n            self._devices[name].get_value, access_path=parameter_id\n        )\n    except BadNamespaceError:\n        logger.warning(\n            'Could not get %r. Device %r at (\"%s\") is not connected.',\n            parameter_id,\n            name,\n            self._devices[name]._url,\n        )\n    except KeyError:\n        logger.warning(\"Device with name %r not found. Is it enabled?\", name)\n</code></pre>"},{"location":"reference/server/#icon.server.api.devices_controller.DevicesController.update_device","title":"update_device","text":"<pre><code>update_device(\n    *,\n    name: str,\n    status: Literal[\"disabled\", \"enabled\"] | None = None,\n    url: str | None = None,\n    retry_attempts: int | None = None,\n    retry_delay_seconds: float | None = None,\n) -&gt; Device\n</code></pre> <p>Update a device record and its live connection.</p> <p>When transitioning to <code>disabled</code>, the client is disconnected and removed. When transitioning to <code>enabled</code>, a client is (re)created and registered.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Device name.</p> required <code>status</code> <code>Literal['disabled', 'enabled'] | None</code> <p>Target enable/disable status.</p> <code>None</code> <code>url</code> <code>str | None</code> <p>Updated pydase URL.</p> <code>None</code> <code>retry_attempts</code> <code>int | None</code> <p>Updated retry attempts metadata.</p> <code>None</code> <code>retry_delay_seconds</code> <code>float | None</code> <p>Updated retry delay metadata.</p> <code>None</code> <p>Returns:</p> Type Description <code>Device</code> <p>The updated <code>Device</code> model.</p> Source code in <code>src/icon/server/api/devices_controller.py</code> <pre><code>def update_device(\n    self,\n    *,\n    name: str,\n    status: Literal[\"disabled\", \"enabled\"] | None = None,\n    url: str | None = None,\n    retry_attempts: int | None = None,\n    retry_delay_seconds: float | None = None,\n) -&gt; Device:\n    \"\"\"Update a device record and its live connection.\n\n    When transitioning to `disabled`, the client is disconnected and removed.\n    When transitioning to `enabled`, a client is (re)created and registered.\n\n    Args:\n        name: Device name.\n        status: Target enable/disable status.\n        url: Updated pydase URL.\n        retry_attempts: Updated retry attempts metadata.\n        retry_delay_seconds: Updated retry delay metadata.\n\n    Returns:\n        The updated `Device` model.\n    \"\"\"\n\n    device = DeviceRepository.update_device(\n        name=name,\n        url=url,\n        status=DeviceStatus(status) if status is not None else None,\n        retry_attempts=retry_attempts,\n        retry_delay_seconds=retry_delay_seconds,\n    )\n\n    if status == \"disabled\" and name in self._devices:\n        if name in self.device_proxies:\n            self.device_proxies.pop(name)\n        if name in self._devices:\n            client = self._devices.pop(name)\n            client.disconnect()\n    elif status == \"enabled\":\n        client = pydase.Client(\n            url=device.url,\n            client_id=\"icon-devices-controller\",\n            block_until_connected=False,\n        )\n        self._devices[name] = client\n        self.device_proxies[device.name] = client.proxy\n\n    return device\n</code></pre>"},{"location":"reference/server/#icon.server.api.devices_controller.DevicesController.update_parameter_value","title":"update_parameter_value  <code>async</code>","text":"<pre><code>update_parameter_value(\n    *,\n    name: str,\n    parameter_id: str,\n    new_value: DeviceParameterValueyType | QuantityDict,\n    type_: Literal[\"float\", \"int\", \"Quantity\"],\n) -&gt; None\n</code></pre> <p>Set a parameter value on a connected device.</p> <p>Performs type-normalization (<code>float</code>, <code>int</code>, or <code>Quantity</code>) before delegating to the device client.</p> <p>Logs a warning if the device is not connected or not found.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Device name.</p> required <code>parameter_id</code> <code>str</code> <p>Access path on the device service.</p> required <code>new_value</code> <code>DeviceParameterValueyType | QuantityDict</code> <p>New value (native type or quantity dict).</p> required <code>type_</code> <code>Literal['float', 'int', 'Quantity']</code> <p>Expected type of the value for normalization.</p> required Source code in <code>src/icon/server/api/devices_controller.py</code> <pre><code>async def update_parameter_value(\n    self,\n    *,\n    name: str,\n    parameter_id: str,\n    new_value: DeviceParameterValueyType | u.QuantityDict,\n    type_: Literal[\"float\", \"int\", \"Quantity\"],\n) -&gt; None:\n    \"\"\"Set a parameter value on a connected device.\n\n    Performs type-normalization (`float`, `int`, or `Quantity`) before delegating\n    to the device client.\n\n    Logs a warning if the device is not connected or not found.\n\n    Args:\n        name: Device name.\n        parameter_id: Access path on the device service.\n        new_value: New value (native type or quantity dict).\n        type_: Expected type of the value for normalization.\n    \"\"\"\n\n    if type_ == \"float\" and not isinstance(new_value, dict):\n        new_value = float(new_value)\n    elif type_ == \"int\" and not isinstance(new_value, dict):\n        new_value = int(new_value)\n    elif type_ == \"Quantity\" and isinstance(new_value, dict):\n        new_value = u.Quantity(new_value[\"magnitude\"], new_value[\"unit\"])  # type: ignore\n\n    try:\n        await asyncio.to_thread(\n            self._devices[name].update_value,\n            access_path=parameter_id,\n            new_value=new_value,\n        )\n    except BadNamespaceError:\n        logger.warning(\n            'Could not set %r. Device %r at (\"%s\") is not connected.',\n            parameter_id,\n            name,\n            self._devices[name]._url,\n        )\n    except KeyError:\n        logger.warning(\"Device with name %r not found. Is it enabled?\", name)\n</code></pre>"},{"location":"reference/server/#icon.server.api.experiment_data_controller","title":"experiment_data_controller","text":""},{"location":"reference/server/#icon.server.api.experiment_data_controller.ExperimentDataController","title":"ExperimentDataController","text":"<p>               Bases: <code>DataService</code></p> <p>Controller for accessing stored experiment data.</p> <p>Provides API methods to fetch experiment data associated with jobs.</p>"},{"location":"reference/server/#icon.server.api.experiment_data_controller.ExperimentDataController.get_experiment_data_by_job_id","title":"get_experiment_data_by_job_id  <code>async</code>","text":"<pre><code>get_experiment_data_by_job_id(\n    job_id: int,\n) -&gt; dict[str, Any]\n</code></pre> <p>Return experiment data for a given job.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>The unique identifier of the job.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The experiment data linked to the job as a dict resulting</p> <code>dict[str, Any]</code> <p>from serializing a ExperimentData instance.</p> Source code in <code>src/icon/server/api/experiment_data_controller.py</code> <pre><code>async def get_experiment_data_by_job_id(self, job_id: int) -&gt; dict[str, Any]:\n    \"\"\"Return experiment data for a given job.\n\n    Args:\n        job_id: The unique identifier of the job.\n\n    Returns:\n        The experiment data linked to the job as a dict resulting\n        from serializing a\n         [ExperimentData][icon.server.data_access.repositories.experiment_data_repository.ExperimentData] instance.\n    \"\"\"\n\n    return asdict(\n        ExperimentDataRepository.get_experiment_data_by_job_id(job_id=job_id)\n    )\n</code></pre>"},{"location":"reference/server/#icon.server.api.experiments_controller","title":"experiments_controller","text":""},{"location":"reference/server/#icon.server.api.experiments_controller.ExperimentsController","title":"ExperimentsController","text":"<pre><code>ExperimentsController()\n</code></pre> <p>               Bases: <code>DataService</code></p> <p>Controller for experiment metadata.</p> <p>Stores the current set of experiments and exposes them to the API. Updates are compared against the existing metadata and, if changes are detected, an update event is pushed to the Socket.IO emit queue.</p> Source code in <code>src/icon/server/api/experiments_controller.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n    self._experiments: ExperimentDict = {}\n</code></pre>"},{"location":"reference/server/#icon.server.api.experiments_controller.ExperimentsController.get_experiments","title":"get_experiments","text":"<pre><code>get_experiments() -&gt; ExperimentDict\n</code></pre> <p>Return the current experiment metadata.</p> <p>Returns:</p> Type Description <code>ExperimentDict</code> <p>Mapping of experiment IDs to their metadata.</p> Source code in <code>src/icon/server/api/experiments_controller.py</code> <pre><code>def get_experiments(self) -&gt; ExperimentDict:\n    \"\"\"Return the current experiment metadata.\n\n    Returns:\n        Mapping of experiment IDs to their metadata.\n    \"\"\"\n\n    return self._experiments\n</code></pre>"},{"location":"reference/server/#icon.server.api.experiments_controller.ExperimentsController.get_metadata","title":"get_metadata","text":"<pre><code>get_metadata(experiment_id: str) -&gt; ExperimentMetadata\n</code></pre> <p>Serve experiment metadata for experiment id <code>experiment_id</code>.</p> Source code in <code>src/icon/server/api/experiments_controller.py</code> <pre><code>def get_metadata(self, experiment_id: str) -&gt; ExperimentMetadata:\n    \"\"\"Serve experiment metadata for experiment id `experiment_id`.\"\"\"\n    return self._experiments[experiment_id]\n</code></pre>"},{"location":"reference/server/#icon.server.api.models","title":"models","text":""},{"location":"reference/server/#icon.server.api.models.device_dict","title":"device_dict","text":""},{"location":"reference/server/#icon.server.api.models.device_dict.DeviceDict","title":"DeviceDict","text":"<p>               Bases: <code>TypedDict</code></p> <p>Dictionary representation of a device returned by the API.</p>"},{"location":"reference/server/#icon.server.api.models.device_dict.DeviceDict.created","title":"created  <code>instance-attribute</code>","text":"<pre><code>created: str\n</code></pre> <p>Creation timestamp in ISO format.</p>"},{"location":"reference/server/#icon.server.api.models.device_dict.DeviceDict.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description: str | None\n</code></pre> <p>Optional human-readable description.</p>"},{"location":"reference/server/#icon.server.api.models.device_dict.DeviceDict.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: int\n</code></pre> <p>Database identifier of the device.</p>"},{"location":"reference/server/#icon.server.api.models.device_dict.DeviceDict.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>Unique device name.</p>"},{"location":"reference/server/#icon.server.api.models.device_dict.DeviceDict.reachable","title":"reachable  <code>instance-attribute</code>","text":"<pre><code>reachable: bool\n</code></pre> <p>Whether the device is currently connected.</p>"},{"location":"reference/server/#icon.server.api.models.device_dict.DeviceDict.scannable_params","title":"scannable_params  <code>instance-attribute</code>","text":"<pre><code>scannable_params: list[str]\n</code></pre> <p>List of scannable parameter access paths.</p>"},{"location":"reference/server/#icon.server.api.models.device_dict.DeviceDict.status","title":"status  <code>instance-attribute</code>","text":"<pre><code>status: str\n</code></pre> <p>Device status, e.g. \u201cenabled\u201d or \u201cdisabled\u201d.</p>"},{"location":"reference/server/#icon.server.api.models.device_dict.DeviceDict.url","title":"url  <code>instance-attribute</code>","text":"<pre><code>url: str\n</code></pre> <p>pydase server URL of the device.</p>"},{"location":"reference/server/#icon.server.api.models.experiment_dict","title":"experiment_dict","text":""},{"location":"reference/server/#icon.server.api.models.experiment_dict.ExperimentDict","title":"ExperimentDict  <code>module-attribute</code>","text":"<pre><code>ExperimentDict = dict[str, ExperimentMetadata]\n</code></pre> <p>Dictionary mapping the unique experiment identifier to its metadata.</p> Example <pre><code>experiment_dict: ExperimentDict = {\n    \"experiment_library.experiments.my_experiment.MyExperiment (Cool Det)\": ExperimentMetadata(\n        class_name=\"MyExperiment\",\n        constructor_kwargs={\n            \"name\": \"Cool Det\",\n        },\n        parameters={\n            \"Local Parameters\": {\n                \"namespace='experiment_library.experiments.my_experiment.MyExperiment.Cool Det' parameter_group='default' param_type='ParameterTypes.AMPLITUDE'\": {\n                    \"allowed_values\": None,\n                    \"default_value\": 0.0,\n                    \"display_name\": \"amplitude\",\n                    \"max_value\": 100.0,\n                    \"min_value\": 0.0,\n                    \"unit\": \"%\",\n                },\n            },\n            \"ParameterGroup\": {\n                \"namespace='experiment_library.globals.global_parameters' parameter_group='ParameterGroup' param_type='ParameterTypes.AMPLITUDE'\": {\n                    \"allowed_values\": None,\n                    \"default_value\": 0.0,\n                    \"display_name\": \"amplitude\",\n                    \"max_value\": 100.0,\n                    \"min_value\": 0.0,\n                    \"unit\": \"%\",\n                },\n            },\n        },\n    ),\n}\n</code></pre>"},{"location":"reference/server/#icon.server.api.models.experiment_dict.ExperimentMetadata","title":"ExperimentMetadata  <code>dataclass</code>","text":"<pre><code>ExperimentMetadata(\n    class_name: str,\n    constructor_kwargs: dict[str, Any],\n    parameters: dict[str, dict[str, ParameterMetadata]],\n)\n</code></pre> <p>Metadata for a single experiment.</p>"},{"location":"reference/server/#icon.server.api.models.experiment_dict.ExperimentMetadata.class_name","title":"class_name  <code>instance-attribute</code>","text":"<pre><code>class_name: str\n</code></pre> <p>Name of the experiment class.</p>"},{"location":"reference/server/#icon.server.api.models.experiment_dict.ExperimentMetadata.constructor_kwargs","title":"constructor_kwargs  <code>instance-attribute</code>","text":"<pre><code>constructor_kwargs: dict[str, Any]\n</code></pre> <p>Constructor keyword arguments used to instantiate the experiment.</p>"},{"location":"reference/server/#icon.server.api.models.experiment_dict.ExperimentMetadata.parameters","title":"parameters  <code>instance-attribute</code>","text":"<pre><code>parameters: dict[str, dict[str, ParameterMetadata]]\n</code></pre> <p>Mapping of display groups to parameter metadata.</p>"},{"location":"reference/server/#icon.server.api.models.parameter_metadata","title":"parameter_metadata","text":""},{"location":"reference/server/#icon.server.api.models.parameter_metadata.ParameterMetadata","title":"ParameterMetadata","text":"<p>               Bases: <code>TypedDict</code></p> <p>Metadata describing a single parameter.</p>"},{"location":"reference/server/#icon.server.api.models.parameter_metadata.ParameterMetadata.allowed_values","title":"allowed_values  <code>instance-attribute</code>","text":"<pre><code>allowed_values: list[Any] | None\n</code></pre> <p>Explicit list of allowed values (for ComboboxParameters), otherwise None.</p>"},{"location":"reference/server/#icon.server.api.models.parameter_metadata.ParameterMetadata.default_value","title":"default_value  <code>instance-attribute</code>","text":"<pre><code>default_value: float | int\n</code></pre> <p>Default value assigned to the parameter.</p>"},{"location":"reference/server/#icon.server.api.models.parameter_metadata.ParameterMetadata.display_name","title":"display_name  <code>instance-attribute</code>","text":"<pre><code>display_name: str\n</code></pre> <p>Human-readable name of the parameter.</p>"},{"location":"reference/server/#icon.server.api.models.parameter_metadata.ParameterMetadata.max_value","title":"max_value  <code>instance-attribute</code>","text":"<pre><code>max_value: float | None\n</code></pre> <p>Maximum allowed value for the parameter.</p>"},{"location":"reference/server/#icon.server.api.models.parameter_metadata.ParameterMetadata.min_value","title":"min_value  <code>instance-attribute</code>","text":"<pre><code>min_value: float | None\n</code></pre> <p>Minimum allowed value for the parameter.</p>"},{"location":"reference/server/#icon.server.api.models.parameter_metadata.ParameterMetadata.unit","title":"unit  <code>instance-attribute</code>","text":"<pre><code>unit: str\n</code></pre> <p>Unit in which the parameter value is expressed.</p>"},{"location":"reference/server/#icon.server.api.models.scan_parameter","title":"scan_parameter","text":""},{"location":"reference/server/#icon.server.api.models.scan_parameter.DatabaseParameter","title":"DatabaseParameter  <code>dataclass</code>","text":"<pre><code>DatabaseParameter(\n    id: str, values: list[float | int | bool | str]\n)\n</code></pre> <p>Specification of a database parameter to scan during a job.</p>"},{"location":"reference/server/#icon.server.api.models.scan_parameter.DatabaseParameter.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>Unique identifier of the parameter.</p>"},{"location":"reference/server/#icon.server.api.models.scan_parameter.DatabaseParameter.values","title":"values  <code>instance-attribute</code>","text":"<pre><code>values: list[float | int | bool | str]\n</code></pre> <p>List of explicit values to scan for this parameter.</p>"},{"location":"reference/server/#icon.server.api.models.scan_parameter.DeviceParameter","title":"DeviceParameter  <code>dataclass</code>","text":"<pre><code>DeviceParameter(\n    id: str, values: list[float | int], device_name: str\n)\n</code></pre> <p>Specification of a device parameter to scan during a job.</p>"},{"location":"reference/server/#icon.server.api.models.scan_parameter.DeviceParameter.device_name","title":"device_name  <code>instance-attribute</code>","text":"<pre><code>device_name: str\n</code></pre> <p>Name of the device this parameter belongs to.</p>"},{"location":"reference/server/#icon.server.api.models.scan_parameter.DeviceParameter.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>Unique identifier of the parameter.</p>"},{"location":"reference/server/#icon.server.api.models.scan_parameter.DeviceParameter.values","title":"values  <code>instance-attribute</code>","text":"<pre><code>values: list[float | int]\n</code></pre> <p>List of explicit values to scan for this parameter.</p>"},{"location":"reference/server/#icon.server.api.models.scan_parameter.RealtimeParameter","title":"RealtimeParameter  <code>dataclass</code>","text":"<pre><code>RealtimeParameter(n_scan_points: int)\n</code></pre> <p>Specification of the realtime parameter to scan during a job.</p>"},{"location":"reference/server/#icon.server.api.models.scan_parameter.RealtimeParameter.n_scan_points","title":"n_scan_points  <code>instance-attribute</code>","text":"<pre><code>n_scan_points: int\n</code></pre> <p>Number of discrete scan points.</p> <p>If <code>0</code>, the scan is continuous.</p>"},{"location":"reference/server/#icon.server.api.parameters_controller","title":"parameters_controller","text":""},{"location":"reference/server/#icon.server.api.parameters_controller.ParametersController","title":"ParametersController","text":"<pre><code>ParametersController()\n</code></pre> <p>               Bases: <code>DataService</code></p> <p>Controller for parameter metadata and shared parameter values.</p> <p>Maintains metadata for all parameters and their display groups, exposes read/write access to parameter value via the API, and ensures parameters are initialized in the InfluxDB backend.</p> Source code in <code>src/icon/server/api/parameters_controller.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n    self._all_parameter_metadata: dict[str, ParameterMetadata] = {}\n    self._display_group_metadata: dict[str, dict[str, ParameterMetadata]] = {}\n</code></pre>"},{"location":"reference/server/#icon.server.api.parameters_controller.ParametersController.get_all_parameters","title":"get_all_parameters","text":"<pre><code>get_all_parameters() -&gt; dict[str, DatabaseValueType]\n</code></pre> <p>Return the current values of all shared parameters.</p> <p>Returns:</p> Type Description <code>dict[str, DatabaseValueType]</code> <p>Mapping of parameter IDs to their values.</p> Source code in <code>src/icon/server/api/parameters_controller.py</code> <pre><code>def get_all_parameters(self) -&gt; dict[str, DatabaseValueType]:\n    \"\"\"Return the current values of all shared parameters.\n\n    Returns:\n        Mapping of parameter IDs to their values.\n    \"\"\"\n\n    return dict(ParametersRepository.get_shared_parameters())\n</code></pre>"},{"location":"reference/server/#icon.server.api.parameters_controller.ParametersController.get_display_groups","title":"get_display_groups","text":"<pre><code>get_display_groups() -&gt; dict[\n    str, dict[str, ParameterMetadata]\n]\n</code></pre> <p>Return metadata grouped by display group.</p> <p>Returns:</p> Type Description <code>dict[str, dict[str, ParameterMetadata]]</code> <p>Mapping from display group names to parameter metadata.</p> Source code in <code>src/icon/server/api/parameters_controller.py</code> <pre><code>def get_display_groups(self) -&gt; dict[str, dict[str, ParameterMetadata]]:\n    \"\"\"Return metadata grouped by display group.\n\n    Returns:\n        Mapping from display group names to parameter metadata.\n    \"\"\"\n\n    return self._display_group_metadata\n</code></pre>"},{"location":"reference/server/#icon.server.api.parameters_controller.ParametersController.initialise_parameters_repository","title":"initialise_parameters_repository","text":"<pre><code>initialise_parameters_repository() -&gt; None\n</code></pre> <p>Initialize the global <code>ParametersRepository</code>.</p> <p>Loads existing parameters from InfluxDB, populates the shared parameters dict in the shared resource manager, and marks the <code>ParametersRepository</code> as initialized.</p> Source code in <code>src/icon/server/api/parameters_controller.py</code> <pre><code>def initialise_parameters_repository(self) -&gt; None:\n    \"\"\"Initialize the global `ParametersRepository`.\n\n    Loads existing parameters from InfluxDB, populates the shared parameters dict in\n    the shared resource manager, and marks the `ParametersRepository` as\n    initialized.\n    \"\"\"\n\n    icon.server.shared_resource_manager.parameters_dict.update(\n        ParametersRepository.get_influxdb_parameters()\n    )\n    ParametersRepository.initialize(\n        shared_parameters=icon.server.shared_resource_manager.parameters_dict\n    )\n    logger.info(\"ParametersRepository successfully initialised.\")\n</code></pre>"},{"location":"reference/server/#icon.server.api.parameters_controller.ParametersController.update_parameter_by_id","title":"update_parameter_by_id","text":"<pre><code>update_parameter_by_id(\n    parameter_id: str, value: Any\n) -&gt; None\n</code></pre> <p>Update a single parameter value in InfluxDB.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_id</code> <code>str</code> <p>The unique identifier of the parameter.</p> required <code>value</code> <code>Any</code> <p>The new value to assign.</p> required Source code in <code>src/icon/server/api/parameters_controller.py</code> <pre><code>def update_parameter_by_id(self, parameter_id: str, value: Any) -&gt; None:\n    \"\"\"Update a single parameter value in InfluxDB.\n\n    Args:\n        parameter_id: The unique identifier of the parameter.\n        value: The new value to assign.\n    \"\"\"\n\n    ParametersRepository.update_parameters(parameter_mapping={parameter_id: value})\n</code></pre>"},{"location":"reference/server/#icon.server.api.parameters_controller.get_added_removed_and_updated_keys","title":"get_added_removed_and_updated_keys","text":"<pre><code>get_added_removed_and_updated_keys(\n    new_dict: dict[str, Any], cached_dict: dict[str, Any]\n) -&gt; tuple[list[str], list[str], list[str]]\n</code></pre> <p>Compare two dictionaries and return added, removed, and updated keys.</p> <p>Parameters:</p> Name Type Description Default <code>new_dict</code> <code>dict[str, Any]</code> <p>The latest dictionary state.</p> required <code>cached_dict</code> <code>dict[str, Any]</code> <p>The previously cached dictionary state.</p> required <p>Returns:</p> Type Description <code>tuple[list[str], list[str], list[str]]</code> <p>A tuple of three lists:</p> <ul> <li>added keys</li> <li>removed keys</li> <li>updated keys (present in both but with changed values)</li> </ul> Source code in <code>src/icon/server/api/parameters_controller.py</code> <pre><code>def get_added_removed_and_updated_keys(\n    new_dict: dict[str, Any], cached_dict: dict[str, Any]\n) -&gt; tuple[list[str], list[str], list[str]]:\n    \"\"\"Compare two dictionaries and return added, removed, and updated keys.\n\n    Args:\n        new_dict: The latest dictionary state.\n        cached_dict: The previously cached dictionary state.\n\n    Returns:\n        A tuple of three lists:\n\n            - added keys\n            - removed keys\n            - updated keys (present in both but with changed values)\n    \"\"\"\n\n    keys1 = set(cached_dict)\n    keys2 = set(new_dict)\n\n    added_keys = keys2 - keys1\n    removed_keys = keys1 - keys2\n\n    intersect_keys = keys1 &amp; keys2\n    updated_keys = {key for key in intersect_keys if new_dict[key] != cached_dict[key]}\n\n    return list(added_keys), list(removed_keys), list(updated_keys)\n</code></pre>"},{"location":"reference/server/#icon.server.api.scans_controller","title":"scans_controller","text":""},{"location":"reference/server/#icon.server.api.scans_controller.ScansController","title":"ScansController","text":"<pre><code>ScansController(\n    pre_processing_update_queues: list[Queue[UpdateQueue]],\n)\n</code></pre> <p>               Bases: <code>DataService</code></p> <p>Controller for triggering update events for jobs across multiple worker processes.</p> <p>Each worker process has its own update queue (<code>[multiprocessing.Queue][]</code>), which this controller writes to when an update event is triggered.</p> Source code in <code>src/icon/server/api/scans_controller.py</code> <pre><code>def __init__(\n    self,\n    pre_processing_update_queues: list[multiprocessing.Queue[UpdateQueue]],\n) -&gt; None:\n    super().__init__()\n    self._pre_processing_update_queues = pre_processing_update_queues\n</code></pre>"},{"location":"reference/server/#icon.server.api.scans_controller.ScansController.trigger_update_job_params","title":"trigger_update_job_params  <code>async</code>","text":"<pre><code>trigger_update_job_params(\n    *, job_id: int | None = None\n) -&gt; None\n</code></pre> <p>Triggers an \u2018update_parameters\u2019 event for the given job ID.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int | None</code> <p>The ID of the job whose parameters should be updated. If None, all jobs will update their parameters.</p> <code>None</code> Source code in <code>src/icon/server/api/scans_controller.py</code> <pre><code>async def trigger_update_job_params(self, *, job_id: int | None = None) -&gt; None:\n    \"\"\"Triggers an 'update_parameters' event for the given job ID.\n\n    Args:\n        job_id: The ID of the job whose parameters should be updated. If None, all\n            jobs will update their parameters.\n    \"\"\"\n\n    for pre_processing_update_queue in self._pre_processing_update_queues:\n        pre_processing_update_queue.put(\n            {\n                \"event\": \"update_parameters\",\n                \"job_id\": job_id,\n            }\n        )\n</code></pre>"},{"location":"reference/server/#icon.server.api.scheduler_controller","title":"scheduler_controller","text":""},{"location":"reference/server/#icon.server.api.scheduler_controller.SchedulerController","title":"SchedulerController","text":"<pre><code>SchedulerController(devices_controller: DevicesController)\n</code></pre> <p>               Bases: <code>DataService</code></p> <p>Controller to submit, inspect, and cancel scheduled jobs.</p> <p>Provides methods to submit new jobs, cancel pending or running jobs, and query jobs or runs by ID or status. Ensures scan parameters are cast to the correct runtime type before persisting them.</p> <p>Parameters:</p> Name Type Description Default <code>devices_controller</code> <code>DevicesController</code> <p>Reference to the devices controller. Used to read current values of device parameters when casting scan values.</p> required Source code in <code>src/icon/server/api/scheduler_controller.py</code> <pre><code>def __init__(self, devices_controller: DevicesController) -&gt; None:\n    \"\"\"\n    Args:\n        devices_controller: Reference to the devices controller. Used to read\n            current values of device parameters when casting scan values.\n    \"\"\"\n\n    super().__init__()\n    self._devices_controller = devices_controller\n</code></pre>"},{"location":"reference/server/#icon.server.api.scheduler_controller.SchedulerController.cancel_job","title":"cancel_job","text":"<pre><code>cancel_job(*, job_id: int) -&gt; None\n</code></pre> <p>Cancel a queued or running job.</p> <p>The following status updates are performed:</p> <ul> <li>Job: PROCESSING/SUBMITTED \u2192 PROCESSED</li> <li>JobRun: PENDING/PROCESSING \u2192 CANCELLED</li> </ul> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>ID of the job to cancel.</p> required Source code in <code>src/icon/server/api/scheduler_controller.py</code> <pre><code>def cancel_job(self, *, job_id: int) -&gt; None:\n    \"\"\"Cancel a queued or running job.\n\n    The following status updates are performed:\n\n    - Job: PROCESSING/SUBMITTED \u2192 PROCESSED\n    - JobRun: PENDING/PROCESSING \u2192 CANCELLED\n\n    Args:\n        job_id: ID of the job to cancel.\n    \"\"\"\n\n    job = JobRepository.get_job_by_id(job_id=job_id)\n    if job.status in (JobStatus.PROCESSING, JobStatus.SUBMITTED):\n        JobRepository.update_job_status(job=job, status=JobStatus.PROCESSED)\n        job_run = JobRunRepository.get_run_by_job_id(job_id=job_id)\n        if job_run.status in (JobRunStatus.PENDING, JobRunStatus.PROCESSING):\n            JobRunRepository.update_run_by_id(\n                run_id=job_run.id,\n                status=JobRunStatus.CANCELLED,\n                log=\"Cancelled through user interaction.\",\n            )\n</code></pre>"},{"location":"reference/server/#icon.server.api.scheduler_controller.SchedulerController.get_job_by_id","title":"get_job_by_id","text":"<pre><code>get_job_by_id(*, job_id: int) -&gt; Job\n</code></pre> <p>Fetch a job with its experiment source and scan parameters.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>Job identifier.</p> required <p>Returns:</p> Type Description <code>Job</code> <p>The job record.</p> Source code in <code>src/icon/server/api/scheduler_controller.py</code> <pre><code>def get_job_by_id(self, *, job_id: int) -&gt; Job:\n    \"\"\"Fetch a job with its experiment source and scan parameters.\n\n    Args:\n        job_id: Job identifier.\n\n    Returns:\n        The job record.\n    \"\"\"\n\n    return JobRepository.get_job_by_id(\n        job_id=job_id, load_experiment_source=True, load_scan_parameters=True\n    )\n</code></pre>"},{"location":"reference/server/#icon.server.api.scheduler_controller.SchedulerController.get_job_run_by_id","title":"get_job_run_by_id","text":"<pre><code>get_job_run_by_id(*, job_id: int) -&gt; JobRun\n</code></pre> <p>Fetch the run record for a given job.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>Job identifier.</p> required <p>Returns:</p> Type Description <code>JobRun</code> <p>The associated run record.</p> Source code in <code>src/icon/server/api/scheduler_controller.py</code> <pre><code>def get_job_run_by_id(self, *, job_id: int) -&gt; JobRun:\n    \"\"\"Fetch the run record for a given job.\n\n    Args:\n        job_id: Job identifier.\n\n    Returns:\n        The associated run record.\n    \"\"\"\n\n    return JobRunRepository.get_run_by_job_id(job_id=job_id)\n</code></pre>"},{"location":"reference/server/#icon.server.api.scheduler_controller.SchedulerController.get_scheduled_jobs","title":"get_scheduled_jobs","text":"<pre><code>get_scheduled_jobs(\n    *,\n    status: JobStatus | None = None,\n    start: str | None = None,\n    stop: str | None = None,\n) -&gt; dict[int, Job]\n</code></pre> <p>List jobs filtered by status and optional ISO timeframe.</p> <p>Parameters:</p> Name Type Description Default <code>status</code> <code>JobStatus | None</code> <p>Optional job status filter.</p> <code>None</code> <code>start</code> <code>str | None</code> <p>Optional ISO8601 start timestamp (inclusive).</p> <code>None</code> <code>stop</code> <code>str | None</code> <p>Optional ISO8601 stop timestamp (exclusive).</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[int, Job]</code> <p>Mapping from job ID to job record.</p> Source code in <code>src/icon/server/api/scheduler_controller.py</code> <pre><code>def get_scheduled_jobs(\n    self,\n    *,\n    status: JobStatus | None = None,\n    start: str | None = None,\n    stop: str | None = None,\n) -&gt; dict[int, Job]:\n    \"\"\"List jobs filtered by status and optional ISO timeframe.\n\n    Args:\n        status: Optional job status filter.\n        start: Optional ISO8601 start timestamp (inclusive).\n        stop: Optional ISO8601 stop timestamp (exclusive).\n\n    Returns:\n        Mapping from job ID to job record.\n    \"\"\"\n\n    start_date = datetime.fromisoformat(start) if start is not None else None\n    stop_date = datetime.fromisoformat(stop) if stop is not None else None\n\n    return {\n        job.id: job\n        for job in JobRepository.get_jobs_by_status_and_timeframe(\n            status=status, start=start_date, stop=stop_date\n        )\n    }\n</code></pre>"},{"location":"reference/server/#icon.server.api.scheduler_controller.SchedulerController.submit_job","title":"submit_job  <code>async</code>","text":"<pre><code>submit_job(\n    *,\n    experiment_id: str,\n    scan_parameters: list[dict[str, Any]],\n    priority: int = 20,\n    local_parameters_timestamp: datetime | None = None,\n    repetitions: int = 1,\n    number_of_shots: int = 50,\n    git_commit_hash: str | None = None,\n    auto_calibration: bool = False,\n) -&gt; int\n</code></pre> <p>Create and submit a job with typed scan parameters.</p> <p>Each scan parameter\u2019s values are cast to the current type of the target parameter (device parameter via <code>DevicesController</code> or shared parameter via <code>ParametersRepository</code>).</p> <p>Parameters:</p> Name Type Description Default <code>experiment_id</code> <code>str</code> <p>Experiment identifier (from experiment library).</p> required <code>scan_parameters</code> <code>list[dict[str, Any]]</code> <p>List of scan parameter specs (id, values, optional device_name).</p> required <code>priority</code> <code>int</code> <p>Higher values run sooner.</p> <code>20</code> <code>local_parameters_timestamp</code> <code>datetime | None</code> <p>ISO timestamp to snapshot local parameters; defaults to <code>datetime.now(tz=timezone)</code>.</p> <code>None</code> <code>repetitions</code> <code>int</code> <p>Number of experiment repetitions.</p> <code>1</code> <code>number_of_shots</code> <code>int</code> <p>Shots per data point.</p> <code>50</code> <code>git_commit_hash</code> <code>str | None</code> <p>Git commit to associate with the job; if <code>None</code>, job is marked <code>debug_mode=True</code>.</p> <code>None</code> <code>auto_calibration</code> <code>bool</code> <p>Whether to run auto-calibration for the job.</p> <code>False</code> <p>Returns:</p> Type Description <code>int</code> <p>The persisted job ID.</p> Source code in <code>src/icon/server/api/scheduler_controller.py</code> <pre><code>async def submit_job(  # noqa: PLR0913\n    self,\n    *,\n    experiment_id: str,\n    scan_parameters: list[dict[str, Any]],\n    priority: int = 20,\n    local_parameters_timestamp: datetime | None = None,\n    repetitions: int = 1,\n    number_of_shots: int = 50,\n    git_commit_hash: str | None = None,\n    auto_calibration: bool = False,\n) -&gt; int:\n    \"\"\"Create and submit a job with typed scan parameters.\n\n    Each scan parameter's values are cast to the current type of the target\n    parameter (device parameter via `DevicesController` or shared parameter via\n    `ParametersRepository`).\n\n    Args:\n        experiment_id: Experiment identifier (from experiment library).\n        scan_parameters: List of scan parameter specs (id, values, optional\n            device_name).\n        priority: Higher values run sooner.\n        local_parameters_timestamp: ISO timestamp to snapshot local parameters;\n            defaults to `datetime.now(tz=timezone)`.\n        repetitions: Number of experiment repetitions.\n        number_of_shots: Shots per data point.\n        git_commit_hash: Git commit to associate with the job; if `None`, job is\n            marked `debug_mode=True`.\n        auto_calibration: Whether to run auto-calibration for the job.\n\n    Returns:\n        The persisted job ID.\n    \"\"\"\n\n    if local_parameters_timestamp is None:\n        local_parameters_timestamp = datetime.now(tz=timezone)\n\n    experiment_source = ExperimentSource(experiment_id=experiment_id)\n\n    experiment_source = ExperimentSourceRepository.get_or_create_experiment(\n        experiment_source=experiment_source\n    )\n\n    def to_sqlite_model(\n        param: ScanParameter,\n    ) -&gt; sqlite_scan_parameter.ScanParameter:\n        if isinstance(param, RealtimeParameter):\n            return sqlite_scan_parameter.ScanParameter(\n                variable_id=\"Real Time\",\n                scan_values=[1] * param.n_scan_points,\n                realtime=True,\n            )\n        if isinstance(param, DatabaseParameter):\n            return sqlite_scan_parameter.ScanParameter(\n                variable_id=param.id,\n                scan_values=param.values,\n            )\n        return sqlite_scan_parameter.ScanParameter(\n            variable_id=param.id,\n            scan_values=param.values,\n            device_id=DeviceRepository.get_device_by_name(\n                name=param.device_name\n            ).id,\n        )\n\n    concretized_params = [\n        scan_parameter_from_dict(\n            {**param, \"values\": await self._cast_scan_values_to_param_type(**param)}\n        )\n        for param in scan_parameters\n    ]\n    realtime_params = [\n        param\n        for param in concretized_params\n        if isinstance(param, RealtimeParameter)\n    ]\n    if len(realtime_params) &gt; 1:\n        raise ValueError(\"Only 0 or 1 realtime parameter is allowed\")\n    if (\n        realtime_params\n        and realtime_params[0].n_scan_points == 0\n        and repetitions &gt; 1\n    ):\n        raise ValueError(\n            \"Only 1 repetition possible if continuous realtime is present\"\n        )\n    sqlite_scan_parameters = [\n        to_sqlite_model(param) for param in concretized_params\n    ]\n\n    job = Job(\n        experiment_source=experiment_source,\n        priority=priority,\n        local_parameters_timestamp=local_parameters_timestamp,\n        scan_parameters=sqlite_scan_parameters,\n        repetitions=repetitions,\n        git_commit_hash=git_commit_hash,\n        number_of_shots=number_of_shots,\n        auto_calibration=auto_calibration,\n        debug_mode=git_commit_hash is None,\n    )\n    job = JobRepository.submit_job(job=job)\n\n    return job.id\n</code></pre>"},{"location":"reference/server/#icon.server.api.status_controller","title":"status_controller","text":""},{"location":"reference/server/#icon.server.api.status_controller.StatusController","title":"StatusController","text":"<pre><code>StatusController()\n</code></pre> <p>               Bases: <code>DataService</code></p> <p>Controller for system status monitoring.</p> <p>Periodically checks availability of InfluxDB and hardware and emits status events via the Socket.IO queue.</p> Source code in <code>src/icon/server/api/status_controller.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n    self.__hardware_controller = HardwareController(connect=False)\n    self._influxdb_available = False\n    self._hardware_available = False\n</code></pre>"},{"location":"reference/server/#icon.server.api.status_controller.StatusController.check_hardware_status","title":"check_hardware_status  <code>async</code>","text":"<pre><code>check_hardware_status() -&gt; None\n</code></pre> <p>Check hardware connection and reconnect if necessary.</p> <p>Ensures the hardware controller matches the configured host/port and reconnects in a background thread if required.</p> <p>Emits a <code>\"status.hardware\"</code> event to the Socket.IO queue.</p> Source code in <code>src/icon/server/api/status_controller.py</code> <pre><code>async def check_hardware_status(self) -&gt; None:\n    \"\"\"Check hardware connection and reconnect if necessary.\n\n    Ensures the hardware controller matches the configured host/port and reconnects\n    in a background thread if required.\n\n    Emits a `\"status.hardware\"` event to the Socket.IO queue.\n    \"\"\"\n\n    status = self.__hardware_controller.connected\n\n    if (\n        not status\n        or self.__hardware_controller._host != get_config().hardware.host\n        or self.__hardware_controller._port != get_config().hardware.port\n    ):\n        await asyncio.to_thread(self.__hardware_controller.connect)\n\n    self._hardware_available = status\n    emit_queue.put({\"event\": \"status.hardware\", \"data\": status})\n</code></pre>"},{"location":"reference/server/#icon.server.api.status_controller.StatusController.check_influxdb_status","title":"check_influxdb_status","text":"<pre><code>check_influxdb_status() -&gt; None\n</code></pre> <p>Check if InfluxDB is responsive and update status.</p> <p>Emits a <code>\"status.influxdb\"</code> event to the Socket.IO queue.</p> Source code in <code>src/icon/server/api/status_controller.py</code> <pre><code>def check_influxdb_status(self) -&gt; None:\n    \"\"\"Check if InfluxDB is responsive and update status.\n\n    Emits a `\"status.influxdb\"` event to the Socket.IO queue.\n    \"\"\"\n\n    status = influxdb_v1.is_responsive()\n\n    self._influxdb_available = status\n    emit_queue.put({\"event\": \"status.influxdb\", \"data\": status})\n</code></pre>"},{"location":"reference/server/#icon.server.api.status_controller.StatusController.get_status","title":"get_status","text":"<pre><code>get_status() -&gt; dict[str, bool]\n</code></pre> <p>Return the current system status flags.</p> <p>Returns:</p> Type Description <code>dict[str, bool]</code> <p>A dictionary with:</p> <ul> <li><code>\"influxdb\"</code>: Whether InfluxDB is responsive.</li> <li><code>\"hardware\"</code>: Whether the hardware connection is active.</li> </ul> Source code in <code>src/icon/server/api/status_controller.py</code> <pre><code>def get_status(self) -&gt; dict[str, bool]:\n    \"\"\"Return the current system status flags.\n\n    Returns:\n        A dictionary with:\n\n            - `\"influxdb\"`: Whether InfluxDB is responsive.\n            - `\"hardware\"`: Whether the hardware connection is active.\n    \"\"\"\n\n    return {\n        \"influxdb\": self._influxdb_available,\n        \"hardware\": self._hardware_available,\n    }\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.models.enums","title":"icon.server.data_access.models.enums","text":"<p>This module defines enums used by the SQLAlchemy models.</p> <p>These enums represent database-level states for jobs, job runs, and devices. They are stored as strings in the database and used throughout ICON\u2019s scheduling and device management logic.</p>"},{"location":"reference/server/#icon.server.data_access.models.enums.DeviceStatus","title":"DeviceStatus","text":"<p>               Bases: <code>Enum</code></p> <p>Operational states of a device.</p>"},{"location":"reference/server/#icon.server.data_access.models.enums.DeviceStatus.DISABLED","title":"DISABLED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DISABLED = 'disabled'\n</code></pre> <p>Device is disabled and should not be used.</p>"},{"location":"reference/server/#icon.server.data_access.models.enums.DeviceStatus.ENABLED","title":"ENABLED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ENABLED = 'enabled'\n</code></pre> <p>Device is enabled and may be connected.</p>"},{"location":"reference/server/#icon.server.data_access.models.enums.JobRunStatus","title":"JobRunStatus","text":"<p>               Bases: <code>Enum</code></p> <p>Lifecycle states of a job run.</p>"},{"location":"reference/server/#icon.server.data_access.models.enums.JobRunStatus.CANCELLED","title":"CANCELLED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CANCELLED = 'cancelled'\n</code></pre> <p>Run was cancelled before completion.</p>"},{"location":"reference/server/#icon.server.data_access.models.enums.JobRunStatus.DONE","title":"DONE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DONE = 'done'\n</code></pre> <p>Run completed successfully.</p>"},{"location":"reference/server/#icon.server.data_access.models.enums.JobRunStatus.FAILED","title":"FAILED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FAILED = 'failed'\n</code></pre> <p>Run ended unsuccessfully due to an error.</p>"},{"location":"reference/server/#icon.server.data_access.models.enums.JobRunStatus.PENDING","title":"PENDING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PENDING = 'pending'\n</code></pre> <p>Run is queued but has not started yet.</p>"},{"location":"reference/server/#icon.server.data_access.models.enums.JobRunStatus.PROCESSING","title":"PROCESSING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PROCESSING = 'processing'\n</code></pre> <p>Run is currently executing.</p>"},{"location":"reference/server/#icon.server.data_access.models.enums.JobStatus","title":"JobStatus","text":"<p>               Bases: <code>Enum</code></p> <p>Lifecycle states of a job submission.</p>"},{"location":"reference/server/#icon.server.data_access.models.enums.JobStatus.PROCESSED","title":"PROCESSED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PROCESSED = 'processed'\n</code></pre> <p>Job has finished or was cancelled and is no longer active.</p>"},{"location":"reference/server/#icon.server.data_access.models.enums.JobStatus.PROCESSING","title":"PROCESSING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PROCESSING = 'processing'\n</code></pre> <p>Job has been put into the pre-processing task queue.</p>"},{"location":"reference/server/#icon.server.data_access.models.enums.JobStatus.SUBMITTED","title":"SUBMITTED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SUBMITTED = 'submitted'\n</code></pre> <p>Job has been created and is waiting to be scheduled.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite","title":"icon.server.data_access.models.sqlite","text":"<p>This module contains the SQLAlchemy models for ICON.</p> <p>All models must be imported and added to the <code>__all__</code> list here so that Alembic can correctly detect them during schema autogeneration. Alembic inspects <code>Base.metadata</code>, which is only populated with models that are actually imported at runtime.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Base","title":"Base","text":"<p>               Bases: <code>DeclarativeBase</code></p> <p>Base class for all SQLAlchemy ORM models in ICON.</p> <p>This class configures the declarative mapping and provides a datetime type mapping for all models that inherit from it.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Base.type_annotation_map","title":"type_annotation_map  <code>class-attribute</code>","text":"<pre><code>type_annotation_map: dict[type, Any] = {\n    datetime: TIMESTAMP(timezone=True)\n}\n</code></pre> <p>Custom type mapping used when interpreting Python type annotations.</p> <p>Currently, <code>datetime.datetime</code> is mapped to <code>sqlalchemy.TIMESTAMP(timezone=True)</code> to ensure timezone-aware timestamps across all models.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Device","title":"Device","text":"<p>               Bases: <code>Base</code></p> <p>SQLAlchemy model for a registered device.</p> <p>Represents an external device accessible via a pydase service. Stores configuration, connection details, and retry behaviour. A device may be linked to multiple scan parameters.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Device.created","title":"created  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>created: Mapped[datetime] = mapped_column(\n    default=lambda: now(timezone)\n)\n</code></pre> <p>Timestamp when the device entry was created.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Device.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description: Mapped[str | None] = mapped_column(\n    default=None\n)\n</code></pre> <p>Optional human-readable description of the device.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Device.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: Mapped[int] = mapped_column(\n    primary_key=True, autoincrement=True\n)\n</code></pre> <p>Primary key identifier for the device.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Device.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: Mapped[str] = mapped_column(unique=True, index=True)\n</code></pre> <p>Unique name of the device.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Device.retry_attempts","title":"retry_attempts  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>retry_attempts: Mapped[int] = mapped_column(\n    default=3, nullable=False\n)\n</code></pre> <p>Number of attempts to verify the device value was set correctly.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Device.retry_delay_seconds","title":"retry_delay_seconds  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>retry_delay_seconds: Mapped[float] = mapped_column(\n    default=0.0, nullable=False\n)\n</code></pre> <p>Delay in seconds between retry attempts</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Device.scan_parameters","title":"scan_parameters  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scan_parameters: Mapped[list[ScanParameter]] = relationship(\n    \"ScanParameter\", back_populates=\"device\"\n)\n</code></pre> <p>Relationship to scan parameters linked to this device.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Device.status","title":"status  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status: Mapped[DeviceStatus] = mapped_column(\n    default=ENABLED, index=True\n)\n</code></pre> <p>Current status of the device (enabled or disabled).</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Device.url","title":"url  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>url: Mapped[str] = mapped_column()\n</code></pre> <p>pydase service URL of the device.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.ExperimentSource","title":"ExperimentSource","text":"<p>               Bases: <code>Base</code></p> <p>SQLAlchemy model for experiment sources.</p> <p>Represents a unique experiment identifier from the experiment library. Each experiment source may be linked to multiple jobs.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.ExperimentSource.experiment_id","title":"experiment_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>experiment_id: Mapped[str] = mapped_column()\n</code></pre> <p>Unique experiment identifier string (as defined in the experiment library).</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.ExperimentSource.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: Mapped[int] = mapped_column(\n    primary_key=True, autoincrement=True\n)\n</code></pre> <p>Primary key identifier for the experiment source.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.ExperimentSource.jobs","title":"jobs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>jobs: Mapped[list[Job]] = relationship(\n    back_populates=\"experiment_source\"\n)\n</code></pre> <p>Relationship to jobs associated with this experiment source.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Job","title":"Job","text":"<p>               Bases: <code>Base</code></p> <p>SQLAlchemy model for experiment jobs.</p> <p>Represents a scheduled or running experiment job, including its metadata, status, and relationships to experiment sources, runs, and scan parameters.</p> Constraints <ul> <li><code>priority</code> must be between 0 and 20.</li> <li>Indexed by <code>(experiment_source_id, status, priority, created)</code>.</li> </ul>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Job.auto_calibration","title":"auto_calibration  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>auto_calibration: Mapped[bool] = mapped_column(\n    default=False\n)\n</code></pre> <p>Whether auto-calibration is enabled for this job. Currently unused.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Job.created","title":"created  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>created: Mapped[datetime] = mapped_column(\n    default=lambda: now(timezone)\n)\n</code></pre> <p>Timestamp when the job was created. This cannot be set manually.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Job.debug_mode","title":"debug_mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>debug_mode: Mapped[bool] = mapped_column(default=False)\n</code></pre> <p>Whether the job was submitted in debug mode (no commit hash).</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Job.experiment_source","title":"experiment_source  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>experiment_source: Mapped[ExperimentSource] = relationship(\n    back_populates=\"jobs\"\n)\n</code></pre> <p>Relationship to the experiment source.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Job.experiment_source_id","title":"experiment_source_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>experiment_source_id: Mapped[int] = mapped_column(\n    ForeignKey(\"experiment_sources.id\")\n)\n</code></pre> <p>Foreign key referencing the associated experiment source.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Job.git_commit_hash","title":"git_commit_hash  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>git_commit_hash: Mapped[str | None] = mapped_column(\n    default=None\n)\n</code></pre> <p>Git commit hash of the experiment code associated with the job.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Job.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: Mapped[int] = mapped_column(\n    primary_key=True, autoincrement=True\n)\n</code></pre> <p>Primary key identifier for the job.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Job.local_parameters_timestamp","title":"local_parameters_timestamp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>local_parameters_timestamp: Mapped[datetime] = (\n    mapped_column(default=now(timezone))\n)\n</code></pre> <p>Timestamp of the local parameter snapshot used for this job.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Job.number_of_shots","title":"number_of_shots  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>number_of_shots: Mapped[int] = mapped_column(default=50)\n</code></pre> <p>Number of shots per repetition.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Job.parent_job","title":"parent_job  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>parent_job: Mapped[Job | None] = relationship(\n    \"Job\",\n    remote_side=[id],\n    back_populates=\"resubmitted_jobs\",\n)\n</code></pre> <p>Relationship to the parent job from which this job was resubmitted.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Job.parent_job_id","title":"parent_job_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>parent_job_id: Mapped[int | None] = mapped_column(\n    ForeignKey(\"job_submissions.id\"), nullable=True\n)\n</code></pre> <p>Foreign key referencing the original job if this job was resubmitted.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Job.priority","title":"priority  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>priority: Mapped[int] = mapped_column(default=20)\n</code></pre> <p>Job priority, between 0 (lowest) and 20 (highest).</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Job.repetitions","title":"repetitions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>repetitions: Mapped[int] = mapped_column(default=1)\n</code></pre> <p>Number of times the experiment should be repeated.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Job.resubmitted_jobs","title":"resubmitted_jobs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>resubmitted_jobs: Mapped[list[Job]] = relationship(\n    \"Job\", back_populates=\"parent_job\"\n)\n</code></pre> <p>List of jobs resubmitted from this job.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Job.run","title":"run  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>run: Mapped[JobRun] = relationship(back_populates='job')\n</code></pre> <p>Relationship to the job run associated with this job.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Job.scan_parameters","title":"scan_parameters  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scan_parameters: Mapped[list[ScanParameter]] = relationship(\n    back_populates=\"job\"\n)\n</code></pre> <p>List of scan parameters associated with this job.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.Job.status","title":"status  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status: Mapped[JobStatus] = mapped_column(default=SUBMITTED)\n</code></pre> <p>Current status of the job (submitted, processing, etc.).</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.JobRun","title":"JobRun","text":"<p>               Bases: <code>Base</code></p> <p>SQLAlchemy model for job runs.</p> <p>Represents the execution of a job, including its scheduled time, current status, and log messages.</p> Constraints <ul> <li>Indexed by <code>(job_id, status, scheduled_time)</code>.</li> <li><code>scheduled_time</code> must be unique across runs.</li> </ul>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.JobRun.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: Mapped[int] = mapped_column(\n    primary_key=True, autoincrement=True\n)\n</code></pre> <p>Primary key identifier for the job run.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.JobRun.job","title":"job  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>job: Mapped[Job] = relationship(back_populates='run')\n</code></pre> <p>Relationship to the job associated with this run.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.JobRun.job_id","title":"job_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>job_id: Mapped[int] = mapped_column(\n    ForeignKey(\"job_submissions.id\")\n)\n</code></pre> <p>Foreign key referencing the job being executed.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.JobRun.log","title":"log  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log: Mapped[str | None] = mapped_column(default=None)\n</code></pre> <p>Optional log message for this run (e.g., cancellation reason).</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.JobRun.parameter_update_timestamp","title":"parameter_update_timestamp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>parameter_update_timestamp: Mapped[datetime | None] = (\n    mapped_column(default=None)\n)\n</code></pre> <p>Timestamp of the last parameter update.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.JobRun.scheduled_time","title":"scheduled_time  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scheduled_time: Mapped[datetime] = mapped_column(\n    default=now(timezone)\n)\n</code></pre> <p>Time when the run was scheduled to start.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.JobRun.status","title":"status  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status: Mapped[JobRunStatus] = mapped_column(\n    default=PENDING\n)\n</code></pre> <p>Current status of the run (pending, processing, cancelled, etc.).</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.ScanParameter","title":"ScanParameter","text":"<p>               Bases: <code>Base</code></p> <p>SQLAlchemy model for scan parameters.</p> <p>Represents a parameter scanned during a job execution. Each parameter is linked to a job and optionally to a device.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.ScanParameter.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: Mapped[Device | None] = relationship(\n    back_populates=\"scan_parameters\", lazy=\"joined\"\n)\n</code></pre> <p>Relationship to the device associated with this parameter.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.ScanParameter.device_id","title":"device_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device_id: Mapped[int | None] = mapped_column(\n    ForeignKey(\"devices.id\"), nullable=True\n)\n</code></pre> <p>Foreign key referencing the associated device, if any.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.ScanParameter.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: Mapped[int] = mapped_column(\n    primary_key=True, autoincrement=True\n)\n</code></pre> <p>Primary key identifier for the scan parameter.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.ScanParameter.job","title":"job  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>job: Mapped[Job] = relationship(\n    back_populates=\"scan_parameters\"\n)\n</code></pre> <p>Relationship to the job.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.ScanParameter.job_id","title":"job_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>job_id: Mapped[int] = mapped_column(\n    ForeignKey(\"job_submissions.id\")\n)\n</code></pre> <p>Foreign key referencing the job this parameter belongs to.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.ScanParameter.scan_values","title":"scan_values  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scan_values: Mapped[list[DatabaseValueType]] = (\n    mapped_column(JSONEncodedList, nullable=False)\n)\n</code></pre> <p>List of values scanned for this parameter (stored as JSON).</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.ScanParameter.variable_id","title":"variable_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>variable_id: Mapped[str] = mapped_column()\n</code></pre> <p>Identifier of the parameter being scanned.</p>"},{"location":"reference/server/#icon.server.data_access.models.sqlite.ScanParameter.unique_id","title":"unique_id","text":"<pre><code>unique_id() -&gt; str\n</code></pre> <p>Return a unique identifier for the parameter.</p> <p>Returns:</p> Type Description <code>str</code> <p><code>\"Device(&lt;device_name&gt;) &lt;variable_id&gt;\"</code> if a device is associated, otherwise just <code>&lt;variable_id&gt;</code>.</p> Source code in <code>src/icon/server/data_access/models/sqlite/scan_parameter.py</code> <pre><code>def unique_id(self) -&gt; str:\n    \"\"\"Return a unique identifier for the parameter.\n\n    Returns:\n        `\"Device(&lt;device_name&gt;) &lt;variable_id&gt;\"` if a device is associated, otherwise\n            just `&lt;variable_id&gt;`.\n    \"\"\"\n\n    return (\n        f\"Device({self.device.name}) {self.variable_id}\"\n        if self.device is not None\n        else self.variable_id\n    )\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories","title":"icon.server.data_access.repositories","text":"<p>This module contains the repository layer for ICON\u2019s data access.</p> <p>Repositories encapsulate database access logic and hide the underlying persistence technology (SQLAlchemy sessions, InfluxDB queries, etc.) from the rest of the application. They expose simple, intention-revealing methods for creating, retrieving, and updating domain objects, while emitting Socket.IO events when relevant.</p> <p>By using repositories, controllers and services can work with high-level operations (e.g. \u201csubmit a job\u201d, \u201cupdate a device\u201d) without needing to know how the data is stored or which database backend is used. This keeps the codebase modular, easier to maintain, and allows the persistence layer to evolve independently of business logic.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.device_repository","title":"device_repository","text":""},{"location":"reference/server/#icon.server.data_access.repositories.device_repository.DeviceRepository","title":"DeviceRepository","text":"<p>Repository for <code>Device</code> entities.</p> <p>Provides methods to create, update, and query devices in the SQLite database. All methods open their own SQLAlchemy session and return detached ORM objects.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.device_repository.DeviceRepository.add_device","title":"add_device  <code>staticmethod</code>","text":"<pre><code>add_device(*, device: Device) -&gt; Device\n</code></pre> <p>Insert a new device into the database.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Device</code> <p>Device instance to persist.</p> required <p>Returns:</p> Type Description <code>Device</code> <p>The persisted device with database-generated fields (e.g., <code>id</code>) populated.</p> Source code in <code>src/icon/server/data_access/repositories/device_repository.py</code> <pre><code>@staticmethod\ndef add_device(*, device: Device) -&gt; Device:\n    \"\"\"Insert a new device into the database.\n\n    Args:\n        device: Device instance to persist.\n\n    Returns:\n        The persisted device with database-generated fields (e.g., `id`) populated.\n    \"\"\"\n\n    with sqlalchemy.orm.session.Session(engine) as session:\n        session.add(device)\n        session.commit()\n        session.refresh(device)\n        logger.debug(\"Added new device %s\", device)\n\n    emit_queue.put(\n        {\n            \"event\": \"device.new\",\n            \"data\": {\n                \"device\": SQLAlchemyDictEncoder.encode(obj=device),\n            },\n        }\n    )\n\n    return device\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.device_repository.DeviceRepository.get_all_device_names","title":"get_all_device_names  <code>staticmethod</code>","text":"<pre><code>get_all_device_names() -&gt; Sequence[str]\n</code></pre> <p>Return the names of all devices.</p> <p>Returns:</p> Type Description <code>Sequence[str]</code> <p>List of device names.</p> Source code in <code>src/icon/server/data_access/repositories/device_repository.py</code> <pre><code>@staticmethod\ndef get_all_device_names() -&gt; Sequence[str]:\n    \"\"\"Return the names of all devices.\n\n    Returns:\n        List of device names.\n    \"\"\"\n\n    with sqlalchemy.orm.Session(engine) as session:\n        stmt = sqlalchemy.select(Device.name)\n        return session.execute(stmt).scalars().all()\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.device_repository.DeviceRepository.get_device_by_id","title":"get_device_by_id  <code>staticmethod</code>","text":"<pre><code>get_device_by_id(*, id: int) -&gt; Device\n</code></pre> <p>Return a device by database ID.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>int</code> <p>Primary key identifier of the device.</p> required <p>Returns:</p> Type Description <code>Device</code> <p>The matching device.</p> <p>Raises:</p> Type Description <code>NoResultFound</code> <p>If no device exists with the given ID.</p> Source code in <code>src/icon/server/data_access/repositories/device_repository.py</code> <pre><code>@staticmethod\ndef get_device_by_id(*, id: int) -&gt; Device:\n    \"\"\"Return a device by database ID.\n\n    Args:\n        id: Primary key identifier of the device.\n\n    Returns:\n        The matching device.\n\n    Raises:\n        NoResultFound: If no device exists with the given ID.\n    \"\"\"\n\n    with sqlalchemy.orm.Session(engine) as session:\n        stmt = sqlalchemy.select(Device).where(Device.id == id)\n        return session.execute(stmt).scalar_one()\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.device_repository.DeviceRepository.get_device_by_name","title":"get_device_by_name  <code>staticmethod</code>","text":"<pre><code>get_device_by_name(*, name: str) -&gt; Device\n</code></pre> <p>Return a device by unique name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Device name.</p> required <p>Returns:</p> Type Description <code>Device</code> <p>The matching device.</p> <p>Raises:</p> Type Description <code>NoDeviceFoundError</code> <p>If no device exists with the given name.</p> Source code in <code>src/icon/server/data_access/repositories/device_repository.py</code> <pre><code>@staticmethod\ndef get_device_by_name(*, name: str) -&gt; Device:\n    \"\"\"Return a device by unique name.\n\n    Args:\n        name: Device name.\n\n    Returns:\n        The matching device.\n\n    Raises:\n        NoDeviceFoundError: If no device exists with the given name.\n    \"\"\"\n\n    try:\n        with sqlalchemy.orm.Session(engine) as session:\n            stmt = sqlalchemy.select(Device).where(Device.name == name)\n            return session.execute(stmt).scalar_one()\n    except sqlalchemy.exc.NoResultFound:\n        raise NoDeviceFoundError(\n            f\"Device with name {name!r} does not exist.\",\n        )\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.device_repository.DeviceRepository.get_devices_by_status","title":"get_devices_by_status  <code>staticmethod</code>","text":"<pre><code>get_devices_by_status(\n    *, status: DeviceStatus | None = None\n) -&gt; Sequence[Device]\n</code></pre> <p>Return devices filtered by status.</p> <p>Parameters:</p> Name Type Description Default <code>status</code> <code>DeviceStatus | None</code> <p>Optional device status to filter on.</p> <code>None</code> <p>Returns:</p> Type Description <code>Sequence[Device]</code> <p>All devices matching the filter (or all devices if no filter is given).</p> Source code in <code>src/icon/server/data_access/repositories/device_repository.py</code> <pre><code>@staticmethod\ndef get_devices_by_status(\n    *,\n    status: DeviceStatus | None = None,\n) -&gt; Sequence[Device]:\n    \"\"\"Return devices filtered by status.\n\n    Args:\n        status: Optional device status to filter on.\n\n    Returns:\n        All devices matching the filter (or all devices if no filter is given).\n    \"\"\"\n\n    with sqlalchemy.orm.Session(engine) as session:\n        stmt = sqlalchemy.select(Device)\n\n        if status is not None:\n            stmt = stmt.where(Device.status == status)\n\n        return session.execute(stmt).scalars().all()\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.device_repository.DeviceRepository.update_device","title":"update_device  <code>staticmethod</code>","text":"<pre><code>update_device(\n    *,\n    name: str,\n    url: str | None = None,\n    status: DeviceStatus | None = None,\n    retry_attempts: int | None = None,\n    retry_delay_seconds: float | None = None,\n) -&gt; Device\n</code></pre> <p>Update an existing device by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique device name.</p> required <code>url</code> <code>str | None</code> <p>New device URL (cannot change if the device is enabled).</p> <code>None</code> <code>status</code> <code>DeviceStatus | None</code> <p>New device status (enabled/disabled).</p> <code>None</code> <code>retry_attempts</code> <code>int | None</code> <p>Updated retry attempt count.</p> <code>None</code> <code>retry_delay_seconds</code> <code>float | None</code> <p>Updated retry delay in seconds.</p> <code>None</code> <p>Returns:</p> Type Description <code>Device</code> <p>The updated device.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If attempting to change the URL of an enabled device.</p> <code>NoDeviceFoundError</code> <p>If no device with the given name exists.</p> Source code in <code>src/icon/server/data_access/repositories/device_repository.py</code> <pre><code>@staticmethod\ndef update_device(\n    *,\n    name: str,\n    url: str | None = None,\n    status: DeviceStatus | None = None,\n    retry_attempts: int | None = None,\n    retry_delay_seconds: float | None = None,\n) -&gt; Device:\n    \"\"\"Update an existing device by name.\n\n    Args:\n        name: Unique device name.\n        url: New device URL (cannot change if the device is enabled).\n        status: New device status (enabled/disabled).\n        retry_attempts: Updated retry attempt count.\n        retry_delay_seconds: Updated retry delay in seconds.\n\n    Returns:\n        The updated device.\n\n    Raises:\n        RuntimeError: If attempting to change the URL of an enabled device.\n        NoDeviceFoundError: If no device with the given name exists.\n    \"\"\"\n\n    updated_properties = {\n        name: new_value\n        for name, new_value in {\n            \"url\": url,\n            \"status\": status if status is not None else None,\n            \"retry_attempts\": retry_attempts,\n            \"retry_delay_seconds\": retry_delay_seconds,\n        }.items()\n        if new_value is not None\n    }\n\n    if \"url\" in updated_properties:\n        device = DeviceRepository.get_device_by_name(name=name)\n        if device.status == DeviceStatus.ENABLED:\n            raise RuntimeError(\"Cannot change url of an enabled device\")\n\n    with sqlalchemy.orm.Session(engine) as session:\n        session.execute(\n            update(Device).where(Device.name == name).values(updated_properties)\n        )\n        session.commit()\n\n        device = session.execute(\n            select(Device).where(Device.name == name)\n        ).scalar_one()\n        session.expunge(device)\n\n        logger.debug(\"Updated device %s\", device)\n\n    serialized_properties = {\n        key: value.value if isinstance(value, enum.Enum) else value\n        for key, value in updated_properties.items()\n    }\n\n    if \"status\" in updated_properties:\n        serialized_properties[\"reachable\"] = False\n\n    emit_queue.put(\n        {\n            \"event\": \"device.update\",\n            \"data\": {\n                \"device_name\": device.name,\n                \"updated_properties\": serialized_properties,\n            },\n        }\n    )\n\n    return device\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.device_repository.NoDeviceFoundError","title":"NoDeviceFoundError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a device could not be found by the given identifier.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository","title":"experiment_data_repository","text":""},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ExperimentData","title":"ExperimentData  <code>dataclass</code>","text":"<pre><code>ExperimentData(\n    plot_windows: PlotWindowsDict,\n    shot_channels: dict[str, dict[int, list[int]]],\n    result_channels: dict[str, dict[int, float]],\n    vector_channels: dict[str, dict[int, list[float]]],\n    scan_parameters: dict[str, dict[int, str | float]],\n    json_sequences: list[list[int | str]],\n    realtime_scan: bool,\n    parameters: dict[str, ParameterValue],\n)\n</code></pre> <p>Container for all experiment data returned to the API.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ExperimentData.json_sequences","title":"json_sequences  <code>instance-attribute</code>","text":"<pre><code>json_sequences: list[list[int | str]]\n</code></pre> <p>List of [index, sequence_json] pairs (list for pydase JSON compatibility).</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ExperimentData.parameters","title":"parameters  <code>instance-attribute</code>","text":"<pre><code>parameters: dict[str, ParameterValue]\n</code></pre> <p>Mapping of parameter id to time series (tuple of timestamp str and value).</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ExperimentData.plot_windows","title":"plot_windows  <code>instance-attribute</code>","text":"<pre><code>plot_windows: PlotWindowsDict\n</code></pre> <p>Plot window metadata grouped by channel class.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ExperimentData.realtime_scan","title":"realtime_scan  <code>instance-attribute</code>","text":"<pre><code>realtime_scan: bool\n</code></pre> <p>True if the experiment has a realtime scan parameter.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ExperimentData.result_channels","title":"result_channels  <code>instance-attribute</code>","text":"<pre><code>result_channels: dict[str, dict[int, float]]\n</code></pre> <p>Result channels as channel_name -&gt; {index -&gt; value}.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ExperimentData.scan_parameters","title":"scan_parameters  <code>instance-attribute</code>","text":"<pre><code>scan_parameters: dict[str, dict[int, str | float]]\n</code></pre> <p>Scan parameters as param_id -&gt; {index -&gt; value/timestamp}.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ExperimentData.shot_channels","title":"shot_channels  <code>instance-attribute</code>","text":"<pre><code>shot_channels: dict[str, dict[int, list[int]]]\n</code></pre> <p>Shot channels as channel_name -&gt; {index -&gt; values}.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ExperimentData.vector_channels","title":"vector_channels  <code>instance-attribute</code>","text":"<pre><code>vector_channels: dict[str, dict[int, list[float]]]\n</code></pre> <p>Vector channels as channel_name -&gt; {index -&gt; values}.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ExperimentDataPoint","title":"ExperimentDataPoint","text":"<p>               Bases: <code>ResultDict</code></p> <p>A single data point with its context.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ExperimentDataPoint.index","title":"index  <code>instance-attribute</code>","text":"<pre><code>index: int\n</code></pre> <p>Sequential index of this data point.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ExperimentDataPoint.scan_params","title":"scan_params  <code>instance-attribute</code>","text":"<pre><code>scan_params: dict[str, DatabaseValueType]\n</code></pre> <p>Parameter values that produced this data point.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ExperimentDataPoint.sequence_json","title":"sequence_json  <code>instance-attribute</code>","text":"<pre><code>sequence_json: str\n</code></pre> <p>Serialized sequence JSON used for this data point.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ExperimentDataPoint.timestamp","title":"timestamp  <code>instance-attribute</code>","text":"<pre><code>timestamp: str\n</code></pre> <p>Acquisition timestamp (ISO string).</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ExperimentDataRepository","title":"ExperimentDataRepository","text":"<p>Repository for HDF5-based experiment data.</p> <p>Manages HDF5 file creation and updates (metadata, results, parameters), with file-level locking to support concurrent writers.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ExperimentDataRepository.get_experiment_data_by_job_id","title":"get_experiment_data_by_job_id  <code>staticmethod</code>","text":"<pre><code>get_experiment_data_by_job_id(\n    *, job_id: int\n) -&gt; ExperimentData\n</code></pre> <p>Load all stored data for a job from its HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>Job identifier.</p> required <p>Returns:</p> Type Description <code>ExperimentData</code> <p>Experiment data payload suitable for the API.</p> Source code in <code>src/icon/server/data_access/repositories/experiment_data_repository.py</code> <pre><code>@staticmethod\ndef get_experiment_data_by_job_id(\n    *,\n    job_id: int,\n) -&gt; ExperimentData:\n    \"\"\"Load all stored data for a job from its HDF5 file.\n\n    Args:\n        job_id: Job identifier.\n\n    Returns:\n        Experiment data payload suitable for the API.\n    \"\"\"\n\n    data = ExperimentData(\n        plot_windows={\n            \"result_channels\": [],\n            \"shot_channels\": [],\n            \"vector_channels\": [],\n        },\n        shot_channels={},\n        result_channels={},\n        vector_channels={},\n        scan_parameters={},\n        json_sequences=[],\n        realtime_scan=False,\n        parameters={},\n    )\n\n    filename = get_filename_by_job_id(job_id)\n    file = f\"{get_config().data.results_dir}/{filename}\"\n\n    if not os.path.exists(file):\n        logger.warning(\"The file %s does not exist.\", file)\n        return data\n\n    lock_path = (\n        f\"{get_config().data.results_dir}/.{filename}\"\n        f\"{ExperimentDataRepository.LOCK_EXTENSION}\"\n    )\n    with FileLock(lock_path), h5py.File(file, \"r\") as h5file:\n        data.realtime_scan = bool(h5file.attrs[\"realtime_scan\"])\n        # Parse JSON strings in relevant columns back into Python objects\n\n        scan_parameters: npt.NDArray = h5file[\"scan_parameters\"][:]  # type: ignore\n        data.scan_parameters = {\n            param: {\n                index: value[0].item().decode()\n                if isinstance(value[0], np.bytes_)\n                else value[0].item()\n                for index, value in enumerate(scan_parameters[param])\n            }\n            for param in cast(\"tuple[str, ...]\", scan_parameters.dtype.names)\n        }\n\n        result_channel_dataset = cast(\"h5py.Dataset\", h5file[\"result_channels\"])\n        data.plot_windows[\"result_channels\"] = json.loads(\n            cast(\"str\", result_channel_dataset.attrs[\"Plot window metadata\"])\n        )\n        result_channels = cast(\"npt.NDArray\", result_channel_dataset[:])  # type: ignore\n        data.result_channels = {\n            channel_name: dict(\n                enumerate(\n                    cast(\"list[float]\", result_channels[channel_name].tolist())\n                )\n            )\n            for channel_name in cast(\"tuple[str, ...]\", result_channels.dtype.names)\n        }\n\n        # Convert shot channels into dicts with index as key\n        shot_channels_group = cast(\"h5py.Group\", h5file[\"shot_channels\"])\n        data.plot_windows[\"shot_channels\"] = json.loads(\n            cast(\"str\", shot_channels_group.attrs[\"Plot window metadata\"])\n        )\n        data.shot_channels = {\n            key: dict(enumerate(value[:].tolist()))  # type: ignore\n            for key, value in cast(\n                \"Sequence[tuple[str, h5py.Dataset]]\", shot_channels_group.items()\n            )\n        }\n\n        vector_channels_group = cast(\"h5py.Group\", h5file[\"vector_channels\"])\n        data.plot_windows[\"vector_channels\"] = json.loads(\n            cast(\"str\", vector_channels_group.attrs[\"Plot window metadata\"])\n        )\n        data.vector_channels = {\n            channel_name: {\n                int(data_point): vector_dataset[:].tolist()\n                for data_point, vector_dataset in cast(\n                    \"Sequence[tuple[str, h5py.Dataset]]\", vector_group.items()\n                )\n            }\n            for channel_name, vector_group in cast(\n                \"Sequence[tuple[str, h5py.Group]]\", vector_channels_group.items()\n            )\n        }\n\n        sequence_json_dataset = cast(\"h5py.Dataset\", h5file[\"sequence_json\"])\n        data.json_sequences = [\n            [cast(\"np.int32\", entry[\"index\"]).item(), entry[\"Sequence\"].decode()]\n            for entry in sequence_json_dataset\n        ]\n        data.parameters = extract_parameter_values(h5file)\n    return data\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ExperimentDataRepository.update_metadata_by_job_id","title":"update_metadata_by_job_id  <code>staticmethod</code>","text":"<pre><code>update_metadata_by_job_id(\n    *,\n    job_id: int,\n    number_of_shots: int,\n    repetitions: int,\n    readout_metadata: ReadoutMetadata,\n    local_parameter_timestamp: datetime | None = None,\n    parameters: list[ScanParameter] = [],\n) -&gt; None\n</code></pre> <p>Create or update HDF5 metadata for a job.</p> <p>Initializes datasets, sets file-level attributes, and stores plot window metadata for result/shot/vector channels.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>Job identifier.</p> required <code>number_of_shots</code> <code>int</code> <p>Shots per data point.</p> required <code>repetitions</code> <code>int</code> <p>Number of repetitions.</p> required <code>readout_metadata</code> <code>ReadoutMetadata</code> <p>Plot/window/channel metadata.</p> required <code>local_parameter_timestamp</code> <code>datetime | None</code> <p>Optional timestamp for local parameters.</p> <code>None</code> <code>parameters</code> <code>list[ScanParameter]</code> <p>Scan parameters.</p> <code>[]</code> Source code in <code>src/icon/server/data_access/repositories/experiment_data_repository.py</code> <pre><code>@staticmethod\ndef update_metadata_by_job_id(  # noqa: PLR0913\n    *,\n    job_id: int,\n    number_of_shots: int,\n    repetitions: int,\n    readout_metadata: ReadoutMetadata,\n    local_parameter_timestamp: datetime | None = None,\n    parameters: list[ScanParameter] = [],\n) -&gt; None:\n    \"\"\"Create or update HDF5 metadata for a job.\n\n    Initializes datasets, sets file-level attributes, and stores plot window\n    metadata for result/shot/vector channels.\n\n    Args:\n        job_id: Job identifier.\n        number_of_shots: Shots per data point.\n        repetitions: Number of repetitions.\n        readout_metadata: Plot/window/channel metadata.\n        local_parameter_timestamp: Optional timestamp for local parameters.\n        parameters: Scan parameters.\n    \"\"\"\n\n    filename = get_filename_by_job_id(job_id)\n    file = f\"{get_config().data.results_dir}/{filename}\"\n\n    job = JobRepository.get_job_by_id(job_id=job_id, load_experiment_source=True)\n\n    lock_path = (\n        f\"{get_config().data.results_dir}/.{filename}\"\n        f\"{ExperimentDataRepository.LOCK_EXTENSION}\"\n    )\n    with FileLock(lock_path), h5py.File(file, \"a\") as h5file:\n        h5file.attrs[\"number_of_data_points\"] = 0\n        h5file.attrs[\"number_of_shots\"] = number_of_shots\n        h5file.attrs[\"experiment_id\"] = job.experiment_source.experiment_id\n        h5file.attrs[\"job_id\"] = job_id\n        h5file.attrs[\"repetitions\"] = repetitions\n        h5file.attrs[\"realtime_scan\"] = contains_realtime_parameter(parameters)\n\n        if local_parameter_timestamp is not None:\n            h5file.attrs[\"local_parameter_timestamp\"] = local_parameter_timestamp\n\n        scan_parameter_dtype = [\n            (\"timestamp\", \"S26\"),\n            *[\n                (param.variable_id, np.float64)\n                for param in parameters\n                if not param.realtime\n            ],\n        ]\n        h5file.create_dataset(\n            \"scan_parameters\",\n            shape=(0, 1),\n            maxshape=(None, 1),\n            chunks=True,\n            dtype=scan_parameter_dtype,\n            compression=\"gzip\",\n            compression_opts=9,\n        )\n\n        for parameter in parameters:\n            if parameter.device is not None:\n                h5file[\"scan_parameters\"].attrs[parameter.unique_id()] = (\n                    f\"name={parameter.device.name} url={parameter.device.url}\"\n                    f\"description={parameter.device.description}\"\n                )\n\n        result_dataset = get_result_channels_dataset(\n            h5file=h5file, result_channels=readout_metadata[\"readout_channel_names\"]\n        )\n\n        result_dataset.attrs[\"Plot window metadata\"] = json.dumps(\n            readout_metadata[\"readout_channel_windows\"]\n        )\n        shot_group = h5file.require_group(\"shot_channels\")\n        shot_group.attrs[\"Plot window metadata\"] = json.dumps(\n            readout_metadata[\"shot_channel_windows\"]\n        )\n        vector_group = h5file.require_group(\"vector_channels\")\n        vector_group.attrs[\"Plot window metadata\"] = json.dumps(\n            readout_metadata[\"vector_channel_windows\"]\n        )\n\n    emit_queue.put(\n        {\n            \"event\": f\"experiment_{job_id}_metadata\",\n            \"data\": {\n                \"readout_metadata\": {\n                    \"result_channels\": readout_metadata[\"readout_channel_windows\"],\n                    \"shot_channels\": readout_metadata[\"shot_channel_windows\"],\n                    \"vector_channels\": readout_metadata[\"vector_channel_windows\"],\n                },\n            },\n        }\n    )\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ExperimentDataRepository.write_experiment_data_by_job_id","title":"write_experiment_data_by_job_id  <code>staticmethod</code>","text":"<pre><code>write_experiment_data_by_job_id(\n    *, job_id: int, data_point: ExperimentDataPoint\n) -&gt; None\n</code></pre> <p>Append a complete data point to the HDF5 file and emit an event.</p> <p>Writes scan parameters, result/shot/vector channels, and sequence JSON.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>Job identifier.</p> required <code>data_point</code> <code>ExperimentDataPoint</code> <p>Data point payload to append.</p> required Source code in <code>src/icon/server/data_access/repositories/experiment_data_repository.py</code> <pre><code>@staticmethod\ndef write_experiment_data_by_job_id(\n    *,\n    job_id: int,\n    data_point: ExperimentDataPoint,\n) -&gt; None:\n    \"\"\"Append a complete data point to the HDF5 file and emit an event.\n\n    Writes scan parameters, result/shot/vector channels, and sequence JSON.\n\n    Args:\n        job_id: Job identifier.\n        data_point: Data point payload to append.\n    \"\"\"\n\n    filename = get_filename_by_job_id(job_id)\n    file = f\"{get_config().data.results_dir}/{filename}\"\n\n    lock_path = (\n        f\"{get_config().data.results_dir}/.{filename}\"\n        f\"{ExperimentDataRepository.LOCK_EXTENSION}\"\n    )\n    with FileLock(lock_path), h5py.File(file, \"a\") as h5file:\n        try:\n            number_of_shots: int = h5file.attrs[\"number_of_shots\"]\n            number_of_data_points: int = h5file.attrs[\"number_of_data_points\"]\n        except KeyError:\n            raise Exception(\n                \"Metadata does not contain relevant information. Please use \"\n                \"ExperimentDataRepository.update_metadata_by_job_id first!\"\n            )\n\n        write_scan_parameters_and_timestamp_to_dataset(\n            h5file=h5file,\n            data_point_index=data_point[\"index\"],\n            scan_params=data_point[\"scan_params\"],\n            timestamp=data_point[\"timestamp\"],\n            number_of_data_points=number_of_data_points,\n        )\n\n        write_results_to_dataset(\n            h5file=h5file,\n            data_point_index=data_point[\"index\"],\n            result_channels=data_point[\"result_channels\"],\n            number_of_data_points=number_of_data_points,\n        )\n\n        write_shot_channels_to_datasets(\n            h5file=h5file,\n            data_point_index=data_point[\"index\"],\n            shot_channels=data_point[\"shot_channels\"],\n            number_of_data_points=number_of_data_points,\n            number_of_shots=number_of_shots,\n        )\n\n        write_vector_channels_to_datasets(\n            h5file=h5file,\n            data_point_index=data_point[\"index\"],\n            vector_channels=data_point[\"vector_channels\"],\n        )\n\n        write_sequence_json_to_dataset(\n            h5file=h5file,\n            data_point_index=data_point[\"index\"],\n            sequence_json=data_point[\"sequence_json\"],\n        )\n\n        if data_point[\"index\"] &gt;= number_of_data_points:\n            h5file.attrs[\"number_of_data_points\"] = data_point[\"index\"] + 1\n\n        logger.debug(\"Appended data to %s\", file)\n\n    emit_queue.put(\n        {\n            \"event\": f\"experiment_{job_id}\",\n            \"data\": data_point,\n        }\n    )\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ExperimentDataRepository.write_parameter_update_by_job_id","title":"write_parameter_update_by_job_id  <code>staticmethod</code>","text":"<pre><code>write_parameter_update_by_job_id(\n    *,\n    job_id: int,\n    timestamp: str,\n    parameter_values: dict[str, str | int | float | bool],\n) -&gt; None\n</code></pre> <p>Append parameter updates under the \u2018parameters\u2019 group.</p> <p>Creates a dataset per parameter storing (timestamp, value) entries. Appends only when the value changed from the last entry.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>Job identifier.</p> required <code>timestamp</code> <code>str</code> <p>ISO timestamp string.</p> required <code>parameter_values</code> <code>dict[str, str | int | float | bool]</code> <p>Mapping of parameter id to value.</p> required Source code in <code>src/icon/server/data_access/repositories/experiment_data_repository.py</code> <pre><code>@staticmethod\ndef write_parameter_update_by_job_id(\n    *,\n    job_id: int,\n    timestamp: str,\n    parameter_values: dict[str, str | int | float | bool],\n) -&gt; None:\n    \"\"\"Append parameter updates under the 'parameters' group.\n\n    Creates a dataset per parameter storing (timestamp, value) entries.\n    Appends only when the value changed from the last entry.\n\n    Args:\n        job_id: Job identifier.\n        timestamp: ISO timestamp string.\n        parameter_values: Mapping of parameter id to value.\n    \"\"\"\n\n    filename = get_filename_by_job_id(job_id)\n    file = f\"{get_config().data.results_dir}/{filename}\"\n    lock_path = (\n        f\"{get_config().data.results_dir}/.{filename}\"\n        f\"{ExperimentDataRepository.LOCK_EXTENSION}\"\n    )\n    parameter_updates = {}\n    with FileLock(lock_path), h5py.File(file, \"a\") as h5file:\n        parameters_group = h5file.require_group(\"parameters\")\n\n        for param_id, value in parameter_values.items():\n            dtype = [(\"timestamp\", \"S26\"), (\"value\", get_hdf5_dtype(value))]\n\n            if param_id in parameters_group:\n                ds: h5py.Dataset = parameters_group[param_id]\n                if ds.shape[0] &gt; 0:\n                    last_entry = ds[-1]\n                    last_value = last_entry[\"value\"]\n                    if isinstance(value, str):\n                        if last_value.decode() == value:\n                            continue\n                    elif last_value == value:\n                        continue\n\n                index = ds.shape[0]\n                resize_dataset(ds, next_index=index, axis=0)\n            else:\n                ds = parameters_group.create_dataset(\n                    param_id,\n                    shape=(1,),\n                    maxshape=(None,),\n                    dtype=dtype,\n                )\n                index = 0\n\n            ds[index] = (timestamp.encode(), value)\n            parameter_updates[param_id] = ParameterValue(timestamp, value)\n\n        logger.debug(\n            \"Wrote parameter update for job %d at %s\",\n            job_id,\n            timestamp,\n        )\n    emit_queue.put(\n        {\n            \"event\": f\"experiment_params_{job_id}\",\n            \"data\": {\n                param_id: asdict(val) for param_id, val in parameter_updates.items()\n            },\n        }\n    )\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.PlotWindowMetadata","title":"PlotWindowMetadata","text":"<p>               Bases: <code>TypedDict</code></p> <p>Metadata describing a single plot window for visualization in the frontend.</p> <p>This metadata includes the plot\u2019s index within its type, the type of plot (e.g., vector, histogram, or readout), and the list of channel names that are to be plotted in the respective window.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.PlotWindowMetadata.channel_names","title":"channel_names  <code>instance-attribute</code>","text":"<pre><code>channel_names: list[str]\n</code></pre> <p>A list of channel names to be plotted in this window</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.PlotWindowMetadata.index","title":"index  <code>instance-attribute</code>","text":"<pre><code>index: int\n</code></pre> <p>The order of the plot window within its type (e.g., 0, 1, 2\u2026)</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.PlotWindowMetadata.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the plot window</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.PlotWindowMetadata.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Literal['vector', 'histogram', 'readout']\n</code></pre> <p>The type of the plot window</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.PlotWindowsDict","title":"PlotWindowsDict","text":"<p>               Bases: <code>TypedDict</code></p> <p>Grouping of plot window metadata by channel type.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.PlotWindowsDict.result_channels","title":"result_channels  <code>instance-attribute</code>","text":"<pre><code>result_channels: list[PlotWindowMetadata]\n</code></pre> <p>Plot window metadata for result channels.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.PlotWindowsDict.shot_channels","title":"shot_channels  <code>instance-attribute</code>","text":"<pre><code>shot_channels: list[PlotWindowMetadata]\n</code></pre> <p>Plot window metadata for shot channels.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.PlotWindowsDict.vector_channels","title":"vector_channels  <code>instance-attribute</code>","text":"<pre><code>vector_channels: list[PlotWindowMetadata]\n</code></pre> <p>Plot window metadata for vector channels.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ReadoutMetadata","title":"ReadoutMetadata","text":"<p>               Bases: <code>TypedDict</code></p> <p>Metadata describing readout/shot/vector channels and their plot windows.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ReadoutMetadata.readout_channel_names","title":"readout_channel_names  <code>instance-attribute</code>","text":"<pre><code>readout_channel_names: list[str]\n</code></pre> <p>A list of all readout channel names</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ReadoutMetadata.readout_channel_windows","title":"readout_channel_windows  <code>instance-attribute</code>","text":"<pre><code>readout_channel_windows: list[PlotWindowMetadata]\n</code></pre> <p>List of <code>PlotWindowMetadata</code> of result channels</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ReadoutMetadata.shot_channel_names","title":"shot_channel_names  <code>instance-attribute</code>","text":"<pre><code>shot_channel_names: list[str]\n</code></pre> <p>A list of all shot channel names</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ReadoutMetadata.shot_channel_windows","title":"shot_channel_windows  <code>instance-attribute</code>","text":"<pre><code>shot_channel_windows: list[PlotWindowMetadata]\n</code></pre> <p>List of <code>PlotWindowMetadata</code> of shot channels</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ReadoutMetadata.vector_channel_names","title":"vector_channel_names  <code>instance-attribute</code>","text":"<pre><code>vector_channel_names: list[str]\n</code></pre> <p>A list of all vector channel names</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ReadoutMetadata.vector_channel_windows","title":"vector_channel_windows  <code>instance-attribute</code>","text":"<pre><code>vector_channel_windows: list[PlotWindowMetadata]\n</code></pre> <p>List of <code>PlotWindowMetadata</code> of vector channels</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ResultDict","title":"ResultDict","text":"<p>               Bases: <code>TypedDict</code></p> <p>Scalar/vector/shot readouts for a single data point.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ResultDict.result_channels","title":"result_channels  <code>instance-attribute</code>","text":"<pre><code>result_channels: dict[str, float]\n</code></pre> <p>Mapping from result channel name to scalar value.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ResultDict.shot_channels","title":"shot_channels  <code>instance-attribute</code>","text":"<pre><code>shot_channels: dict[str, list[int]]\n</code></pre> <p>Mapping from shot channel name to per-shot integers.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.ResultDict.vector_channels","title":"vector_channels  <code>instance-attribute</code>","text":"<pre><code>vector_channels: dict[str, list[float]]\n</code></pre> <p>Mapping from vector channel name to list of floats.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.get_filename_by_job_id","title":"get_filename_by_job_id","text":"<pre><code>get_filename_by_job_id(job_id: int) -&gt; str\n</code></pre> <p>Return the HDF5 filename for a job.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>Job identifier.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Filename derived from the job\u2019s scheduled time (e.g., \u201c.h5\u201d). Source code in <code>src/icon/server/data_access/repositories/experiment_data_repository.py</code> <pre><code>def get_filename_by_job_id(job_id: int) -&gt; str:\n    \"\"\"Return the HDF5 filename for a job.\n\n    Args:\n        job_id: Job identifier.\n\n    Returns:\n        Filename derived from the job's scheduled time (e.g., \"&lt;iso&gt;.h5\").\n    \"\"\"\n\n    scheduled_time = JobRunRepository.get_scheduled_time_by_job_id(job_id=job_id)\n    return f\"{scheduled_time}.h5\"\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.resize_dataset","title":"resize_dataset","text":"<pre><code>resize_dataset(\n    dataset: Dataset, next_index: int, axis: int\n) -&gt; None\n</code></pre> <p>Resize a dataset to accommodate writing at a target index.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>HDF5 dataset to resize.</p> required <code>next_index</code> <code>int</code> <p>Index that must be writable.</p> required <code>axis</code> <code>int</code> <p>Axis along which to grow.</p> required Source code in <code>src/icon/server/data_access/repositories/experiment_data_repository.py</code> <pre><code>def resize_dataset(dataset: h5py.Dataset, next_index: int, axis: int) -&gt; None:\n    \"\"\"Resize a dataset to accommodate writing at a target index.\n\n    Args:\n        dataset: HDF5 dataset to resize.\n        next_index: Index that must be writable.\n        axis: Axis along which to grow.\n    \"\"\"\n\n    dataset.resize(next_index + 1, axis)\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.write_results_to_dataset","title":"write_results_to_dataset","text":"<pre><code>write_results_to_dataset(\n    h5file: File,\n    data_point_index: int,\n    result_channels: dict[str, float],\n    number_of_data_points: int,\n) -&gt; None\n</code></pre> <p>Write scalar result channels into the \u2018result_channels\u2019 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>h5file</code> <code>File</code> <p>Open HDF5 file handle.</p> required <code>data_point_index</code> <code>int</code> <p>Index of the current data point.</p> required <code>result_channels</code> <code>dict[str, float]</code> <p>Mapping of channel name to float value.</p> required <code>number_of_data_points</code> <code>int</code> <p>Current total number of stored data points.</p> required Source code in <code>src/icon/server/data_access/repositories/experiment_data_repository.py</code> <pre><code>def write_results_to_dataset(\n    h5file: h5py.File,\n    data_point_index: int,\n    result_channels: dict[str, float],\n    number_of_data_points: int,\n) -&gt; None:\n    \"\"\"Write scalar result channels into the 'result_channels' dataset.\n\n    Args:\n        h5file: Open HDF5 file handle.\n        data_point_index: Index of the current data point.\n        result_channels: Mapping of channel name to float value.\n        number_of_data_points: Current total number of stored data points.\n    \"\"\"\n\n    sorted_keys = sorted(result_channels)\n\n    result_dataset = get_result_channels_dataset(\n        h5file=h5file,\n        result_channels=sorted_keys,\n        number_of_data_points=number_of_data_points,\n    )\n\n    if set(result_dataset.dtype.names) != set(sorted_keys):\n        raise RuntimeError(\n            f\"Result channels changed from {list(result_dataset.dtype.names)} to \"\n            f\"{sorted_keys}\"\n        )\n\n    if data_point_index &gt;= number_of_data_points:\n        resize_dataset(result_dataset, next_index=data_point_index, axis=0)\n\n    result_dataset[data_point_index] = tuple(result_channels[k] for k in sorted_keys)\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.write_scan_parameters_and_timestamp_to_dataset","title":"write_scan_parameters_and_timestamp_to_dataset","text":"<pre><code>write_scan_parameters_and_timestamp_to_dataset(\n    h5file: File,\n    data_point_index: int,\n    scan_params: dict[str, DatabaseValueType],\n    timestamp: str,\n    number_of_data_points: int,\n) -&gt; None\n</code></pre> <p>Write scan parameters and timestamp to the \u2018scan_parameters\u2019 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>h5file</code> <code>File</code> <p>Open HDF5 file handle.</p> required <code>data_point_index</code> <code>int</code> <p>Index of the current data point.</p> required <code>scan_params</code> <code>dict[str, DatabaseValueType]</code> <p>Parameter values for this data point.</p> required <code>timestamp</code> <code>str</code> <p>Acquisition timestamp (ISO string).</p> required <code>number_of_data_points</code> <code>int</code> <p>Current total number of stored data points.</p> required Source code in <code>src/icon/server/data_access/repositories/experiment_data_repository.py</code> <pre><code>def write_scan_parameters_and_timestamp_to_dataset(\n    h5file: h5py.File,\n    data_point_index: int,\n    scan_params: dict[str, DatabaseValueType],\n    timestamp: str,\n    number_of_data_points: int,\n) -&gt; None:\n    \"\"\"Write scan parameters and timestamp to the 'scan_parameters' dataset.\n\n    Args:\n        h5file: Open HDF5 file handle.\n        data_point_index: Index of the current data point.\n        scan_params: Parameter values for this data point.\n        timestamp: Acquisition timestamp (ISO string).\n        number_of_data_points: Current total number of stored data points.\n    \"\"\"\n\n    scan_parameter_dtype = [\n        (\"timestamp\", \"S26\"),  # timestamps are strings of length 26\n        *[(key, np.float64) for key in scan_params],\n    ]\n    scan_params_dataset = h5file.require_dataset(\n        \"scan_parameters\",\n        shape=(number_of_data_points, 1),\n        maxshape=(None, 1),\n        chunks=True,\n        dtype=scan_parameter_dtype,\n        compression=\"gzip\",\n        compression_opts=9,\n    )\n\n    if data_point_index &gt;= number_of_data_points:\n        resize_dataset(scan_params_dataset, next_index=data_point_index, axis=0)\n\n    parameter_values = tuple(scan_params[key] for key in scan_params)\n    scan_params_dataset[data_point_index] = (\n        timestamp,\n        *parameter_values,\n    )\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.write_sequence_json_to_dataset","title":"write_sequence_json_to_dataset","text":"<pre><code>write_sequence_json_to_dataset(\n    h5file: File, data_point_index: int, sequence_json: str\n) -&gt; None\n</code></pre> <p>Append sequence JSON if it changed since the last entry.</p> <p>Parameters:</p> Name Type Description Default <code>h5file</code> <code>File</code> <p>Open HDF5 file handle.</p> required <code>data_point_index</code> <code>int</code> <p>Index of the current data point.</p> required <code>sequence_json</code> <code>str</code> <p>Serialized sequence JSON to append.</p> required Source code in <code>src/icon/server/data_access/repositories/experiment_data_repository.py</code> <pre><code>def write_sequence_json_to_dataset(\n    h5file: h5py.File,\n    data_point_index: int,\n    sequence_json: str,\n) -&gt; None:\n    \"\"\"Append sequence JSON if it changed since the last entry.\n\n    Args:\n        h5file: Open HDF5 file handle.\n        data_point_index: Index of the current data point.\n        sequence_json: Serialized sequence JSON to append.\n    \"\"\"\n\n    sequence_json_dtype = [\n        (\"index\", np.int32),\n        (\"Sequence\", h5py.string_dtype()),\n    ]\n    sequence_json_dataset = h5file.require_dataset(\n        \"sequence_json\",\n        shape=(0,),\n        maxshape=(None,),\n        chunks=True,\n        dtype=sequence_json_dtype,\n        compression=\"gzip\",\n        compression_opts=9,\n    )\n\n    index = sequence_json_dataset.shape[0]\n    if index &gt; 0:\n        _, sequence_json_old = cast(\n            \"tuple[int, bytes]\", sequence_json_dataset[index - 1]\n        )\n        if sequence_json_old.decode() == sequence_json:\n            logger.debug(\"Sequence JSON didn't change.\")\n            return\n\n    resize_dataset(sequence_json_dataset, next_index=index, axis=0)\n\n    sequence_json_dataset[index] = (data_point_index, sequence_json)\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.write_shot_channels_to_datasets","title":"write_shot_channels_to_datasets","text":"<pre><code>write_shot_channels_to_datasets(\n    h5file: File,\n    data_point_index: int,\n    shot_channels: dict[str, list[int]],\n    number_of_data_points: int,\n    number_of_shots: int,\n) -&gt; None\n</code></pre> <p>Write per-shot data into datasets under the \u2018shot_channels\u2019 group.</p> <p>Parameters:</p> Name Type Description Default <code>h5file</code> <code>File</code> <p>Open HDF5 file handle.</p> required <code>data_point_index</code> <code>int</code> <p>Index of the current data point.</p> required <code>shot_channels</code> <code>dict[str, list[int]]</code> <p>Mapping of channel to per-shot integers.</p> required <code>number_of_data_points</code> <code>int</code> <p>Current total number of stored data points.</p> required <code>number_of_shots</code> <code>int</code> <p>Expected number of shots per channel.</p> required Source code in <code>src/icon/server/data_access/repositories/experiment_data_repository.py</code> <pre><code>def write_shot_channels_to_datasets(\n    h5file: h5py.File,\n    data_point_index: int,\n    shot_channels: dict[str, list[int]],\n    number_of_data_points: int,\n    number_of_shots: int,\n) -&gt; None:\n    \"\"\"Write per-shot data into datasets under the 'shot_channels' group.\n\n    Args:\n        h5file: Open HDF5 file handle.\n        data_point_index: Index of the current data point.\n        shot_channels: Mapping of channel to per-shot integers.\n        number_of_data_points: Current total number of stored data points.\n        number_of_shots: Expected number of shots per channel.\n    \"\"\"\n\n    shot_group = h5file.require_group(\"shot_channels\")\n    for key, value in shot_channels.items():\n        shot_dataset = shot_group.require_dataset(\n            key,\n            shape=(number_of_data_points, number_of_shots),\n            maxshape=(None, number_of_shots),\n            chunks=True,\n            dtype=np.float64,\n            compression=\"gzip\",\n            compression_opts=9,\n        )\n\n        if data_point_index &gt;= number_of_data_points:\n            resize_dataset(shot_dataset, next_index=data_point_index, axis=0)\n        shot_dataset[data_point_index] = value\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_data_repository.write_vector_channels_to_datasets","title":"write_vector_channels_to_datasets","text":"<pre><code>write_vector_channels_to_datasets(\n    h5file: File,\n    data_point_index: int,\n    vector_channels: dict[str, list[float]],\n) -&gt; None\n</code></pre> <p>Write vector channel data under the \u2018vector_channels\u2019 group.</p> <p>Creates one dataset per channel per data point.</p> <p>Parameters:</p> Name Type Description Default <code>h5file</code> <code>File</code> <p>Open HDF5 file handle.</p> required <code>data_point_index</code> <code>int</code> <p>Index of the current data point.</p> required <code>vector_channels</code> <code>dict[str, list[float]]</code> <p>Mapping of channel to vector of floats.</p> required Source code in <code>src/icon/server/data_access/repositories/experiment_data_repository.py</code> <pre><code>def write_vector_channels_to_datasets(\n    h5file: h5py.File,\n    data_point_index: int,\n    vector_channels: dict[str, list[float]],\n) -&gt; None:\n    \"\"\"Write vector channel data under the 'vector_channels' group.\n\n    Creates one dataset per channel per data point.\n\n    Args:\n        h5file: Open HDF5 file handle.\n        data_point_index: Index of the current data point.\n        vector_channels: Mapping of channel to vector of floats.\n    \"\"\"\n\n    vector_group = h5file.require_group(\"vector_channels\")\n    for channel_name, vector in vector_channels.items():\n        channel_group = vector_group.require_group(channel_name)\n        if str(data_point_index) not in channel_group:\n            channel_group.create_dataset(\n                str(data_point_index),\n                data=vector,\n                compression=\"gzip\",\n                compression_opts=9,\n            )\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_source_repository","title":"experiment_source_repository","text":""},{"location":"reference/server/#icon.server.data_access.repositories.experiment_source_repository.ExperimentSourceRepository","title":"ExperimentSourceRepository","text":"<p>Repository for <code>ExperimentSource</code> entities.</p> <p>Provides methods to query and persist experiment sources in the database. Encapsulates the SQLAlchemy session and query logic.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.experiment_source_repository.ExperimentSourceRepository.get_or_create_experiment","title":"get_or_create_experiment  <code>staticmethod</code>","text":"<pre><code>get_or_create_experiment(\n    *, experiment_source: ExperimentSource\n) -&gt; ExperimentSource\n</code></pre> <p>Return an existing experiment source or create it if not found.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_source</code> <code>ExperimentSource</code> <p>The experiment source to look up by <code>experiment_id</code>. If no matching row exists, this instance is inserted into the database.</p> required <p>Returns:</p> Type Description <code>ExperimentSource</code> <p>The existing or newly created experiment source.</p> Source code in <code>src/icon/server/data_access/repositories/experiment_source_repository.py</code> <pre><code>@staticmethod\ndef get_or_create_experiment(\n    *,\n    experiment_source: ExperimentSource,\n) -&gt; ExperimentSource:\n    \"\"\"Return an existing experiment source or create it if not found.\n\n    Args:\n        experiment_source: The experiment source to look up by `experiment_id`. If\n            no matching row exists, this instance is inserted into the database.\n\n    Returns:\n        The existing or newly created experiment source.\n    \"\"\"\n\n    with sqlalchemy.orm.Session(engine) as session:\n        experiment = (\n            session.query(ExperimentSource)\n            .filter_by(experiment_id=experiment_source.experiment_id)\n            .first()\n        )\n\n        if not experiment:\n            experiment = experiment_source\n            session.add(experiment)\n            session.commit()\n            session.refresh(experiment)  # Refresh to get the ID\n            logger.debug(\"Inserted new experiment %s\", experiment)\n\n    return experiment\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.job_repository","title":"job_repository","text":""},{"location":"reference/server/#icon.server.data_access.repositories.job_repository.JobRepository","title":"JobRepository","text":"<p>Repository for <code>Job</code> entities.</p> <p>Encapsulates SQLAlchemy session/query logic and emits Socket.IO events on changes. All methods open their own session and return detached ORM objects.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.job_repository.JobRepository.get_job_by_experiment_source_and_status","title":"get_job_by_experiment_source_and_status  <code>staticmethod</code>","text":"<pre><code>get_job_by_experiment_source_and_status(\n    *,\n    experiment_source_id: int,\n    status: JobStatus | None = None,\n) -&gt; Sequence[Row[tuple[Job]]]\n</code></pre> <p>List jobs for an experiment source, optionally filtered by status.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_source_id</code> <code>int</code> <p>Foreign key of the experiment source.</p> required <code>status</code> <code>JobStatus | None</code> <p>Optional status filter.</p> <code>None</code> <p>Returns:</p> Type Description <code>Sequence[Row[tuple[Job]]]</code> <p>Rows containing <code>Job</code> objects, ordered by priority then creation time.</p> Source code in <code>src/icon/server/data_access/repositories/job_repository.py</code> <pre><code>@staticmethod\ndef get_job_by_experiment_source_and_status(\n    *,\n    experiment_source_id: int,\n    status: JobStatus | None = None,\n) -&gt; Sequence[sqlalchemy.Row[tuple[Job]]]:\n    \"\"\"List jobs for an experiment source, optionally filtered by status.\n\n    Args:\n        experiment_source_id: Foreign key of the experiment source.\n        status: Optional status filter.\n\n    Returns:\n        Rows containing `Job` objects, ordered by priority then creation time.\n    \"\"\"\n\n    with sqlalchemy.orm.Session(engine) as session:\n        stmt = select(Job).where(Job.experiment_source_id == experiment_source_id)\n\n        if status:\n            stmt = stmt.where(Job.status == status)\n\n        stmt = stmt.options(\n            sqlalchemy.orm.joinedload(Job.experiment_source)\n        ).order_by(Job.priority.asc(), Job.created.asc())\n\n        jobs = session.execute(stmt).all()\n        logger.debug(\"Got jobs by experiment_source_id %s\", experiment_source_id)\n    return jobs\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.job_repository.JobRepository.get_job_by_id","title":"get_job_by_id  <code>staticmethod</code>","text":"<pre><code>get_job_by_id(\n    *,\n    job_id: int,\n    load_experiment_source: bool = False,\n    load_scan_parameters: bool = False,\n) -&gt; Job\n</code></pre> <p>Fetch a job by ID with optional eager-loading.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>Job identifier.</p> required <code>load_experiment_source</code> <code>bool</code> <p>If True, eager-load <code>experiment_source</code>.</p> <code>False</code> <code>load_scan_parameters</code> <code>bool</code> <p>If True, eager-load <code>scan_parameters</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Job</code> <p>The requested job.</p> Source code in <code>src/icon/server/data_access/repositories/job_repository.py</code> <pre><code>@staticmethod\ndef get_job_by_id(\n    *,\n    job_id: int,\n    load_experiment_source: bool = False,\n    load_scan_parameters: bool = False,\n) -&gt; Job:\n    \"\"\"Fetch a job by ID with optional eager-loading.\n\n    Args:\n        job_id: Job identifier.\n        load_experiment_source: If True, eager-load `experiment_source`.\n        load_scan_parameters: If True, eager-load `scan_parameters`.\n\n    Returns:\n        The requested job.\n    \"\"\"\n\n    with sqlalchemy.orm.Session(engine) as session:\n        stmt = select(Job).where(Job.id == job_id)\n\n        if load_experiment_source:\n            stmt = stmt.options(sqlalchemy.orm.joinedload(Job.experiment_source))\n        if load_scan_parameters:\n            stmt = stmt.options(sqlalchemy.orm.joinedload(Job.scan_parameters))\n\n        return session.execute(stmt).unique().scalar_one()\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.job_repository.JobRepository.get_jobs_by_status_and_timeframe","title":"get_jobs_by_status_and_timeframe  <code>staticmethod</code>","text":"<pre><code>get_jobs_by_status_and_timeframe(\n    *,\n    status: JobStatus | None = None,\n    start: datetime | None = None,\n    stop: datetime | None = None,\n) -&gt; Sequence[Job]\n</code></pre> <p>List jobs filtered by status and optional creation time window.</p> <p>Parameters:</p> Name Type Description Default <code>status</code> <code>JobStatus | None</code> <p>Optional status filter.</p> <code>None</code> <code>start</code> <code>datetime | None</code> <p>Inclusive start timestamp.</p> <code>None</code> <code>stop</code> <code>datetime | None</code> <p>Exclusive stop timestamp.</p> <code>None</code> <p>Returns:</p> Type Description <code>Sequence[Job]</code> <p>Matching jobs ordered by priority then creation time.</p> Source code in <code>src/icon/server/data_access/repositories/job_repository.py</code> <pre><code>@staticmethod\ndef get_jobs_by_status_and_timeframe(\n    *,\n    status: JobStatus | None = None,\n    start: datetime.datetime | None = None,\n    stop: datetime.datetime | None = None,\n) -&gt; Sequence[Job]:\n    \"\"\"List jobs filtered by status and optional creation time window.\n\n    Args:\n        status: Optional status filter.\n        start: Inclusive start timestamp.\n        stop: Exclusive stop timestamp.\n\n    Returns:\n        Matching jobs ordered by priority then creation time.\n    \"\"\"\n\n    with sqlalchemy.orm.Session(engine) as session:\n        stmt = (\n            select(Job)\n            .options(sqlalchemy.orm.joinedload(Job.experiment_source))\n            .options(sqlalchemy.orm.joinedload(Job.scan_parameters))\n            .order_by(Job.priority.asc())\n            .order_by(Job.created.asc())\n        )\n\n        if status is not None:\n            stmt = stmt.where(Job.status == status)\n        if start is not None:\n            stmt = stmt.where(Job.created &gt;= start)\n        if stop is not None:\n            stmt = stmt.where(Job.created &lt; stop)\n\n        return session.execute(stmt).unique().scalars().all()\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.job_repository.JobRepository.resubmit_job_by_id","title":"resubmit_job_by_id  <code>staticmethod</code>","text":"<pre><code>resubmit_job_by_id(*, job_id: int) -&gt; Job\n</code></pre> <p>Clone an existing job as a new submission.</p> <p>If the source job is not itself a resubmission, the new job\u2019s <code>parent_job_id</code> is set to the original job\u2019s id.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>ID of the job to clone.</p> required <p>Returns:</p> Type Description <code>Job</code> <p>The newly created job.</p> Source code in <code>src/icon/server/data_access/repositories/job_repository.py</code> <pre><code>@staticmethod\ndef resubmit_job_by_id(*, job_id: int) -&gt; Job:\n    \"\"\"Clone an existing job as a new submission.\n\n    If the source job is not itself a resubmission, the new job's `parent_job_id` is\n    set to the original job's id.\n\n    Args:\n        job_id: ID of the job to clone.\n\n    Returns:\n        The newly created job.\n    \"\"\"\n\n    with sqlalchemy.orm.Session(engine) as session:\n        job = session.execute(\n            sqlalchemy.select(Job).where(Job.id == job_id)\n        ).scalar_one()\n        sqlalchemy.orm.make_transient(job)\n\n        if not job.parent_job_id:\n            job.parent_job_id = job.id\n\n        # PK and created are set by DB on insert\n        job.id = None  # type: ignore\n        job.created = None  # type: ignore\n\n        session.add(job)\n        session.commit()\n        session.refresh(job)\n        session.expunge(job)\n\n    emit_queue.put(\n        {\n            \"event\": \"job.new\",\n            \"data\": {\n                \"job\": SQLAlchemyDictEncoder.encode(\n                    JobRepository.get_job_by_id(\n                        job_id=job.id,\n                        load_experiment_source=True,\n                        load_scan_parameters=True,\n                    )\n                ),\n            },\n        }\n    )\n\n    return job\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.job_repository.JobRepository.submit_job","title":"submit_job  <code>staticmethod</code>","text":"<pre><code>submit_job(*, job: Job) -&gt; Job\n</code></pre> <p>Insert a new job and emit a creation event.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>The job instance to persist.</p> required <p>Returns:</p> Type Description <code>Job</code> <p>The persisted job with generated fields populated.</p> Source code in <code>src/icon/server/data_access/repositories/job_repository.py</code> <pre><code>@staticmethod\ndef submit_job(*, job: Job) -&gt; Job:\n    \"\"\"Insert a new job and emit a creation event.\n\n    Args:\n        job: The job instance to persist.\n\n    Returns:\n        The persisted job with generated fields populated.\n    \"\"\"\n    with sqlalchemy.orm.Session(engine) as session:\n        session.add(job)\n        session.commit()\n        session.refresh(job)\n        session.expunge(job)\n\n        logger.debug(\"Submitted new job %s\", job)\n\n    emit_queue.put(\n        {\n            \"event\": \"job.new\",\n            \"data\": {\n                \"job\": SQLAlchemyDictEncoder.encode(\n                    JobRepository.get_job_by_id(\n                        job_id=job.id,\n                        load_experiment_source=True,\n                        load_scan_parameters=True,\n                    )\n                ),\n            },\n        }\n    )\n\n    return job\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.job_repository.JobRepository.update_job_status","title":"update_job_status  <code>staticmethod</code>","text":"<pre><code>update_job_status(*, job: Job, status: JobStatus) -&gt; Job\n</code></pre> <p>Update a job\u2019s status and emit an update event.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>Job to update (identified by its <code>id</code>).</p> required <code>status</code> <code>JobStatus</code> <p>New job status.</p> required <p>Returns:</p> Type Description <code>Job</code> <p>The updated job with relationships loaded.</p> Source code in <code>src/icon/server/data_access/repositories/job_repository.py</code> <pre><code>@staticmethod\ndef update_job_status(*, job: Job, status: JobStatus) -&gt; Job:\n    \"\"\"Update a job's status and emit an update event.\n\n    Args:\n        job: Job to update (identified by its `id`).\n        status: New job status.\n\n    Returns:\n        The updated job with relationships loaded.\n    \"\"\"\n\n    with sqlalchemy.orm.Session(engine) as session:\n        session.execute(update(Job).where(Job.id == job.id).values(status=status))\n        session.commit()\n\n        job = (\n            session.execute(\n                select(Job)\n                .where(Job.id == job.id)\n                .options(\n                    sqlalchemy.orm.joinedload(Job.experiment_source),\n                    sqlalchemy.orm.joinedload(Job.scan_parameters),\n                )\n            )\n            .unique()\n            .scalar_one()\n        )\n        session.expunge(job)\n\n        logger.debug(\"Updated job %s\", job)\n\n    emit_queue.put(\n        {\n            \"event\": \"job.update\",\n            \"data\": {\n                \"job_id\": job.id,\n                \"updated_properties\": {\"status\": status.value},\n            },\n        }\n    )\n\n    return job\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.job_run_repository","title":"job_run_repository","text":""},{"location":"reference/server/#icon.server.data_access.repositories.job_run_repository.JobRunRepository","title":"JobRunRepository","text":"<p>Repository for <code>JobRun</code> entities.</p> <p>Provides methods to insert, update, and query job runs from the database. Emits Socket.IO events when job runs are created or updated.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.job_run_repository.JobRunRepository.get_parameter_update_timestamp","title":"get_parameter_update_timestamp  <code>staticmethod</code>","text":"<pre><code>get_parameter_update_timestamp(*, run_id: int) -&gt; datetime\n</code></pre> <p>Get the paramter update timestamp.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <p>ID of the job.</p> required <p>Returns:     The parameter update timestamp.</p> Source code in <code>src/icon/server/data_access/repositories/job_run_repository.py</code> <pre><code>@staticmethod\ndef get_parameter_update_timestamp(*, run_id: int) -&gt; datetime:\n    \"\"\"Get the paramter update timestamp.\n\n    Args:\n        job_id: ID of the job.\n    Returns:\n        The parameter update timestamp.\n    \"\"\"\n\n    with sqlalchemy.orm.Session(engine) as session:\n        stmt = select(JobRun.parameter_update_timestamp).where(JobRun.id == run_id)\n\n        timestamp = session.execute(stmt).scalar_one()\n        logger.debug(\"Got parameter update timestamp for run %s\", run_id)\n\n    return timestamp.replace(tzinfo=UTC)\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.job_run_repository.JobRunRepository.get_run_by_job_id","title":"get_run_by_job_id  <code>staticmethod</code>","text":"<pre><code>get_run_by_job_id(\n    *, job_id: int, load_job: bool = False\n) -&gt; JobRun\n</code></pre> <p>Return the run associated with a given job ID.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>ID of the job.</p> required <code>load_job</code> <code>bool</code> <p>If True, eagerly load the related <code>Job</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>JobRun</code> <p>The run linked to the given job.</p> Source code in <code>src/icon/server/data_access/repositories/job_run_repository.py</code> <pre><code>@staticmethod\ndef get_run_by_job_id(*, job_id: int, load_job: bool = False) -&gt; JobRun:\n    \"\"\"Return the run associated with a given job ID.\n\n    Args:\n        job_id: ID of the job.\n        load_job: If True, eagerly load the related `Job`.\n\n    Returns:\n        The run linked to the given job.\n    \"\"\"\n\n    with sqlalchemy.orm.Session(engine) as session:\n        stmt = select(JobRun).where(JobRun.job_id == job_id)\n\n        if load_job:\n            stmt = stmt.options(sqlalchemy.orm.joinedload(JobRun.job))\n\n        run = session.execute(stmt).scalar_one()\n        logger.debug(\"Got JobRun by job_id %s\", job_id)\n    return run\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.job_run_repository.JobRunRepository.get_runs_by_status","title":"get_runs_by_status  <code>staticmethod</code>","text":"<pre><code>get_runs_by_status(\n    *,\n    status: JobRunStatus | list[JobRunStatus],\n    load_job: bool = False,\n) -&gt; Sequence[JobRun]\n</code></pre> <p>Return job runs filtered by status.</p> <p>Parameters:</p> Name Type Description Default <code>status</code> <code>JobRunStatus | list[JobRunStatus]</code> <p>Single or list of run statuses to filter on.</p> required <code>load_job</code> <code>bool</code> <p>If True, eagerly load the related <code>Job</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Sequence[JobRun]</code> <p>All matching runs.</p> Source code in <code>src/icon/server/data_access/repositories/job_run_repository.py</code> <pre><code>@staticmethod\ndef get_runs_by_status(\n    *,\n    status: JobRunStatus | list[JobRunStatus],\n    load_job: bool = False,\n) -&gt; Sequence[JobRun]:\n    \"\"\"Return job runs filtered by status.\n\n    Args:\n        status: Single or list of run statuses to filter on.\n        load_job: If True, eagerly load the related `Job`.\n\n    Returns:\n        All matching runs.\n    \"\"\"\n\n    if not isinstance(status, list):\n        status = [status]\n\n    with sqlalchemy.orm.Session(engine) as session:\n        stmt = (\n            select(JobRun)\n            .where(JobRun.status.in_(status))\n            .order_by(JobRun.scheduled_time.asc())\n        )\n\n        if load_job:\n            stmt = stmt.options(sqlalchemy.orm.joinedload(JobRun.job))\n\n        return session.execute(stmt).scalars().all()\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.job_run_repository.JobRunRepository.get_scheduled_time_by_job_id","title":"get_scheduled_time_by_job_id  <code>staticmethod</code>","text":"<pre><code>get_scheduled_time_by_job_id(*, job_id: int) -&gt; datetime\n</code></pre> <p>Return the scheduled time of a run by job ID.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>ID of the job.</p> required <p>Returns:</p> Type Description <code>datetime</code> <p>The scheduled start time of the run.</p> Source code in <code>src/icon/server/data_access/repositories/job_run_repository.py</code> <pre><code>@staticmethod\ndef get_scheduled_time_by_job_id(*, job_id: int) -&gt; datetime:\n    \"\"\"Return the scheduled time of a run by job ID.\n\n    Args:\n        job_id: ID of the job.\n\n    Returns:\n        The scheduled start time of the run.\n    \"\"\"\n\n    with sqlalchemy.orm.Session(engine) as session:\n        stmt = select(JobRun.scheduled_time).where(JobRun.job_id == job_id)\n\n        scheduled_time = session.execute(stmt).scalar_one()\n        logger.debug(\"Got scheduled time for job_id %s\", job_id)\n    return scheduled_time\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.job_run_repository.JobRunRepository.insert_run","title":"insert_run  <code>staticmethod</code>","text":"<pre><code>insert_run(*, run: JobRun) -&gt; JobRun\n</code></pre> <p>Insert a new job run and emit a creation event.</p> <p>Parameters:</p> Name Type Description Default <code>run</code> <code>JobRun</code> <p>The job run instance to persist.</p> required <p>Returns:</p> Type Description <code>JobRun</code> <p>The persisted job run with generated fields populated.</p> Source code in <code>src/icon/server/data_access/repositories/job_run_repository.py</code> <pre><code>@staticmethod\ndef insert_run(*, run: JobRun) -&gt; JobRun:\n    \"\"\"Insert a new job run and emit a creation event.\n\n    Args:\n        run: The job run instance to persist.\n\n    Returns:\n        The persisted job run with generated fields populated.\n    \"\"\"\n\n    with sqlalchemy.orm.Session(engine) as session:\n        session.add(run)\n        session.commit()\n        session.refresh(run)\n        logger.debug(\"Created new run %s\", run)\n\n    emit_queue.put(\n        {\n            \"event\": \"job_run.new\",\n            \"data\": {\n                \"job_run\": SQLAlchemyDictEncoder.encode(obj=run),\n            },\n        }\n    )\n\n    return run\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.job_run_repository.JobRunRepository.set_parameter_update_timestamp","title":"set_parameter_update_timestamp  <code>staticmethod</code>","text":"<pre><code>set_parameter_update_timestamp(\n    *, run_id: int, timestamp: datetime\n) -&gt; None\n</code></pre> <p>Set the paramter update timestamp.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <p>ID of the job.</p> required <code>timestamp</code> <code>datetime</code> <p>New parameter update timestamp.</p> required Source code in <code>src/icon/server/data_access/repositories/job_run_repository.py</code> <pre><code>@staticmethod\ndef set_parameter_update_timestamp(*, run_id: int, timestamp: datetime) -&gt; None:\n    \"\"\"Set the paramter update timestamp.\n\n    Args:\n        job_id: ID of the job.\n        timestamp: New parameter update timestamp.\n    \"\"\"\n\n    with sqlalchemy.orm.Session(engine) as session:\n        stmt = (\n            update(JobRun)\n            .where(JobRun.id == run_id)\n            .values(parameter_update_timestamp=timestamp.astimezone(UTC))\n            .returning(JobRun)\n        )\n\n        run = session.execute(stmt).scalar_one()\n        session.commit()\n\n        logger.debug(\"Updated parameter update timestam for run %s\", run)\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.job_run_repository.JobRunRepository.update_run_by_id","title":"update_run_by_id  <code>staticmethod</code>","text":"<pre><code>update_run_by_id(\n    *,\n    run_id: int,\n    status: JobRunStatus,\n    log: str | None = None,\n) -&gt; JobRun\n</code></pre> <p>Update a job run by ID and emit an update event.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>int</code> <p>The ID of the job run to update.</p> required <code>status</code> <code>JobRunStatus</code> <p>New status of the run.</p> required <code>log</code> <code>str | None</code> <p>Optional log message (e.g. failure reason).</p> <code>None</code> <p>Returns:</p> Type Description <code>JobRun</code> <p>The updated job run.</p> Source code in <code>src/icon/server/data_access/repositories/job_run_repository.py</code> <pre><code>@staticmethod\ndef update_run_by_id(\n    *,\n    run_id: int,\n    status: JobRunStatus,\n    log: str | None = None,\n) -&gt; JobRun:\n    \"\"\"Update a job run by ID and emit an update event.\n\n    Args:\n        run_id: The ID of the job run to update.\n        status: New status of the run.\n        log: Optional log message (e.g. failure reason).\n\n    Returns:\n        The updated job run.\n    \"\"\"\n\n    with sqlalchemy.orm.Session(engine) as session:\n        stmt = (\n            update(JobRun)\n            .where(JobRun.id == run_id)\n            .values(status=status, log=log)\n            .returning(JobRun)\n        )\n        run = session.execute(stmt).scalar_one()\n        session.commit()\n\n        logger.debug(\"Updated run %s\", run)\n\n    emit_queue.put(\n        {\n            \"event\": \"job_run.update\",\n            \"data\": {\n                \"run_id\": run_id,\n                \"updated_properties\": {\n                    \"status\": status.value,\n                    \"log\": log,\n                },\n            },\n        }\n    )\n\n    return run\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.job_run_repository.job_run_cancelled_or_failed","title":"job_run_cancelled_or_failed","text":"<pre><code>job_run_cancelled_or_failed(job_id: int) -&gt; bool\n</code></pre> <p>Check if a job\u2019s run was cancelled or failed.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>ID of the job whose run should be checked.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the run status is CANCELLED or FAILED, False otherwise.</p> Source code in <code>src/icon/server/data_access/repositories/job_run_repository.py</code> <pre><code>def job_run_cancelled_or_failed(job_id: int) -&gt; bool:\n    \"\"\"Check if a job's run was cancelled or failed.\n\n    Args:\n        job_id: ID of the job whose run should be checked.\n\n    Returns:\n        True if the run status is CANCELLED or FAILED, False otherwise.\n    \"\"\"\n\n    job_run = JobRunRepository.get_run_by_job_id(job_id=job_id)\n    if job_run.status in (JobRunStatus.CANCELLED, JobRunStatus.FAILED):\n        logger.info(\n            \"JobRun with id %s %s.\",\n            job_run.id,\n            job_run.status.value,\n        )\n        return True\n    return False\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.parameters_repository","title":"parameters_repository","text":""},{"location":"reference/server/#icon.server.data_access.repositories.parameters_repository.NotInitialisedError","title":"NotInitialisedError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when repository methods are called before initialization.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.parameters_repository.ParametersRepository","title":"ParametersRepository","text":"<p>Repository for parameter values and metadata.</p> <p>Provides methods to read and update shared parameter state (via a <code>multiprocessing.Manager</code> dict) and to persist/retrieve parameters from InfluxDB. Emits Socket.IO events on updates.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.parameters_repository.ParametersRepository.get_influxdb_parameter_by_id","title":"get_influxdb_parameter_by_id  <code>staticmethod</code>","text":"<pre><code>get_influxdb_parameter_by_id(\n    parameter_id: str,\n) -&gt; DatabaseValueType | None\n</code></pre> <p>Return a single parameter value from InfluxDB.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_id</code> <code>str</code> <p>ID of the parameter.</p> required <p>Returns:</p> Type Description <code>DatabaseValueType | None</code> <p>The parameter value, or None if not found.</p> Source code in <code>src/icon/server/data_access/repositories/parameters_repository.py</code> <pre><code>@staticmethod\ndef get_influxdb_parameter_by_id(parameter_id: str) -&gt; DatabaseValueType | None:\n    \"\"\"Return a single parameter value from InfluxDB.\n\n    Args:\n        parameter_id: ID of the parameter.\n\n    Returns:\n        The parameter value, or None if not found.\n    \"\"\"\n\n    with InfluxDBv1Session() as influxdb:\n        result_dict = influxdb.query(\n            measurement=get_config().databases.influxdbv1.measurement,\n            field=parameter_id,\n        )\n        if result_dict is None:\n            logger.error(\n                \"Could not find parameter with id %s in database %s\",\n                parameter_id,\n                get_config().databases.influxdbv1.measurement,\n            )\n            return None\n        return result_dict[parameter_id]\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.parameters_repository.ParametersRepository.get_influxdb_parameter_keys","title":"get_influxdb_parameter_keys  <code>staticmethod</code>","text":"<pre><code>get_influxdb_parameter_keys() -&gt; list[str]\n</code></pre> <p>Return all parameter field keys from InfluxDB v1.</p> Source code in <code>src/icon/server/data_access/repositories/parameters_repository.py</code> <pre><code>@staticmethod\ndef get_influxdb_parameter_keys() -&gt; list[str]:\n    \"\"\"Return all parameter field keys from InfluxDB v1.\"\"\"\n\n    with InfluxDBv1Session() as influxdbv1:\n        return influxdbv1.get_field_keys(\n            get_config().databases.influxdbv1.measurement\n        )\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.parameters_repository.ParametersRepository.get_influxdb_parameters","title":"get_influxdb_parameters  <code>staticmethod</code>","text":"<pre><code>get_influxdb_parameters(\n    *,\n    before: str | None = None,\n    namespace: str | None = None,\n) -&gt; dict[str, DatabaseValueType]\n</code></pre> <p>Return the latest parameter values from InfluxDB.</p> <p>Parameters:</p> Name Type Description Default <code>before</code> <code>str | None</code> <p>Optional ISO timestamp to query parameters before.</p> <code>None</code> <code>namespace</code> <code>str | None</code> <p>Optional namespace filter.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, DatabaseValueType]</code> <p>Mapping of parameter IDs to values.</p> Source code in <code>src/icon/server/data_access/repositories/parameters_repository.py</code> <pre><code>@staticmethod\ndef get_influxdb_parameters(\n    *, before: str | None = None, namespace: str | None = None\n) -&gt; dict[str, DatabaseValueType]:\n    \"\"\"Return the latest parameter values from InfluxDB.\n\n    Args:\n        before: Optional ISO timestamp to query parameters before.\n        namespace: Optional namespace filter.\n\n    Returns:\n        Mapping of parameter IDs to values.\n    \"\"\"\n\n    with InfluxDBv1Session() as influxdbv1:\n        return influxdbv1.query_last(\n            get_config().databases.influxdbv1.measurement,\n            before=before,\n            namespace=namespace,\n        )\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.parameters_repository.ParametersRepository.get_shared_parameter_by_id","title":"get_shared_parameter_by_id  <code>classmethod</code>","text":"<pre><code>get_shared_parameter_by_id(\n    *, parameter_id: str\n) -&gt; DatabaseValueType | None\n</code></pre> <p>Return a single parameter value from shared state.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_id</code> <code>str</code> <p>ID of the parameter.</p> required <p>Returns:</p> Type Description <code>DatabaseValueType | None</code> <p>The parameter value, or None if not set.</p> Source code in <code>src/icon/server/data_access/repositories/parameters_repository.py</code> <pre><code>@classmethod\ndef get_shared_parameter_by_id(\n    cls,\n    *,\n    parameter_id: str,\n) -&gt; DatabaseValueType | None:\n    \"\"\"Return a single parameter value from shared state.\n\n    Args:\n        parameter_id: ID of the parameter.\n\n    Returns:\n        The parameter value, or None if not set.\n    \"\"\"\n\n    cls._check_initialised()\n\n    return cls._shared_parameters.get(parameter_id, None)\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.parameters_repository.ParametersRepository.get_shared_parameters","title":"get_shared_parameters  <code>classmethod</code>","text":"<pre><code>get_shared_parameters() -&gt; DictProxy[\n    str, DatabaseValueType\n]\n</code></pre> <p>Return the full shared parameter dictionary.</p> <p>Returns:</p> Type Description <code>DictProxy[str, DatabaseValueType]</code> <p>Proxy dictionary of parameters.</p> Source code in <code>src/icon/server/data_access/repositories/parameters_repository.py</code> <pre><code>@classmethod\ndef get_shared_parameters(cls) -&gt; DictProxy[str, DatabaseValueType]:\n    \"\"\"Return the full shared parameter dictionary.\n\n    Returns:\n        Proxy dictionary of parameters.\n    \"\"\"\n\n    cls._check_initialised()\n\n    return cls._shared_parameters\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.parameters_repository.ParametersRepository.initialize","title":"initialize  <code>classmethod</code>","text":"<pre><code>initialize(\n    *, shared_parameters: DictProxy[str, DatabaseValueType]\n) -&gt; None\n</code></pre> <p>Initialize the repository with a shared parameters dict.</p> <p>Parameters:</p> Name Type Description Default <code>shared_parameters</code> <code>DictProxy[str, DatabaseValueType]</code> <p>Proxy dictionary used to store shared state.</p> required Source code in <code>src/icon/server/data_access/repositories/parameters_repository.py</code> <pre><code>@classmethod\ndef initialize(\n    cls, *, shared_parameters: DictProxy[str, DatabaseValueType]\n) -&gt; None:\n    \"\"\"Initialize the repository with a shared parameters dict.\n\n    Args:\n        shared_parameters: Proxy dictionary used to store shared state.\n    \"\"\"\n\n    cls._shared_parameters = shared_parameters\n    cls.initialised = True\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.parameters_repository.ParametersRepository.update_parameters","title":"update_parameters  <code>classmethod</code>","text":"<pre><code>update_parameters(\n    *, parameter_mapping: dict[str, DatabaseValueType]\n) -&gt; None\n</code></pre> <p>Update parameters in both shared state and InfluxDB.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_mapping</code> <code>dict[str, DatabaseValueType]</code> <p>Mapping of parameter IDs to values.</p> required Source code in <code>src/icon/server/data_access/repositories/parameters_repository.py</code> <pre><code>@classmethod\ndef update_parameters(\n    cls,\n    *,\n    parameter_mapping: dict[str, DatabaseValueType],\n) -&gt; None:\n    \"\"\"Update parameters in both shared state and InfluxDB.\n\n    Args:\n        parameter_mapping: Mapping of parameter IDs to values.\n    \"\"\"\n\n    for key, value in parameter_mapping.items():\n        if (\n            isinstance(value, int)\n            and not isinstance(value, bool)\n            and \"ParameterTypes.INT\" not in key\n        ):\n            parameter_mapping[key] = float(value)\n\n    cls._update_shared_parameters(parameter_mapping=parameter_mapping)\n    cls._update_influxdb_parameters(parameter_mapping=parameter_mapping)\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.parameters_repository.get_specifiers_from_parameter_identifier","title":"get_specifiers_from_parameter_identifier","text":"<pre><code>get_specifiers_from_parameter_identifier(\n    parameter_identifier: str,\n) -&gt; dict[str, str]\n</code></pre> <p>Extract specifiers from a parameter identifier string.</p> <p>Parameter identifiers encode metadata as <code>key='value'</code> pairs. This helper parses them into a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_identifier</code> <code>str</code> <p>Identifier string to parse.</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Mapping of specifier keys to values.</p> Source code in <code>src/icon/server/data_access/repositories/parameters_repository.py</code> <pre><code>def get_specifiers_from_parameter_identifier(\n    parameter_identifier: str,\n) -&gt; dict[str, str]:\n    \"\"\"Extract specifiers from a parameter identifier string.\n\n    Parameter identifiers encode metadata as `key='value'` pairs. This helper parses\n    them into a dictionary.\n\n    Args:\n        parameter_identifier: Identifier string to parse.\n\n    Returns:\n        Mapping of specifier keys to values.\n    \"\"\"\n\n    pattern = re.compile(r\"(\\w+)='([^']*)'\")\n    matches = pattern.findall(parameter_identifier)\n\n    return dict(matches)\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.pycrystal_library_repository","title":"pycrystal_library_repository","text":""},{"location":"reference/server/#icon.server.data_access.repositories.pycrystal_library_repository.ParameterMetadataDict","title":"ParameterMetadataDict  <code>module-attribute</code>","text":"<pre><code>ParameterMetadataDict = TypedDict(\n    \"ParameterMetadataDict\",\n    {\n        \"all parameters\": dict[str, ParameterMetadata],\n        \"display groups\": dict[\n            str, dict[str, ParameterMetadata]\n        ],\n    },\n)\n</code></pre> <p>Dictionary of parameter metadata.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.pycrystal_library_repository.ParameterAndExperimentMetadata","title":"ParameterAndExperimentMetadata","text":"<p>               Bases: <code>TypedDict</code></p> <p>Combined metadata for experiments and parameters.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.pycrystal_library_repository.ParameterAndExperimentMetadata.experiment_metadata","title":"experiment_metadata  <code>instance-attribute</code>","text":"<pre><code>experiment_metadata: ExperimentDict\n</code></pre> <p>Dictionary mapping the unique experiment identifier to its metadata.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.pycrystal_library_repository.ParameterAndExperimentMetadata.parameter_metadata","title":"parameter_metadata  <code>instance-attribute</code>","text":"<pre><code>parameter_metadata: ParameterMetadataDict\n</code></pre> <p>Dictionary of parameter metadata.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.pycrystal_library_repository.PycrystalLibraryRepository","title":"PycrystalLibraryRepository","text":"<p>Repository for interacting with the <code>pycrystal</code> experiment library.</p> <p>Provides methods to fetch experiment and parameter metadata and to generate sequences by executing helper scripts inside the experiment library\u2019s virtual environment.</p>"},{"location":"reference/server/#icon.server.data_access.repositories.pycrystal_library_repository.PycrystalLibraryRepository.generate_json_sequence","title":"generate_json_sequence  <code>async</code> <code>staticmethod</code>","text":"<pre><code>generate_json_sequence(\n    *,\n    exp_module_name: str,\n    exp_instance_name: str,\n    parameter_dict: dict[str, DatabaseValueType],\n    n_shots: int,\n) -&gt; str\n</code></pre> <p>Generate a JSON sequence for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>exp_module_name</code> <code>str</code> <p>Module name of the experiment.</p> required <code>exp_instance_name</code> <code>str</code> <p>Name of the experiment instance.</p> required <code>parameter_dict</code> <code>dict[str, DatabaseValueType]</code> <p>Mapping of parameter IDs to values.</p> required <p>Returns:</p> Type Description <code>str</code> <p>JSON string containing the generated sequence.</p> Source code in <code>src/icon/server/data_access/repositories/pycrystal_library_repository.py</code> <pre><code>@staticmethod\nasync def generate_json_sequence(\n    *,\n    exp_module_name: str,\n    exp_instance_name: str,\n    parameter_dict: dict[str, DatabaseValueType],\n    n_shots: int,\n) -&gt; str:\n    \"\"\"Generate a JSON sequence for an experiment.\n\n    Args:\n        exp_module_name: Module name of the experiment.\n        exp_instance_name: Name of the experiment instance.\n        parameter_dict: Mapping of parameter IDs to values.\n\n    Returns:\n        JSON string containing the generated sequence.\n    \"\"\"\n\n    template_vars = {\n        \"key_val_dict\": parameter_dict,\n        \"module_name\": exp_module_name,\n        \"exp_instance_name\": exp_instance_name,\n        \"n_shots\": n_shots,\n    }\n\n    code = PycrystalLibraryRepository._get_code(\n        Path(__file__).parent.parent / \"templates/generate_pycrystal_sequence.py\",\n        **template_vars,\n    )\n    return await PycrystalLibraryRepository._run_code(code)\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.pycrystal_library_repository.PycrystalLibraryRepository.get_experiment_and_parameter_metadata","title":"get_experiment_and_parameter_metadata  <code>async</code> <code>staticmethod</code>","text":"<pre><code>get_experiment_and_parameter_metadata() -&gt; (\n    ParameterAndExperimentMetadata\n)\n</code></pre> <p>Fetch the experiment and parameter metadata.</p> <p>Returns:</p> Type Description <code>ParameterAndExperimentMetadata</code> <p>Dictionary with experiment metadata and parameter metadata.</p> Source code in <code>src/icon/server/data_access/repositories/pycrystal_library_repository.py</code> <pre><code>@staticmethod\nasync def get_experiment_and_parameter_metadata() -&gt; ParameterAndExperimentMetadata:\n    \"\"\"Fetch the experiment and parameter metadata.\n\n    Returns:\n        Dictionary with experiment metadata and parameter metadata.\n    \"\"\"\n\n    code = PycrystalLibraryRepository._get_code(\n        Path(__file__).parent.parent\n        / \"templates/get_pycrystal_experiment_and_parameter_metadata.py\"\n    )\n    stdout = await PycrystalLibraryRepository._run_code(code)\n    return json.loads(stdout)\n</code></pre>"},{"location":"reference/server/#icon.server.data_access.repositories.pycrystal_library_repository.PycrystalLibraryRepository.get_experiment_readout_metadata","title":"get_experiment_readout_metadata  <code>async</code> <code>staticmethod</code>","text":"<pre><code>get_experiment_readout_metadata(\n    *,\n    exp_module_name: str,\n    exp_instance_name: str,\n    parameter_dict: dict[str, DatabaseValueType],\n) -&gt; ReadoutMetadata\n</code></pre> <p>Fetch readout metadata for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>exp_module_name</code> <code>str</code> <p>Module name of the experiment.</p> required <code>exp_instance_name</code> <code>str</code> <p>Name of the experiment instance.</p> required <code>parameter_dict</code> <code>dict[str, DatabaseValueType]</code> <p>Mapping of parameter IDs to values.</p> required <p>Returns:</p> Type Description <code>ReadoutMetadata</code> <p>Dictionary containing readout metadata for the experiment.</p> Source code in <code>src/icon/server/data_access/repositories/pycrystal_library_repository.py</code> <pre><code>@staticmethod\nasync def get_experiment_readout_metadata(\n    *,\n    exp_module_name: str,\n    exp_instance_name: str,\n    parameter_dict: dict[str, DatabaseValueType],\n) -&gt; ReadoutMetadata:\n    \"\"\"Fetch readout metadata for an experiment.\n\n    Args:\n        exp_module_name: Module name of the experiment.\n        exp_instance_name: Name of the experiment instance.\n        parameter_dict: Mapping of parameter IDs to values.\n\n    Returns:\n        Dictionary containing readout metadata for the experiment.\n    \"\"\"\n\n    template_vars = {\n        \"key_val_dict\": parameter_dict,\n        \"module_name\": exp_module_name,\n        \"exp_instance_name\": exp_instance_name,\n    }\n\n    code = PycrystalLibraryRepository._get_code(\n        Path(__file__).parent.parent\n        / \"templates/get_experiment_readout_windows.py\",\n        **template_vars,\n    )\n    stdout = await PycrystalLibraryRepository._run_code(code)\n    return json.loads(stdout)\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing","title":"icon.server.hardware_processing","text":"<p>Modules:</p> Name Description <code>hardware_controller</code> <code>task</code> <code>worker</code>"},{"location":"reference/server/#icon.server.hardware_processing.hardware_controller","title":"hardware_controller","text":"<p>Classes:</p> Name Description <code>HardwareController</code> <p>Attributes:</p> Name Type Description <code>HAS_TIQI_ZEDBOARD</code> <code>logger</code>"},{"location":"reference/server/#icon.server.hardware_processing.hardware_controller.HAS_TIQI_ZEDBOARD","title":"HAS_TIQI_ZEDBOARD  <code>module-attribute</code>","text":"<pre><code>HAS_TIQI_ZEDBOARD = True\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.hardware_controller.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.hardware_controller.HardwareController","title":"HardwareController","text":"<pre><code>HardwareController(connect: bool = True)\n</code></pre> <p>Methods:</p> Name Description <code>connect</code> <code>run</code> <p>Attributes:</p> Name Type Description <code>connected</code> <code>bool</code> Source code in <code>src/icon/server/hardware_processing/hardware_controller.py</code> <pre><code>def __init__(self, connect: bool = True) -&gt; None:\n    self._host = get_config().hardware.host\n    self._port = get_config().hardware.port\n    self._zedboard: tiqi_zedboard.zedboard.Zedboard | None = None\n    if connect:\n        self.connect()\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.hardware_controller.HardwareController.connected","title":"connected  <code>property</code>","text":"<pre><code>connected: bool\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.hardware_controller.HardwareController.connect","title":"connect","text":"<pre><code>connect() -&gt; None\n</code></pre> Source code in <code>src/icon/server/hardware_processing/hardware_controller.py</code> <pre><code>def connect(self) -&gt; None:\n    logger.info(\"Connecting to the Zedboard\")\n    self._host = get_config().hardware.host\n    self._port = get_config().hardware.port\n    if HAS_TIQI_ZEDBOARD:\n        self._zedboard = tiqi_zedboard.zedboard.Zedboard(\n            hostname=self._host, port=self._port\n        )\n        if not self.connected:\n            logger.warning(\"Failed to connect to the Zedboard\")\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.hardware_controller.HardwareController.run","title":"run","text":"<pre><code>run(*, sequence: str, number_of_shots: int) -&gt; ResultDict\n</code></pre> Source code in <code>src/icon/server/hardware_processing/hardware_controller.py</code> <pre><code>def run(self, *, sequence: str, number_of_shots: int) -&gt; ResultDict:\n    if not self.connected:\n        self.connect()\n\n    if not self.connected:\n        if HAS_TIQI_ZEDBOARD:\n            raise RuntimeError(\"Could not connect to the Zedboard\")\n        else:\n            raise RuntimeError(\"Tiqi zedboard package is not available. \"\n                               \"Please use 'uv sync --all-extras' to install all dependencies\")\n\n    self._update_zedboard_sequence(sequence=sequence)\n    self._zedboard.sequence_JSON_parser.Parse_JSON_Header()  # type: ignore\n    results: tiqi_zedboard.zedboard.Result = self._zedboard.sequence_JSON_parser()  # type: ignore\n\n    return {\n        \"result_channels\": results.result_channels,\n        \"vector_channels\": results.vector_channels\n        if results.vector_channels is not None\n        else {},\n        \"shot_channels\": results.shot_channels,\n    }\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.task","title":"task","text":"<p>Classes:</p> Name Description <code>HardwareProcessingTask</code>"},{"location":"reference/server/#icon.server.hardware_processing.task.HardwareProcessingTask","title":"HardwareProcessingTask","text":"<p>               Bases: <code>BaseModel</code></p> <p>Methods:</p> Name Description <code>__lt__</code> <p>Attributes:</p> Name Type Description <code>created</code> <code>datetime</code> <code>data_point_index</code> <code>int</code> <code>data_points_to_process</code> <code>Queue[tuple[int, dict[str, DatabaseValueType]]]</code> <code>global_parameter_timestamp</code> <code>datetime</code> <code>model_config</code> <code>outdated_tasks</code> <code>PriorityQueue[HardwareProcessingTask]</code> <code>pre_processing_task</code> <code>PreProcessingTask</code> <code>priority</code> <code>int</code> <code>processed_data_points</code> <code>Queue[HardwareProcessingTask]</code> <code>scanned_params</code> <code>dict[str, DatabaseValueType]</code> <code>sequence_json</code> <code>str</code> <code>src_dir</code> <code>str</code>"},{"location":"reference/server/#icon.server.hardware_processing.task.HardwareProcessingTask.created","title":"created  <code>instance-attribute</code>","text":"<pre><code>created: datetime\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.task.HardwareProcessingTask.data_point_index","title":"data_point_index  <code>instance-attribute</code>","text":"<pre><code>data_point_index: int\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.task.HardwareProcessingTask.data_points_to_process","title":"data_points_to_process  <code>instance-attribute</code>","text":"<pre><code>data_points_to_process: Queue[\n    tuple[int, dict[str, DatabaseValueType]]\n]\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.task.HardwareProcessingTask.global_parameter_timestamp","title":"global_parameter_timestamp  <code>instance-attribute</code>","text":"<pre><code>global_parameter_timestamp: datetime\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.task.HardwareProcessingTask.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.task.HardwareProcessingTask.outdated_tasks","title":"outdated_tasks  <code>instance-attribute</code>","text":"<pre><code>outdated_tasks: PriorityQueue[HardwareProcessingTask]\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.task.HardwareProcessingTask.pre_processing_task","title":"pre_processing_task  <code>instance-attribute</code>","text":"<pre><code>pre_processing_task: PreProcessingTask\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.task.HardwareProcessingTask.priority","title":"priority  <code>instance-attribute</code>","text":"<pre><code>priority: int\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.task.HardwareProcessingTask.processed_data_points","title":"processed_data_points  <code>instance-attribute</code>","text":"<pre><code>processed_data_points: Queue[HardwareProcessingTask]\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.task.HardwareProcessingTask.scanned_params","title":"scanned_params  <code>instance-attribute</code>","text":"<pre><code>scanned_params: dict[str, DatabaseValueType]\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.task.HardwareProcessingTask.sequence_json","title":"sequence_json  <code>instance-attribute</code>","text":"<pre><code>sequence_json: str\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.task.HardwareProcessingTask.src_dir","title":"src_dir  <code>instance-attribute</code>","text":"<pre><code>src_dir: str\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.task.HardwareProcessingTask.__lt__","title":"__lt__","text":"<pre><code>__lt__(other: HardwareProcessingTask) -&gt; bool\n</code></pre> Source code in <code>src/icon/server/hardware_processing/task.py</code> <pre><code>def __lt__(self, other: HardwareProcessingTask) -&gt; bool:\n    return self.priority &lt; other.priority or self.created &lt; other.created\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.worker","title":"worker","text":"<p>Classes:</p> Name Description <code>HardwareProcessingWorker</code> <p>Functions:</p> Name Description <code>parse_parameter_id</code> <p>Parses a parameter ID string into a device name and variable ID.</p> <p>Attributes:</p> Name Type Description <code>logger</code> <code>timezone</code>"},{"location":"reference/server/#icon.server.hardware_processing.worker.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.worker.timezone","title":"timezone  <code>module-attribute</code>","text":"<pre><code>timezone = timezone(timezone)\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.worker.HardwareProcessingWorker","title":"HardwareProcessingWorker","text":"<pre><code>HardwareProcessingWorker(\n    hardware_processing_queue: PriorityQueue[\n        HardwareProcessingTask\n    ],\n    post_processing_queue: Queue[PostProcessingTask],\n    manager: SharedResourceManager,\n)\n</code></pre> <p>               Bases: <code>Process</code></p> <p>Methods:</p> Name Description <code>run</code> Source code in <code>src/icon/server/hardware_processing/worker.py</code> <pre><code>def __init__(\n    self,\n    hardware_processing_queue: queue.PriorityQueue[HardwareProcessingTask],\n    post_processing_queue: multiprocessing.Queue[PostProcessingTask],\n    manager: SharedResourceManager,\n) -&gt; None:\n    super().__init__()\n    self._queue = hardware_processing_queue\n    self._post_processing_queue = post_processing_queue\n    self._manager = manager\n    self._pydase_clients: dict[str, pydase.Client] = {}\n\n    self._hardware_controller = HardwareController()\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.worker.HardwareProcessingWorker.run","title":"run","text":"<pre><code>run() -&gt; None\n</code></pre> Source code in <code>src/icon/server/hardware_processing/worker.py</code> <pre><code>def run(self) -&gt; None:\n    self._pydase_clients = {\n        device.name: pydase.Client(\n            url=device.url, block_until_connected=False, auto_update_proxy=False\n        )\n        for device in DeviceRepository.get_devices_by_status(\n            status=DeviceStatus.ENABLED\n        )\n    }\n\n    while True:\n        task = self._queue.get()\n\n        if job_run_cancelled_or_failed(\n            job_id=task.pre_processing_task.job.id,\n        ):\n            continue\n\n        parameter_update_timestamp = (\n            JobRunRepository.get_parameter_update_timestamp(\n                run_id=task.pre_processing_task.job_run.id,\n            )\n        )\n        if task.created &lt; parameter_update_timestamp:\n            task.outdated_tasks.put(task)\n            continue\n        try:\n            self._set_pydase_service_values(scanned_params=task.scanned_params)\n\n            timestamp = datetime.now(timezone)\n            result = self._hardware_controller.run(\n                sequence=task.sequence_json,\n                number_of_shots=task.pre_processing_task.job.number_of_shots,\n            )\n\n            experiment_data_point: ExperimentDataPoint = {\n                \"index\": task.data_point_index,\n                \"scan_params\": task.scanned_params,\n                \"result_channels\": result[\"result_channels\"],\n                \"shot_channels\": result[\"shot_channels\"],\n                \"vector_channels\": result[\"vector_channels\"],\n                \"timestamp\": timestamp.isoformat(),\n                \"sequence_json\": task.sequence_json,\n            }\n\n            post_processing_task = PostProcessingTask(\n                priority=task.priority,\n                pre_processing_task=task.pre_processing_task,\n                data_point=experiment_data_point,\n                src_dir=task.src_dir,\n                created=task.created,\n            )\n\n            self._post_processing_queue.put(post_processing_task)\n        except Exception as e:\n            logger.exception(e)\n            JobRunRepository.update_run_by_id(\n                run_id=task.pre_processing_task.job_run.id,\n                status=JobRunStatus.FAILED,\n                log=str(e),\n            )\n        finally:\n            task.processed_data_points.put(task)\n</code></pre>"},{"location":"reference/server/#icon.server.hardware_processing.worker.parse_parameter_id","title":"parse_parameter_id","text":"<pre><code>parse_parameter_id(param_id: str) -&gt; tuple[str | None, str]\n</code></pre> <p>Parses a parameter ID string into a device name and variable ID.</p> <p>If the input string is in the format \u201cDevice(device_name) variable_id\u201d, the device name and variable ID are returned as a tuple.</p> <p>Parameters:</p> Name Type Description Default <code>param_id</code> <code>str</code> <p>The parameter identifier string.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>A tuple (device_name, variable_id). If the input does not match the expected</p> <code>str</code> <p>format, device_name is None and the entire param_id is returned as the</p> <code>tuple[str | None, str]</code> <p>variable_id.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; parse_parameter_id(\"Device(my_device) my_param\")\n('my_device', 'my_param')\n</code></pre> <pre><code>&gt;&gt;&gt; parse_parameter_id(\"bare_param\")\n(None, 'bare_param')\n</code></pre> Source code in <code>src/icon/server/hardware_processing/worker.py</code> <pre><code>def parse_parameter_id(param_id: str) -&gt; tuple[str | None, str]:\n    \"\"\"Parses a parameter ID string into a device name and variable ID.\n\n    If the input string is in the format \"Device(device_name) variable_id\",\n    the device name and variable ID are returned as a tuple.\n\n    Parameters:\n        param_id: The parameter identifier string.\n\n    Returns:\n        A tuple (device_name, variable_id). If the input does not match the expected\n        format, device_name is None and the entire param_id is returned as the\n        variable_id.\n\n    Examples:\n        &gt;&gt;&gt; parse_parameter_id(\"Device(my_device) my_param\")\n        ('my_device', 'my_param')\n\n        &gt;&gt;&gt; parse_parameter_id(\"bare_param\")\n        (None, 'bare_param')\n    \"\"\"\n\n    match = re.match(r\"^Device\\(([^)]+)\\) (.*)$\", param_id)\n    if match:\n        return match[1], match[2]\n    return None, param_id\n</code></pre>"},{"location":"reference/server/#icon.server.post_processing","title":"icon.server.post_processing","text":"<p>Modules:</p> Name Description <code>task</code> <code>worker</code>"},{"location":"reference/server/#icon.server.post_processing.task","title":"task","text":"<p>Classes:</p> Name Description <code>PostProcessingTask</code>"},{"location":"reference/server/#icon.server.post_processing.task.PostProcessingTask","title":"PostProcessingTask","text":"<p>               Bases: <code>BaseModel</code></p> <p>Methods:</p> Name Description <code>__lt__</code> <p>Attributes:</p> Name Type Description <code>created</code> <code>datetime</code> <code>data_point</code> <code>ExperimentDataPoint</code> <code>pre_processing_task</code> <code>PreProcessingTask</code> <code>priority</code> <code>int</code> <code>src_dir</code> <code>str</code>"},{"location":"reference/server/#icon.server.post_processing.task.PostProcessingTask.created","title":"created  <code>instance-attribute</code>","text":"<pre><code>created: datetime\n</code></pre>"},{"location":"reference/server/#icon.server.post_processing.task.PostProcessingTask.data_point","title":"data_point  <code>instance-attribute</code>","text":"<pre><code>data_point: ExperimentDataPoint\n</code></pre>"},{"location":"reference/server/#icon.server.post_processing.task.PostProcessingTask.pre_processing_task","title":"pre_processing_task  <code>instance-attribute</code>","text":"<pre><code>pre_processing_task: PreProcessingTask\n</code></pre>"},{"location":"reference/server/#icon.server.post_processing.task.PostProcessingTask.priority","title":"priority  <code>instance-attribute</code>","text":"<pre><code>priority: int\n</code></pre>"},{"location":"reference/server/#icon.server.post_processing.task.PostProcessingTask.src_dir","title":"src_dir  <code>instance-attribute</code>","text":"<pre><code>src_dir: str\n</code></pre>"},{"location":"reference/server/#icon.server.post_processing.task.PostProcessingTask.__lt__","title":"__lt__","text":"<pre><code>__lt__(other: PostProcessingTask) -&gt; bool\n</code></pre> Source code in <code>src/icon/server/post_processing/task.py</code> <pre><code>def __lt__(self, other: PostProcessingTask) -&gt; bool:\n    return self.priority &lt; other.priority\n</code></pre>"},{"location":"reference/server/#icon.server.post_processing.worker","title":"worker","text":"<p>Classes:</p> Name Description <code>PostProcessingWorker</code> <p>Attributes:</p> Name Type Description <code>logger</code>"},{"location":"reference/server/#icon.server.post_processing.worker.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"reference/server/#icon.server.post_processing.worker.PostProcessingWorker","title":"PostProcessingWorker","text":"<pre><code>PostProcessingWorker(\n    post_processing_queue: Queue[PostProcessingTask],\n)\n</code></pre> <p>               Bases: <code>Process</code></p> <p>Methods:</p> Name Description <code>run</code> Source code in <code>src/icon/server/post_processing/worker.py</code> <pre><code>def __init__(\n    self,\n    post_processing_queue: multiprocessing.Queue[PostProcessingTask],\n) -&gt; None:\n    super().__init__()\n    self._post_processing_queue = post_processing_queue\n</code></pre>"},{"location":"reference/server/#icon.server.post_processing.worker.PostProcessingWorker.run","title":"run","text":"<pre><code>run() -&gt; None\n</code></pre> Source code in <code>src/icon/server/post_processing/worker.py</code> <pre><code>def run(self) -&gt; None:\n    logger.info(\"Pre-processing worker started\")\n\n    while True:\n        task = self._post_processing_queue.get()\n\n        if job_run_cancelled_or_failed(\n            job_id=task.pre_processing_task.job.id,\n        ):\n            continue\n\n        ExperimentDataRepository.write_experiment_data_by_job_id(\n            job_id=task.pre_processing_task.job.id,\n            data_point=task.data_point,\n        )\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing","title":"icon.server.pre_processing","text":"<p>Modules:</p> Name Description <code>task</code> <code>worker</code>"},{"location":"reference/server/#icon.server.pre_processing.task","title":"task","text":"<p>Classes:</p> Name Description <code>PreProcessingTask</code>"},{"location":"reference/server/#icon.server.pre_processing.task.PreProcessingTask","title":"PreProcessingTask","text":"<p>               Bases: <code>BaseModel</code></p> <p>Methods:</p> Name Description <code>__lt__</code> <p>Attributes:</p> Name Type Description <code>auto_calibration</code> <code>bool</code> <code>debug_mode</code> <code>bool</code> <code>git_commit_hash</code> <code>str | None</code> <code>job</code> <code>Job</code> <code>job_run</code> <code>JobRun</code> <code>local_parameters_timestamp</code> <code>str</code> <code>model_config</code> <code>priority</code> <code>int</code> <code>repetitions</code> <code>int</code> <code>scan_parameters</code> <code>list[ScanParameter]</code>"},{"location":"reference/server/#icon.server.pre_processing.task.PreProcessingTask.auto_calibration","title":"auto_calibration  <code>instance-attribute</code>","text":"<pre><code>auto_calibration: bool\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.task.PreProcessingTask.debug_mode","title":"debug_mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>debug_mode: bool = False\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.task.PreProcessingTask.git_commit_hash","title":"git_commit_hash  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>git_commit_hash: str | None = None\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.task.PreProcessingTask.job","title":"job  <code>instance-attribute</code>","text":"<pre><code>job: Job\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.task.PreProcessingTask.job_run","title":"job_run  <code>instance-attribute</code>","text":"<pre><code>job_run: JobRun\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.task.PreProcessingTask.local_parameters_timestamp","title":"local_parameters_timestamp  <code>instance-attribute</code>","text":"<pre><code>local_parameters_timestamp: str\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.task.PreProcessingTask.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.task.PreProcessingTask.priority","title":"priority  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>priority: int = Field(ge=0, le=20)\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.task.PreProcessingTask.repetitions","title":"repetitions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>repetitions: int = 1\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.task.PreProcessingTask.scan_parameters","title":"scan_parameters  <code>instance-attribute</code>","text":"<pre><code>scan_parameters: list[ScanParameter]\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.task.PreProcessingTask.__lt__","title":"__lt__","text":"<pre><code>__lt__(other: PreProcessingTask) -&gt; bool\n</code></pre> Source code in <code>src/icon/server/pre_processing/task.py</code> <pre><code>def __lt__(self, other: \"PreProcessingTask\") -&gt; bool:\n    return self.priority &lt; other.priority\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.worker","title":"worker","text":"<p>Classes:</p> Name Description <code>ExperimentIdentifier</code> <code>ParamUpdateMode</code> <code>PreProcessingWorker</code> <p>Functions:</p> Name Description <code>change_process_priority</code> <p>Changes process priority. Only superusers can decrease the niceness of a</p> <code>consume_queue</code> <code>freeze_dict</code> <code>generate_sequence_json</code> <code>get_scan_combinations</code> <p>Generates all combinations of scan parameters for a given job. Repeats each</p> <code>parse_experiment_identifier</code> <p>Parses an experiment identifier and returns:</p> <code>prepare_experiment_library_folder</code> <code>source_dir</code> <p>Attributes:</p> Name Type Description <code>ScanCombination</code> <code>T</code> <code>logger</code> <code>timezone</code>"},{"location":"reference/server/#icon.server.pre_processing.worker.ScanCombination","title":"ScanCombination  <code>module-attribute</code>","text":"<pre><code>ScanCombination = frozenset[tuple[str, DatabaseValueType]]\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.worker.T","title":"T  <code>module-attribute</code>","text":"<pre><code>T = TypeVar('T')\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.worker.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.worker.timezone","title":"timezone  <code>module-attribute</code>","text":"<pre><code>timezone = timezone(timezone)\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.worker.ExperimentIdentifier","title":"ExperimentIdentifier  <code>dataclass</code>","text":"<pre><code>ExperimentIdentifier(\n    module_name: str, class_name: str, instance_name: str\n)\n</code></pre> <p>Methods:</p> Name Description <code>__str__</code> <code>from_str</code> <p>Parses an experiment identifier and returns:</p> <p>Attributes:</p> Name Type Description <code>class_name</code> <code>str</code> <p>Experiment class name (e.g. \u2018ClassName\u2019)</p> <code>instance_name</code> <code>str</code> <p>Experiment instance name (e.g. \u2018Instance name\u2019)</p> <code>module_name</code> <code>str</code> <p>Module path (e.g. \u2018experiment_library.experiments.exp_name\u2019)</p>"},{"location":"reference/server/#icon.server.pre_processing.worker.ExperimentIdentifier.class_name","title":"class_name  <code>instance-attribute</code>","text":"<pre><code>class_name: str\n</code></pre> <p>Experiment class name (e.g. \u2018ClassName\u2019)</p>"},{"location":"reference/server/#icon.server.pre_processing.worker.ExperimentIdentifier.instance_name","title":"instance_name  <code>instance-attribute</code>","text":"<pre><code>instance_name: str\n</code></pre> <p>Experiment instance name (e.g. \u2018Instance name\u2019)</p>"},{"location":"reference/server/#icon.server.pre_processing.worker.ExperimentIdentifier.module_name","title":"module_name  <code>instance-attribute</code>","text":"<pre><code>module_name: str\n</code></pre> <p>Module path (e.g. \u2018experiment_library.experiments.exp_name\u2019)</p>"},{"location":"reference/server/#icon.server.pre_processing.worker.ExperimentIdentifier.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> Source code in <code>src/icon/server/pre_processing/worker.py</code> <pre><code>def __str__(self) -&gt; str:\n    return f\"{self.module_name}.{self.class_name}.{self.instance_name}\"\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.worker.ExperimentIdentifier.from_str","title":"from_str  <code>classmethod</code>","text":"<pre><code>from_str(identifier_str: str) -&gt; Self\n</code></pre> <p>Parses an experiment identifier and returns: - the module path (e.g. \u2018experiment_library.experiments.exp_name\u2019) - the experiment class name (e.g. \u2018ClassName\u2019) - the experiment instance name (e.g. \u2018Instance name\u2019)</p> Example <p>\u201cexperiment_library.experiments.exp_name.ClassName (Instance name)\u201d -&gt; (\u201cexperiment_library.experiments.exp_name\u201d, \u201cClassName\u201d, \u201cInstance name\u201d)</p> Source code in <code>src/icon/server/pre_processing/worker.py</code> <pre><code>@classmethod\ndef from_str(cls, identifier_str: str) -&gt; Self:\n    \"\"\"Parses an experiment identifier and returns:\n    - the module path (e.g. 'experiment_library.experiments.exp_name')\n    - the experiment class name (e.g. 'ClassName')\n    - the experiment instance name (e.g. 'Instance name')\n\n    Example:\n        \"experiment_library.experiments.exp_name.ClassName (Instance name)\"\n        -&gt; (\"experiment_library.experiments.exp_name\", \"ClassName\", \"Instance name\")\n    \"\"\"\n    match = re.match(r\"^(.*)\\.([^. ]+) \\(([^)]+)\\)$\", identifier_str)\n    if not match:\n        raise ValueError(\n            \"Unexpected format of experiment identifier: \", identifier_str\n        )\n    return cls(match.group(1), match.group(2), match.group(3))\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.worker.ParamUpdateMode","title":"ParamUpdateMode","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Attributes:</p> Name Type Description <code>ALL_FROM_TIMESTAMP</code> <code>ALL_UP_TO_DATE</code> <code>LOCALS_FROM_TS_GLOBALS_LATEST</code> <code>ONLY_NEW_PARAMETERS</code>"},{"location":"reference/server/#icon.server.pre_processing.worker.ParamUpdateMode.ALL_FROM_TIMESTAMP","title":"ALL_FROM_TIMESTAMP  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ALL_FROM_TIMESTAMP = 'all_from_timestamp'\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.worker.ParamUpdateMode.ALL_UP_TO_DATE","title":"ALL_UP_TO_DATE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ALL_UP_TO_DATE = 'all_up_to_date'\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.worker.ParamUpdateMode.LOCALS_FROM_TS_GLOBALS_LATEST","title":"LOCALS_FROM_TS_GLOBALS_LATEST  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>LOCALS_FROM_TS_GLOBALS_LATEST = 'locals_ts_globals_now'\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.worker.ParamUpdateMode.ONLY_NEW_PARAMETERS","title":"ONLY_NEW_PARAMETERS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ONLY_NEW_PARAMETERS = 'only_new_parameters'\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.worker.PreProcessingWorker","title":"PreProcessingWorker","text":"<pre><code>PreProcessingWorker(\n    worker_number: int,\n    pre_processing_queue: PriorityQueue[PreProcessingTask],\n    update_queue: Queue[UpdateQueue],\n    hardware_processing_queue: PriorityQueue[\n        HardwareProcessingTask\n    ],\n    manager: SharedResourceManager,\n)\n</code></pre> <p>               Bases: <code>Process</code></p> <p>Methods:</p> Name Description <code>run</code> Source code in <code>src/icon/server/pre_processing/worker.py</code> <pre><code>def __init__(\n    self,\n    worker_number: int,\n    pre_processing_queue: queue.PriorityQueue[PreProcessingTask],\n    update_queue: multiprocessing.Queue[UpdateQueue],\n    hardware_processing_queue: queue.PriorityQueue[HardwareProcessingTask],\n    manager: SharedResourceManager,\n) -&gt; None:\n    super().__init__()\n    self._queue = pre_processing_queue\n    self._update_queue = update_queue\n    self._hw_processing_queue = hardware_processing_queue\n    self._worker_number = worker_number\n    self._manager = manager\n    # Queues to communicate with the hardware worker:\n    self._data_points_to_process: queue.Queue[\n        tuple[int, dict[str, DatabaseValueType]]\n    ]\n    self._processed_data_points: queue.Queue[HardwareProcessingTask]\n    self._parameter_dict: dict[str, DatabaseValueType] = {}\n    self._outdated_tasks: queue.PriorityQueue[HardwareProcessingTask] = (\n        manager.PriorityQueue()\n    )\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.worker.PreProcessingWorker.run","title":"run","text":"<pre><code>run() -&gt; None\n</code></pre> Source code in <code>src/icon/server/pre_processing/worker.py</code> <pre><code>def run(self) -&gt; None:\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        logger.debug(\"Created temporary directory %s\", tmp_dir)\n\n        while True:\n            pre_processing_task = self._queue.get()\n\n            self._data_points_to_process = self._manager.Queue()\n            self._processed_data_points = self._manager.Queue()\n\n            try:\n                self._process_task(pre_processing_task, tmp_dir=tmp_dir)\n\n                logger.info(\n                    \"JobRun with id '%s' finished\",\n                    pre_processing_task.job_run.id,\n                )\n\n                if (\n                    JobRunRepository.get_run_by_job_id(\n                        job_id=pre_processing_task.job.id\n                    ).status\n                    == JobRunStatus.PROCESSING\n                ):\n                    JobRunRepository.update_run_by_id(\n                        run_id=pre_processing_task.job_run.id,\n                        status=JobRunStatus.DONE,\n                    )\n            except Exception as e:\n                logger.exception(\n                    \"JobRun with id '%s' failed with error: %s\",\n                    pre_processing_task.job_run.id,\n                    e,\n                )\n\n                if (\n                    JobRunRepository.get_run_by_job_id(\n                        job_id=pre_processing_task.job.id\n                    ).status\n                    == JobRunStatus.PROCESSING\n                ):\n                    JobRunRepository.update_run_by_id(\n                        run_id=pre_processing_task.job_run.id,\n                        status=JobRunStatus.FAILED,\n                        log=str(e),\n                    )\n            finally:\n                JobRepository.update_job_status(\n                    job=pre_processing_task.job, status=JobStatus.PROCESSED\n                )\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.worker.change_process_priority","title":"change_process_priority","text":"<pre><code>change_process_priority(priority: int) -&gt; None\n</code></pre> <p>Changes process priority. Only superusers can decrease the niceness of a process.</p> Source code in <code>src/icon/server/pre_processing/worker.py</code> <pre><code>def change_process_priority(priority: int) -&gt; None:\n    \"\"\"Changes process priority. Only superusers can decrease the niceness of a\n    process.\"\"\"\n\n    if os.getuid() == 0:\n        p = psutil.Process(os.getpid())\n\n        p.nice(priority)\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.worker.consume_queue","title":"consume_queue","text":"<pre><code>consume_queue(q: Queue[T] | Queue[T]) -&gt; Iterator[T]\n</code></pre> Source code in <code>src/icon/server/pre_processing/worker.py</code> <pre><code>def consume_queue(q: multiprocessing.Queue[T] | queue.Queue[T]) -&gt; Iterator[T]:\n    while True:\n        try:\n            yield q.get(block=False)\n        except queue.Empty:\n            return\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.worker.freeze_dict","title":"freeze_dict","text":"<pre><code>freeze_dict(\n    combination: dict[str, DatabaseValueType],\n) -&gt; ScanCombination\n</code></pre> Source code in <code>src/icon/server/pre_processing/worker.py</code> <pre><code>def freeze_dict(combination: dict[str, DatabaseValueType]) -&gt; ScanCombination:\n    return frozenset(combination.items())\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.worker.generate_sequence_json","title":"generate_sequence_json","text":"<pre><code>generate_sequence_json(\n    n_shots: int,\n    parameter_dict: dict[str, DatabaseValueType],\n    namespace: ExperimentIdentifier,\n) -&gt; str\n</code></pre> Source code in <code>src/icon/server/pre_processing/worker.py</code> <pre><code>def generate_sequence_json(\n    n_shots: int,\n    parameter_dict: dict[str, DatabaseValueType],\n    namespace: ExperimentIdentifier,\n) -&gt; str:\n    return asyncio.run(\n        PycrystalLibraryRepository.generate_json_sequence(\n            n_shots=n_shots,\n            parameter_dict=parameter_dict,\n            exp_module_name=namespace.module_name,\n            exp_instance_name=namespace.instance_name,\n        )\n    )\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.worker.get_scan_combinations","title":"get_scan_combinations","text":"<pre><code>get_scan_combinations(\n    job: Job,\n) -&gt; list[dict[str, DatabaseValueType]]\n</code></pre> <p>Generates all combinations of scan parameters for a given job. Repeats each combination <code>job.repetitions</code> times.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>The job containing scan parameters.</p> required <p>Returns:</p> Type Description <code>list[dict[str, DatabaseValueType]]</code> <p>A list of dictionaries, where each dictionary represents a combination of</p> <code>list[dict[str, DatabaseValueType]]</code> <p>parameter values.</p> Source code in <code>src/icon/server/pre_processing/worker.py</code> <pre><code>def get_scan_combinations(job: Job) -&gt; list[dict[str, DatabaseValueType]]:\n    \"\"\"Generates all combinations of scan parameters for a given job. Repeats each\n    combination `job.repetitions` times.\n\n    Args:\n        job:\n            The job containing scan parameters.\n\n    Returns:\n        A list of dictionaries, where each dictionary represents a combination of\n        parameter values.\n    \"\"\"\n\n    # Extract variable IDs and their scan values from the job's scan parameters\n    parameter_values = {\n        scan_param.unique_id(): scan_param.scan_values\n        for scan_param in job.scan_parameters\n        if not scan_param.realtime\n    }\n\n    if not parameter_values:\n        return []\n\n    # Generate combinations using itertools.product\n    keys, values = zip(*parameter_values.items())\n\n    combinations = itertools.product(*values)\n\n    # Map each combination back to variable IDs\n    return [\n        dict(zip(keys, combination)) for combination in combinations\n    ] * job.repetitions\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.worker.parse_experiment_identifier","title":"parse_experiment_identifier","text":"<pre><code>parse_experiment_identifier(\n    identifier: str,\n) -&gt; tuple[str, str, str]\n</code></pre> <p>Parses an experiment identifier and returns: - the module path (e.g. \u2018experiment_library.experiments.exp_name\u2019) - the experiment class name (e.g. \u2018ClassName\u2019) - the experiment instance name (e.g. \u2018Instance name\u2019)</p> Example <p>\u201cexperiment_library.experiments.exp_name.ClassName (Instance name)\u201d -&gt; (\u201cexperiment_library.experiments.exp_name\u201d, \u201cClassName\u201d, \u201cInstance name\u201d)</p> Source code in <code>src/icon/server/pre_processing/worker.py</code> <pre><code>def parse_experiment_identifier(identifier: str) -&gt; tuple[str, str, str]:\n    \"\"\"\n    Parses an experiment identifier and returns:\n    - the module path (e.g. 'experiment_library.experiments.exp_name')\n    - the experiment class name (e.g. 'ClassName')\n    - the experiment instance name (e.g. 'Instance name')\n\n    Example:\n        \"experiment_library.experiments.exp_name.ClassName (Instance name)\"\n        -&gt; (\"experiment_library.experiments.exp_name\", \"ClassName\", \"Instance name\")\n    \"\"\"\n\n    match = re.match(r\"^(.*)\\.([^. ]+) \\(([^)]+)\\)$\", identifier)\n    if match:\n        return match.group(1), match.group(2), match.group(3)\n    raise ValueError(\"Unexpected format of experiment identifier: \", identifier)\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.worker.prepare_experiment_library_folder","title":"prepare_experiment_library_folder","text":"<pre><code>prepare_experiment_library_folder(\n    src_dir: str, pre_processing_task: PreProcessingTask\n) -&gt; None\n</code></pre> Source code in <code>src/icon/server/pre_processing/worker.py</code> <pre><code>def prepare_experiment_library_folder(\n    src_dir: str, pre_processing_task: PreProcessingTask\n) -&gt; None:\n    if not icon.server.utils.git_helpers.local_repo_exists(\n        repository_dir=src_dir,\n        repository=get_config().experiment_library.git_repository,\n    ):\n        icon.server.utils.git_helpers.git_clone(\n            repository=get_config().experiment_library.git_repository,\n            dir=src_dir,\n        )\n\n    icon.server.utils.git_helpers.checkout_commit(\n        git_hash=pre_processing_task.git_commit_hash, cwd=src_dir\n    )\n</code></pre>"},{"location":"reference/server/#icon.server.pre_processing.worker.source_dir","title":"source_dir","text":"<pre><code>source_dir(*, debug_mode: bool, tmp_dir: str) -&gt; str\n</code></pre> Source code in <code>src/icon/server/pre_processing/worker.py</code> <pre><code>def source_dir(*, debug_mode: bool, tmp_dir: str) -&gt; str:\n    if (experiment_library_dir := get_config().experiment_library.dir) is None:\n        raise RuntimeError(\"Config: experiment_library.dir is not defined\")\n\n    return experiment_library_dir if debug_mode else tmp_dir\n</code></pre>"},{"location":"reference/server/#icon.server.scheduler","title":"icon.server.scheduler","text":"<p>Modules:</p> Name Description <code>scheduler</code>"},{"location":"reference/server/#icon.server.scheduler.scheduler","title":"scheduler","text":"<p>Classes:</p> Name Description <code>Scheduler</code> <p>Functions:</p> Name Description <code>initialise_job_tables</code> <code>should_exit</code>"},{"location":"reference/server/#icon.server.scheduler.scheduler.Scheduler","title":"Scheduler","text":"<pre><code>Scheduler(\n    pre_processing_queue: PriorityQueue[PreProcessingTask],\n    **kwargs: Any,\n)\n</code></pre> <p>               Bases: <code>Process</code></p> <p>Methods:</p> Name Description <code>run</code> <p>Attributes:</p> Name Type Description <code>kwargs</code> Source code in <code>src/icon/server/scheduler/scheduler.py</code> <pre><code>def __init__(\n    self,\n    pre_processing_queue: queue.PriorityQueue[PreProcessingTask],\n    **kwargs: Any,\n) -&gt; None:\n    super().__init__()\n    self.kwargs = kwargs\n    self._pre_processing_queue = pre_processing_queue\n</code></pre>"},{"location":"reference/server/#icon.server.scheduler.scheduler.Scheduler.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs = kwargs\n</code></pre>"},{"location":"reference/server/#icon.server.scheduler.scheduler.Scheduler.run","title":"run","text":"<pre><code>run() -&gt; None\n</code></pre> Source code in <code>src/icon/server/scheduler/scheduler.py</code> <pre><code>def run(self) -&gt; None:\n    initialise_job_tables()\n    while not should_exit():\n        jobs = JobRepository.get_jobs_by_status_and_timeframe(\n            status=JobStatus.SUBMITTED\n        )\n        for job_ in jobs:\n            job = JobRepository.update_job_status(\n                job=job_, status=JobStatus.PROCESSING\n            )\n            run = JobRun(job_id=job.id, scheduled_time=datetime.now(tz=timezone))\n            run = JobRunRepository.insert_run(run=run)\n\n            self._pre_processing_queue.put(\n                PreProcessingTask(\n                    job=job,\n                    job_run=run,\n                    git_commit_hash=job.git_commit_hash,\n                    scan_parameters=job.scan_parameters,\n                    local_parameters_timestamp=job.local_parameters_timestamp.astimezone(\n                        tz=timezone\n                    ).isoformat(),\n                    priority=job.priority,\n                    auto_calibration=job.auto_calibration,\n                    debug_mode=job.debug_mode,\n                    repetitions=job.repetitions,\n                )\n            )\n        time.sleep(0.1)\n</code></pre>"},{"location":"reference/server/#icon.server.scheduler.scheduler.initialise_job_tables","title":"initialise_job_tables","text":"<pre><code>initialise_job_tables() -&gt; None\n</code></pre> Source code in <code>src/icon/server/scheduler/scheduler.py</code> <pre><code>def initialise_job_tables() -&gt; None:\n    # update job_runs table\n    job_runs = JobRunRepository.get_runs_by_status(\n        status=[JobRunStatus.PENDING, JobRunStatus.PROCESSING]\n    )\n    for job_run in job_runs:\n        JobRunRepository.update_run_by_id(\n            run_id=job_run.id,\n            status=JobRunStatus.CANCELLED,\n            log=\"Cancelled during scheduler initialization.\",\n        )\n\n    # update jobs table\n    jobs = JobRepository.get_jobs_by_status_and_timeframe(status=JobStatus.PROCESSING)\n    for job in jobs:\n        JobRepository.update_job_status(job=job, status=JobStatus.PROCESSED)\n</code></pre>"},{"location":"reference/server/#icon.server.scheduler.scheduler.should_exit","title":"should_exit","text":"<pre><code>should_exit() -&gt; bool\n</code></pre> Source code in <code>src/icon/server/scheduler/scheduler.py</code> <pre><code>def should_exit() -&gt; bool:\n    return False\n</code></pre>"},{"location":"reference/server/#icon.server.utils.types","title":"icon.server.utils.types","text":"<p>Classes:</p> Name Description <code>UpdateQueue</code>"},{"location":"reference/server/#icon.server.utils.types.UpdateQueue","title":"UpdateQueue","text":"<p>               Bases: <code>TypedDict</code></p> <p>Attributes:</p> Name Type Description <code>event</code> <code>Literal['update_parameters', 'calibration']</code> <code>job_id</code> <code>NotRequired[int | None]</code> <code>new_parameters</code> <code>NotRequired[dict[str, DatabaseValueType]]</code>"},{"location":"reference/server/#icon.server.utils.types.UpdateQueue.event","title":"event  <code>instance-attribute</code>","text":"<pre><code>event: Literal['update_parameters', 'calibration']\n</code></pre>"},{"location":"reference/server/#icon.server.utils.types.UpdateQueue.job_id","title":"job_id  <code>instance-attribute</code>","text":"<pre><code>job_id: NotRequired[int | None]\n</code></pre>"},{"location":"reference/server/#icon.server.utils.types.UpdateQueue.new_parameters","title":"new_parameters  <code>instance-attribute</code>","text":"<pre><code>new_parameters: NotRequired[dict[str, DatabaseValueType]]\n</code></pre>"},{"location":"reference/server/#icon.server.web_server","title":"icon.server.web_server","text":"<p>Modules:</p> Name Description <code>icon_server</code> <code>sio_setup</code> <code>socketio_emit_queue</code>"},{"location":"reference/server/#icon.server.web_server.icon_server","title":"icon_server","text":"<p>Classes:</p> Name Description <code>IconServer</code> <p>Attributes:</p> Name Type Description <code>logger</code>"},{"location":"reference/server/#icon.server.web_server.icon_server.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"reference/server/#icon.server.web_server.icon_server.IconServer","title":"IconServer","text":"<p>               Bases: <code>Server</code></p> <p>Methods:</p> Name Description <code>post_startup</code>"},{"location":"reference/server/#icon.server.web_server.icon_server.IconServer.post_startup","title":"post_startup  <code>async</code>","text":"<pre><code>post_startup() -&gt; None\n</code></pre> Source code in <code>src/icon/server/web_server/icon_server.py</code> <pre><code>async def post_startup(self) -&gt; None:\n    sio = self._web_server._sio\n\n    async def emit_worker() -&gt; None:\n        while not self.should_exit:\n            try:\n                emit_event = await asyncio.to_thread(emit_queue.get, timeout=1.0)\n            except queue.Empty:\n                continue\n            await sio.emit(\n                event=emit_event[\"event\"],\n                data=emit_event.get(\"data\", None),\n                room=emit_event.get(\"room\", None),\n            )\n\n    asyncio.create_task(emit_worker())\n\n    def devices_callback(\n        full_access_path: str, value: Any, cached_value_dict: SerializedObject\n    ) -&gt; None:\n        \"\"\"This callback handles structural changes of devices. If the structure of\n        a device changes, it will re-calculate the scannable parameters and emit\n        them to the interested clients.\"\"\"\n\n        if full_access_path.startswith(\"devices.device_proxies\"):\n            emit_scannable_device_params_change(\n                self._observer, full_access_path, value, cached_value_dict\n            )\n\n    self._observer.add_notification_callback(devices_callback)\n</code></pre>"},{"location":"reference/server/#icon.server.web_server.sio_setup","title":"sio_setup","text":"<p>Classes:</p> Name Description <code>AsyncServer</code> <p>Functions:</p> Name Description <code>patch_sio_setup</code> <code>setup_sio_events</code> <p>Attributes:</p> Name Type Description <code>logger</code> <code>pydase_setup_sio_events</code>"},{"location":"reference/server/#icon.server.web_server.sio_setup.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"reference/server/#icon.server.web_server.sio_setup.pydase_setup_sio_events","title":"pydase_setup_sio_events  <code>module-attribute</code>","text":"<pre><code>pydase_setup_sio_events = setup_sio_events\n</code></pre>"},{"location":"reference/server/#icon.server.web_server.sio_setup.AsyncServer","title":"AsyncServer","text":"<p>               Bases: <code>AsyncServer</code></p> <p>Attributes:</p> Name Type Description <code>controlling_sid</code> <code>str | None</code> <p>Socketio SID of the client controlling the frontend.</p>"},{"location":"reference/server/#icon.server.web_server.sio_setup.AsyncServer.controlling_sid","title":"controlling_sid  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>controlling_sid: str | None = None\n</code></pre> <p>Socketio SID of the client controlling the frontend.</p>"},{"location":"reference/server/#icon.server.web_server.sio_setup.patch_sio_setup","title":"patch_sio_setup","text":"<pre><code>patch_sio_setup() -&gt; None\n</code></pre> Source code in <code>src/icon/server/web_server/sio_setup.py</code> <pre><code>def patch_sio_setup() -&gt; None:\n    import pydase.server.web_server.sio_setup  # noqa: PLC0415\n\n    pydase.server.web_server.sio_setup.setup_sio_events = setup_sio_events\n</code></pre>"},{"location":"reference/server/#icon.server.web_server.sio_setup.setup_sio_events","title":"setup_sio_events","text":"<pre><code>setup_sio_events(\n    sio: AsyncServer, state_manager: StateManager\n) -&gt; None\n</code></pre> Source code in <code>src/icon/server/web_server/sio_setup.py</code> <pre><code>def setup_sio_events(\n    sio: AsyncServer,\n    state_manager: pydase.data_service.state_manager.StateManager,\n) -&gt; None:\n    pydase_setup_sio_events(sio, state_manager)\n\n    sio.controlling_sid = None\n\n    @sio.event\n    async def connect(sid: str, environ: Any) -&gt; None:\n        client_id_header = environ.get(\"HTTP_X_CLIENT_ID\", None)\n        remote_username_header = environ.get(\"HTTP_REMOTE_USER\", None)\n\n        if remote_username_header is not None:\n            log_id = f\"user={click.style(remote_username_header, fg='cyan')}\"\n        elif client_id_header is not None:\n            log_id = f\"id={click.style(client_id_header, fg='cyan')}\"\n        else:\n            log_id = f\"sid={click.style(sid, fg='cyan')}\"\n\n        # send current controlling state to the newly connected client\n        await sio.emit(\n            \"control_state\", {\"controlling_sid\": sio.controlling_sid}, to=sid\n        )\n\n        async with sio.session(sid) as session:\n            session[\"client_id\"] = log_id\n            logger.info(\"Client [%s] connected\", session[\"client_id\"])\n\n    @sio.event\n    async def disconnect(sid: str) -&gt; None:\n        if sid == sio.controlling_sid:\n            sio.controlling_sid = None\n            await sio.emit(\"control_state\", {\"controlling_sid\": None})\n\n        async with sio.session(sid) as session:\n            logger.info(\"Client [%s] disconnected\", session[\"client_id\"])\n\n    @sio.event\n    async def take_control(sid: str) -&gt; None:\n        sio.controlling_sid = sid\n        await sio.emit(\"control_state\", {\"controlling_sid\": sio.controlling_sid})\n\n    @sio.event\n    async def release_control(sid: str) -&gt; None:\n        if sio.controlling_sid == sid:\n            sio.controlling_sid = None\n            await sio.emit(\"control_state\", {\"controlling_sid\": None})\n</code></pre>"},{"location":"reference/server/#icon.server.web_server.socketio_emit_queue","title":"socketio_emit_queue","text":"<p>Classes:</p> Name Description <code>EmitEvent</code> <p>Attributes:</p> Name Type Description <code>emit_queue</code> <code>Queue[EmitEvent]</code>"},{"location":"reference/server/#icon.server.web_server.socketio_emit_queue.emit_queue","title":"emit_queue  <code>module-attribute</code>","text":"<pre><code>emit_queue: Queue[EmitEvent] = Queue()\n</code></pre>"},{"location":"reference/server/#icon.server.web_server.socketio_emit_queue.EmitEvent","title":"EmitEvent","text":"<p>               Bases: <code>TypedDict</code></p> <p>Attributes:</p> Name Type Description <code>data</code> <code>Any</code> <code>event</code> <code>str</code> <code>room</code> <code>NotRequired[str]</code>"},{"location":"reference/server/#icon.server.web_server.socketio_emit_queue.EmitEvent.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: Any\n</code></pre>"},{"location":"reference/server/#icon.server.web_server.socketio_emit_queue.EmitEvent.event","title":"event  <code>instance-attribute</code>","text":"<pre><code>event: str\n</code></pre>"},{"location":"reference/server/#icon.server.web_server.socketio_emit_queue.EmitEvent.room","title":"room  <code>instance-attribute</code>","text":"<pre><code>room: NotRequired[str]\n</code></pre>"}]}